{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and hyperparameter tune with RAPIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an Azure ML Workspace and setup environmnet on local computer following the steps in [Azure README.md](https://gitlab-master.nvidia.com/drobison/aws-sagemaker-gtc-2020/tree/master/azure/README.md )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SDK version: 1.3.0\n"
    }
   ],
   "source": [
    "# verify installation and check Azure ML SDK version\n",
    "import azureml.core\n",
    "\n",
    "print('SDK version:', azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install [AzCopy](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10) to download dataset from [Azure Blob storage](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-overview) to your local computer (or another storage account)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use 20 million rows (samples) of the [airline dataset](http://kt.ijs.si/elena_ikonomovska/data.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./azcopy cp 'https://airlinedataset.blob.core.windows.net/airline-10years/*' '/Users/nanthini/OneDrive - NVIDIA Corporation/csp/AzureML-RAPIDS/notebooks/data/par/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Workspace name: HPO-Workspace-Nanthini\nAzure region: eastus\nSubscription id: 73612009-b37b-413f-a3f7-ec02f12498cf\nResource group: RAPIDS-HPO-Nanthini\nDefault datastore's name: workspaceblobstore\n"
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "# if a locally-saved configuration file for the workspace is not available, use the following to load workspace\n",
    "# ws = Workspace(subscription_id=subscription_id, resource_group=resource_group, workspace_name=workspace_name)\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "print(\"Default datastore's name: {}\".format(datastore.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the dataset to the workspace's default datastore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Uploading an estimated of 122 files\nTarget already exists. Skipping upload for 10_year_data/part.33.parquet\nTarget already exists. Skipping upload for 10_year_data/part.23.parquet\nTarget already exists. Skipping upload for 10_year_data/part.51.parquet\nTarget already exists. Skipping upload for 10_year_data/part.41.parquet\nTarget already exists. Skipping upload for 10_year_data/part.77.parquet\nTarget already exists. Skipping upload for 10_year_data/part.67.parquet\nTarget already exists. Skipping upload for 10_year_data/_common_metadata\nTarget already exists. Skipping upload for 10_year_data/part.15.parquet\nTarget already exists. Skipping upload for 10_year_data/part.4.parquet\nTarget already exists. Skipping upload for 10_year_data/part.48.parquet\nTarget already exists. Skipping upload for 10_year_data/part.58.parquet\nTarget already exists. Skipping upload for 10_year_data/part.105.parquet\nTarget already exists. Skipping upload for 10_year_data/part.115.parquet\nTarget already exists. Skipping upload for 10_year_data/part.93.parquet\nTarget already exists. Skipping upload for 10_year_data/part.83.parquet\nTarget already exists. Skipping upload for 10_year_data/part.66.parquet\nTarget already exists. Skipping upload for 10_year_data/part.76.parquet\nTarget already exists. Skipping upload for 10_year_data/part.14.parquet\nTarget already exists. Skipping upload for 10_year_data/part.59.parquet\nTarget already exists. Skipping upload for 10_year_data/part.49.parquet\nTarget already exists. Skipping upload for 10_year_data/part.5.parquet\nTarget already exists. Skipping upload for 10_year_data/part.82.parquet\nTarget already exists. Skipping upload for 10_year_data/part.92.parquet\nTarget already exists. Skipping upload for 10_year_data/part.114.parquet\nTarget already exists. Skipping upload for 10_year_data/part.104.parquet\nTarget already exists. Skipping upload for 10_year_data/part.22.parquet\nTarget already exists. Skipping upload for 10_year_data/part.32.parquet\nTarget already exists. Skipping upload for 10_year_data/part.40.parquet\nTarget already exists. Skipping upload for 10_year_data/part.50.parquet\nTarget already exists. Skipping upload for 10_year_data/part.16.parquet\nTarget already exists. Skipping upload for 10_year_data/part.74.parquet\nTarget already exists. Skipping upload for 10_year_data/part.64.parquet\nTarget already exists. Skipping upload for 10_year_data/part.90.parquet\nTarget already exists. Skipping upload for 10_year_data/part.80.parquet\nTarget already exists. Skipping upload for 10_year_data/part.29.parquet\nTarget already exists. Skipping upload for 10_year_data/part.39.parquet\nTarget already exists. Skipping upload for 10_year_data/part.106.parquet\nTarget already exists. Skipping upload for 10_year_data/part.116.parquet\nTarget already exists. Skipping upload for 10_year_data/part.7.parquet\nTarget already exists. Skipping upload for 10_year_data/part.52.parquet\nTarget already exists. Skipping upload for 10_year_data/part.42.parquet\nTarget already exists. Skipping upload for 10_year_data/part.89.parquet\nTarget already exists. Skipping upload for 10_year_data/part.99.parquet\nTarget already exists. Skipping upload for 10_year_data/part.30.parquet\nTarget already exists. Skipping upload for 10_year_data/part.20.parquet\nTarget already exists. Skipping upload for 10_year_data/part.43.parquet\nTarget already exists. Skipping upload for 10_year_data/part.53.parquet\nTarget already exists. Skipping upload for 10_year_data/part.21.parquet\nTarget already exists. Skipping upload for 10_year_data/part.31.parquet\nTarget already exists. Skipping upload for 10_year_data/part.98.parquet\nTarget already exists. Skipping upload for 10_year_data/part.88.parquet\nTarget already exists. Skipping upload for 10_year_data/part.17.parquet\nTarget already exists. Skipping upload for 10_year_data/part.65.parquet\nTarget already exists. Skipping upload for 10_year_data/part.75.parquet\nTarget already exists. Skipping upload for 10_year_data/part.38.parquet\nTarget already exists. Skipping upload for 10_year_data/part.28.parquet\nTarget already exists. Skipping upload for 10_year_data/part.117.parquet\nTarget already exists. Skipping upload for 10_year_data/part.107.parquet\nTarget already exists. Skipping upload for 10_year_data/part.81.parquet\nTarget already exists. Skipping upload for 10_year_data/part.91.parquet\nTarget already exists. Skipping upload for 10_year_data/part.6.parquet\nTarget already exists. Skipping upload for 10_year_data/_metadata\nTarget already exists. Skipping upload for 10_year_data/part.102.parquet\nTarget already exists. Skipping upload for 10_year_data/part.112.parquet\nTarget already exists. Skipping upload for 10_year_data/part.94.parquet\nTarget already exists. Skipping upload for 10_year_data/part.84.parquet\nTarget already exists. Skipping upload for 10_year_data/part.3.parquet\nTarget already exists. Skipping upload for 10_year_data/part.12.parquet\nTarget already exists. Skipping upload for 10_year_data/part.70.parquet\nTarget already exists. Skipping upload for 10_year_data/part.60.parquet\nTarget already exists. Skipping upload for 10_year_data/part.56.parquet\nTarget already exists. Skipping upload for 10_year_data/part.46.parquet\nTarget already exists. Skipping upload for 10_year_data/part.34.parquet\nTarget already exists. Skipping upload for 10_year_data/part.24.parquet\nTarget already exists. Skipping upload for 10_year_data/part.69.parquet\nTarget already exists. Skipping upload for 10_year_data/part.79.parquet\nTarget already exists. Skipping upload for 10_year_data/part.47.parquet\nTarget already exists. Skipping upload for 10_year_data/part.57.parquet\nTarget already exists. Skipping upload for 10_year_data/part.25.parquet\nTarget already exists. Skipping upload for 10_year_data/part.35.parquet\nTarget already exists. Skipping upload for 10_year_data/part.78.parquet\nTarget already exists. Skipping upload for 10_year_data/part.68.parquet\nTarget already exists. Skipping upload for 10_year_data/part.85.parquet\nTarget already exists. Skipping upload for 10_year_data/part.95.parquet\nTarget already exists. Skipping upload for 10_year_data/part.113.parquet\nTarget already exists. Skipping upload for 10_year_data/part.103.parquet\nTarget already exists. Skipping upload for 10_year_data/part.2.parquet\nTarget already exists. Skipping upload for 10_year_data/part.13.parquet\nTarget already exists. Skipping upload for 10_year_data/part.61.parquet\nTarget already exists. Skipping upload for 10_year_data/part.71.parquet\nTarget already exists. Skipping upload for 10_year_data/part.37.parquet\nTarget already exists. Skipping upload for 10_year_data/part.27.parquet\nTarget already exists. Skipping upload for 10_year_data/part.118.parquet\nTarget already exists. Skipping upload for 10_year_data/part.108.parquet\nTarget already exists. Skipping upload for 10_year_data/part.55.parquet\nTarget already exists. Skipping upload for 10_year_data/part.45.parquet\nTarget already exists. Skipping upload for 10_year_data/part.9.parquet\nTarget already exists. Skipping upload for 10_year_data/part.18.parquet\nTarget already exists. Skipping upload for 10_year_data/part.0.parquet\nTarget already exists. Skipping upload for 10_year_data/part.97.parquet\nTarget already exists. Skipping upload for 10_year_data/part.87.parquet\nTarget already exists. Skipping upload for 10_year_data/part.101.parquet\nTarget already exists. Skipping upload for 10_year_data/part.111.parquet\nTarget already exists. Skipping upload for 10_year_data/part.73.parquet\nTarget already exists. Skipping upload for 10_year_data/part.63.parquet\nTarget already exists. Skipping upload for 10_year_data/part.11.parquet\nTarget already exists. Skipping upload for 10_year_data/part.1.parquet\nTarget already exists. Skipping upload for 10_year_data/part.110.parquet\nTarget already exists. Skipping upload for 10_year_data/part.100.parquet\nTarget already exists. Skipping upload for 10_year_data/part.86.parquet\nTarget already exists. Skipping upload for 10_year_data/part.96.parquet\nTarget already exists. Skipping upload for 10_year_data/part.62.parquet\nTarget already exists. Skipping upload for 10_year_data/part.72.parquet\nTarget already exists. Skipping upload for 10_year_data/part.10.parquet\nTarget already exists. Skipping upload for 10_year_data/part.26.parquet\nTarget already exists. Skipping upload for 10_year_data/part.36.parquet\nTarget already exists. Skipping upload for 10_year_data/part.109.parquet\nTarget already exists. Skipping upload for 10_year_data/part.119.parquet\nTarget already exists. Skipping upload for 10_year_data/part.8.parquet\nTarget already exists. Skipping upload for 10_year_data/part.44.parquet\nTarget already exists. Skipping upload for 10_year_data/part.54.parquet\nTarget already exists. Skipping upload for 10_year_data/part.19.parquet\nUploaded 0 files\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "$AZUREML_DATAREFERENCE_6b8d3ec2a7f344ce89daaa2a73b72713"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "path_on_datastore = '10_year_data'\n",
    "datastore.upload(src_dir='/Users/nanthini/OneDrive - NVIDIA Corporation/csp/AzureML-RAPIDS/notebooks/data/par', target_path=path_on_datastore, overwrite=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "$AZUREML_DATAREFERENCE_028c9b659c724e5bbfbc3b71ff840262\n"
    }
   ],
   "source": [
    "ds_data = datastore.path(path_on_datastore)\n",
    "print(ds_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this notebook, we will use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training using a dynamically scalable pool of compute resources.\n",
    "\n",
    "This notebook will use 10 nodes for hyperparameter optimization, you can modify `max_node` based on available quota in the desired region. Similar to other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. [This article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) includes details on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vm_size` describes the virtual machine type and size that will be used in the cluster. RAPIDS requires NVIDIA Pascal or newer architecture, you will need to specify compute targets from one of `NC_v2`, `NC_v3`, `ND` or `ND_v2` [GPU virtual machines in Azure](https://docs.microsoft.com/en-us/azure/virtual-machines/sizes-gpu); these are VMs that are provisioned with P40 and V100 GPUs. Let's create an `AmlCompute` cluster of `Standard_NC6s_v3` GPU VMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found compute target. Will use gpu-cluster \n{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-05-07T05:58:50.345000+00:00', 'errors': None, 'creationTime': '2020-05-06T20:26:47.977000+00:00', 'modifiedTime': '2020-05-06T20:27:04.784279+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 5, 'nodeIdleTimeBeforeScaleDown': 'PT300S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC12S_V3'}\n"
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "gpu_cluster_name = 'gpu-cluster'\n",
    "\n",
    "if gpu_cluster_name in ws.compute_targets:\n",
    "    gpu_cluster = ws.compute_targets[gpu_cluster_name]\n",
    "    if gpu_cluster and type(gpu_cluster) is AmlCompute:\n",
    "        print('Found compute target. Will use {0} '.format(gpu_cluster_name))\n",
    "else:\n",
    "    print('creating new cluster')\n",
    "    # m_size parameter below could be modified to one of the RAPIDS-supported VM types\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = 'Standard_NC12s_v3', max_nodes = 5, idle_seconds_before_scaledown = 300)\n",
    "\n",
    "    # create the cluster\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    gpu_cluster.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "# use get_status() to get a detailed status for the current cluster \n",
    "print(gpu_cluster.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a project directory that will contain code from your local machine that you will need access to on the remote resource. This includes the training script and additional files your training script depends on. In this example, the training script is provided: \n",
    "<br>\n",
    "`train_rapids_RF.py` - entry script for RAPIDS Estimator that includes loading dataset into cuDF data frame, training with Random Forest and inference using cuML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './train_rapids'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will log some metrics by using the `Run` object within the training script:\n",
    "\n",
    "```python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    " \n",
    "We will also log the parameters and highest accuracy the model achieves:\n",
    "\n",
    "```python\n",
    "run.log('Accuracy', np.float(accuracy))\n",
    "```\n",
    "\n",
    "These run metrics will become particularly important when we begin hyperparameter tuning our model in the 'Tune model hyperparameters' section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the training script `train_rapids_RF.py` into your project directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'./train_rapids/rapids_csp_azure.py'"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('../code/train_rapids_RF.py', project_folder)\n",
    "shutil.copy('../code/rapids_csp_azure.py', project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your data and training script prepared, you are ready to train on your remote compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'train_rapids'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Environment class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py) allows you to build a Docker image and customize the system that you will use for training. We will build a container image using a RAPIDS container as base image and install necessary packages. This build is necessary only the first time and will take about 15 minutes. The image will be added to your Azure Container Registry and the environment will be cached after the first run, as long as the environment definition remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# create the environment\n",
    "rapids_env = Environment('rapids_env')\n",
    "\n",
    "# create the environment inside a Docker container\n",
    "rapids_env.docker.enabled = True\n",
    "\n",
    "# specify docker steps as a string. Alternatively, load the string from a file\n",
    "dockerfile = \"\"\"\n",
    "FROM rapidsai/rapidsai-nightly:0.14-cuda10.0-runtime-ubuntu18.04-py3.7\n",
    "RUN source activate rapids && \\\n",
    "pip install azureml-sdk && \\\n",
    "pip install azureml-widgets\n",
    "\"\"\"\n",
    "#FROM nvcr.io/nvidia/rapidsai/rapidsai:0.12-cuda10.0-runtime-ubuntu18.04\n",
    "\n",
    "# set base image to None since the image is defined by dockerfile\n",
    "rapids_env.docker.base_image = None\n",
    "rapids_env.docker.base_dockerfile = dockerfile\n",
    "\n",
    "# use rapids environment in the container\n",
    "rapids_env.python.user_managed_dependencies = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core.container_registry import ContainerRegistry\n",
    "\n",
    "# # this is an image available on Docker Hub\n",
    "# image_name = 'zronaghi/rapidsai-nightly:0.13-cuda10.0-runtime-ubuntu18.04-py3.7-azuresdk-030920'\n",
    "\n",
    "# # use rapids environment, don't build a new conda environment\n",
    "# user_managed_dependencies = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RAPIDS Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Estimator](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.estimator.estimator?view=azure-ml-py) class can be used with machine learning frameworks that do not have a pre-configure estimator. \n",
    "\n",
    "`script_params` is a dictionary of command-line arguments to pass to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "$AZUREML_DATAREFERENCE_028c9b659c724e5bbfbc3b71ff840262"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "\n",
    "ds_data.as_mount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data_dir': ds_data.as_mount(),\n",
    "    '--n_bins': 32,\n",
    "}\n",
    "\n",
    "estimator = Estimator(source_directory=project_folder,\n",
    "                      script_params=script_params,\n",
    "                      compute_target=gpu_cluster, \n",
    "                      entry_script='train_rapids_RF.py',\n",
    "                      environment_definition=rapids_env)\n",
    "#                       custom_docker_image=image_name,\n",
    "#                       user_managed=user_managed_dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optimize our model's hyperparameters and improve the accuracy using Azure Machine Learning's hyperparameter tuning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a hyperparameter sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the hyperparameter space to sweep over. We will tune `n_estimators`, `max_depth` and `max_features` parameters. In this example we will use random sampling to try different configuration sets of hyperparameters and maximize `Accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice, loguniform, uniform\n",
    "\n",
    "param_sampling = RandomParameterSampling( {\n",
    "    '--n_estimators': choice(range(50, 500)),\n",
    "    '--max_depth': choice(range(5, 19)),\n",
    "    '--max_features': uniform(0.2, 1.0)\n",
    "    }\n",
    ")   \n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n",
    "                                         hyperparameter_sampling=param_sampling, \n",
    "                                         primary_metric_name='Accuracy',\n",
    "                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                         max_total_runs=50,\n",
    "                                         max_concurrent_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will launch the RAPIDS training script with parameters that were specified in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = experiment.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor HyperDrive runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor and view the progress of the machine learning training run with a [Jupyter widget](https://docs.microsoft.com/en-us/python/api/azureml-widgets/azureml.widgets?view=azure-ml-py).The widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ca7c65d21ba41bd9c54bd1c8711f5c9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "RunId: HD_696d3b9d-839f-4409-9d4d-7c024e7be2e2\nWeb View: https://ml.azure.com/experiments/train_rapids/runs/HD_696d3b9d-839f-4409-9d4d-7c024e7be2e2?wsid=/subscriptions/73612009-b37b-413f-a3f7-ec02f12498cf/resourcegroups/RAPIDS-HPO-Nanthini/workspaces/HPO-Workspace-Nanthini\n\nStreaming azureml-logs/hyperdrive.txt\n=====================================\n\n\"<START>[2020-05-07T03:28:52.730279][API][INFO]Experiment created<END>\\n\"\"<START>[2020-05-07T03:28:53.390289][GENERATOR][INFO]Trying to sample '5' jobs from the hyperparameter space<END>\\n\"\"<START>[2020-05-07T03:28:54.330083][GENERATOR][INFO]Successfully sampled '5' jobs, they will soon be submitted to the execution target.<END>\\n\"<START>[2020-05-07T03:28:55.5823785Z][SCHEDULER][INFO]The execution environment is being prepared. Please be patient as it can take a few minutes.<END>\n\nExecution Summary\n=================\nRunId: HD_696d3b9d-839f-4409-9d4d-7c024e7be2e2\nWeb View: https://ml.azure.com/experiments/train_rapids/runs/HD_696d3b9d-839f-4409-9d4d-7c024e7be2e2?wsid=/subscriptions/73612009-b37b-413f-a3f7-ec02f12498cf/resourcegroups/RAPIDS-HPO-Nanthini/workspaces/HPO-Workspace-Nanthini\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'runId': 'HD_696d3b9d-839f-4409-9d4d-7c024e7be2e2',\n 'target': 'gpu-cluster',\n 'status': 'Completed',\n 'startTimeUtc': '2020-05-07T03:28:52.505906Z',\n 'endTimeUtc': '2020-05-07T05:48:20.549323Z',\n 'properties': {'primary_metric_config': '{\"name\": \"Accuracy\", \"goal\": \"maximize\"}',\n  'resume_from': 'null',\n  'runTemplate': 'HyperDrive',\n  'azureml.runsource': 'hyperdrive',\n  'platform': 'AML',\n  'ContentSnapshotId': '599d2031-4d05-47da-836b-6d92ffb157ca',\n  'score': '0.9670767188072205',\n  'best_child_run_id': 'HD_696d3b9d-839f-4409-9d4d-7c024e7be2e2_0',\n  'best_metric_status': 'Succeeded'},\n 'inputDatasets': [],\n 'logFiles': {'azureml-logs/hyperdrive.txt': 'https://hpoworkspacena5334546303.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_696d3b9d-839f-4409-9d4d-7c024e7be2e2/azureml-logs/hyperdrive.txt?sv=2019-02-02&sr=b&sig=jWaiCIC2kscq0gVGTySHnPvo0zINEXYHmJIsSVBIKtI%3D&st=2020-05-07T05%3A38%3A26Z&se=2020-05-07T13%3A48%3A26Z&sp=r'}}"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperdrive_run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and register best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['--data_dir', '$AZUREML_DATAREFERENCE_0be6a9d7a78346a29d252b707be571e5', '--n_bins', '32', '--max_depth', '18', '--max_features', '0.631046680467573', '--n_estimators', '117']\n"
    }
   ],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print(best_run.get_details()['runDefinition']['arguments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the model files uploaded during the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['azureml-logs/55_azureml-execution-tvmps_01e287d63e76dd6832da52a63724415fa599e07ccb5d0814c8137e08f5e9a4dc_d.txt', 'azureml-logs/65_job_prep-tvmps_01e287d63e76dd6832da52a63724415fa599e07ccb5d0814c8137e08f5e9a4dc_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_01e287d63e76dd6832da52a63724415fa599e07ccb5d0814c8137e08f5e9a4dc_d.txt', 'azureml-logs/process_info.json', 'azureml-logs/process_status.json', 'logs/azureml/209_azureml.log', 'logs/azureml/job_prep_azureml.log', 'logs/azureml/job_release_azureml.log']\n"
    }
   ],
   "source": [
    "print(best_run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the folder (and all files in it) as a model named `train-rapids` under the workspace for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = best_run.register_model(model_name='train-rapids', model_path='outputs/model-rapids.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the cluster\n",
    "# gpu_cluster.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}