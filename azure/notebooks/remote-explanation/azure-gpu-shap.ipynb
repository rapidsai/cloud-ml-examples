{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Intepretability using GPU SHAP on Azure\n",
    "\n",
    "[Model Interpretability](https://christophm.github.io/interpretable-ml-book/interpretability.html) aids in human understanding of the reasons behind a decision by a Machine Learning model. This can help data scientists understand models better and thus, lead to better solutions. [Shapley Values](https://christophm.github.io/interpretable-ml-book/shapley.html) is one way to explain models and in this notebook, we demonstrate model interpretability on Azure using [cuML GPU SHAP](https://docs.rapids.ai/api/cuml/stable/api.html#model-explainability).\n",
    "\n",
    "To run the example on Azure, we'll use the [azure-interpret](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability) package that will internally use [interpret-community](https://github.com/interpretml/interpret-community) package which has GPU SHAP support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# apt-get update && \\\n",
    "# apt-get install -y fuse && \\\n",
    "# apt-get install -y build-essential && \\\n",
    "# apt-get install -y python3-dev && \\\n",
    "# pip install azureml-core && \\\n",
    "# pip install azureml-interpret && \\\n",
    "# pip install -e git+https://github.com/interpretml/interpret-community.git#egg=interpret_community\\&subdirectory=python && \\\n",
    "# pip install raiwidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to set up the environment, and to load and initialize the workspace.\n",
    "\n",
    "1. Follow instructions on [README.md](https://github.com/rapidsai/cloud-ml-examples/blob/main/azure/README.md) for creating the Machine Learning Workspace on Azure. Place the `config.json` file in the same folder and skip to <b>Load Workspace from Config</b>\n",
    "\n",
    "2. Alternatively, you can use the following 2 cells to load an existing or create a new Workspace by updating the `subscription_id`, `resource_group`, `workspace_name` and `region`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment if you're using second method\n",
    "# import os\n",
    "\n",
    "# subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"<subscription_ID>\")\n",
    "# resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"RAPIDS-SHAP\")\n",
    "# workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"azure-intepret\")\n",
    "# workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"eastus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment if you're using second method\n",
    "# from azureml.core import Workspace\n",
    "\n",
    "# try:\n",
    "#     ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "#     # write the details of the workspace to a configuration file to the notebook library\n",
    "#     ws.write_config()\n",
    "#     print(\"Workspace configuration succeeded.\")\n",
    "# except:\n",
    "#     print(\"Workspace not accessible. Creting new workspace...\")\n",
    "#     from azureml.core import Workspace\n",
    "\n",
    "#     # Create the workspace using the specified parameters\n",
    "#     ws = Workspace.create(name = workspace_name,\n",
    "#                           subscription_id = subscription_id,\n",
    "#                           resource_group = resource_group, \n",
    "#                           location = workspace_region,\n",
    "#                           create_resource_group = True,\n",
    "#                           exist_ok = True)\n",
    "#     ws.get_details()\n",
    "\n",
    "#     # write the details of the workspace to a configuration file to the notebook library\n",
    "#     ws.write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Workspace from Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "print(\"Default datastore's name: {}\".format(datastore.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Experiment\n",
    "\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'gpu-shap-on-amlcompute'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision a compute target\n",
    "\n",
    "You can provision an AmlCompute resource by simply defining two parameters thanks to smart defaults. By default it autoscales from 0 nodes and provisions dedicated VMs to run your job in a container. This is useful when you want to continously re-use the same target, debug it between jobs or simply share the resource with other users of your workspace.\n",
    "\n",
    "* `vm_size`: VM family of the nodes provisioned by AmlCompute. RAPIDS requires NVIDIA Pascal or newer architecture, you will need to specify compute targets from one of `NC_v2`, `NC_v3`, `ND` or `ND_v2` [GPU virtual machines in Azure](https://docs.microsoft.com/en-us/azure/virtual-machines/sizes-gpu); these are VMs that are provisioned with P40 and V100 GPUs. Let's create an `AmlCompute` cluster of `Standard_NC6s_v3` GPU VMs\n",
    "* `max_nodes`: Maximum nodes to autoscale to while running a job on AmlCompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "gpu_cluster_name = 'gpu-cluster'\n",
    "\n",
    "if gpu_cluster_name in ws.compute_targets:\n",
    "    gpu_cluster = ws.compute_targets[gpu_cluster_name]\n",
    "    if gpu_cluster and type(gpu_cluster) is AmlCompute:\n",
    "        print('Found compute target. Will use {0} '.format(gpu_cluster_name))\n",
    "else:\n",
    "    print('creating new cluster')\n",
    "    # m_size parameter below could be modified to one of the RAPIDS-supported VM types\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = 'Standard_NC6s_v3',\n",
    "                                                                max_nodes = 1,\n",
    "                                                                idle_seconds_before_scaledown = 300,\n",
    "                                                                vm_priority = \"lowpriority\")\n",
    "    # Use VM types with more than one GPU for multi-GPU option, e.g. Standard_NC12s_v3\n",
    "    \n",
    "    # create the cluster\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    gpu_cluster.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "# use get_status() to get a detailed status for the current cluster \n",
    "print(gpu_cluster.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Custom Docker Image\n",
    "\n",
    "We'll set up using a custom Docker Image using the `Environment` class. This can be used to install other packages necessary for the run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "environment_name = \"rapids\"\n",
    "\n",
    "env = Environment(environment_name)\n",
    "env.docker.enabled = True\n",
    "env.docker.base_image = None\n",
    "#Installing interpret-community from source for now, will update later\n",
    "env.docker.base_dockerfile = \"\"\"\n",
    "FROM rapidsai/rapidsai:21.06-cuda11.0-runtime-ubuntu18.04-py3.8\n",
    "RUN apt-get update && \\\n",
    "apt-get install -y fuse && \\\n",
    "apt-get install -y build-essential && \\\n",
    "apt-get install -y python3-dev && \\\n",
    "source activate rapids && \\\n",
    "pip install azureml-defaults && \\\n",
    "pip install azureml-interpret && \\\n",
    "pip install -e git+https://github.com/interpretml/interpret-community.git#egg=interpret_community\\&subdirectory=python && \\\n",
    "pip install azureml-telemetry\n",
    "\"\"\"\n",
    "env.python.user_managed_dependencies = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create project directory\n",
    "\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on.\n",
    "\n",
    "The training script `train_explain.py` is already created for you. We'll move this to the project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './scripts'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "shutil.copy('train_explain.py', project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder, \n",
    "                      script='train_explain.py',\n",
    "                      compute_target=gpu_cluster,\n",
    "                      environment=env) \n",
    "run = experiment.submit(config=src)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Shows output of the run on stdout.\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download \n",
    "1. Download model explanation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "# Get model explanation data\n",
    "client = ExplanationClient.from_run(run)\n",
    "global_explanation = client.download_model_explanation()\n",
    "local_importance_values = global_explanation.local_importance_values\n",
    "expected_values = global_explanation.expected_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you can use the saved run.id to retrive the feature importance values\n",
    "client = ExplanationClient.from_run_id(ws, experiment_name, run.id)\n",
    "global_explanation = client.download_model_explanation()\n",
    "local_importance_values = global_explanation.local_importance_values\n",
    "expected_values = global_explanation.expected_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top k (e.g., 4) most important features with their importance values\n",
    "global_explanation_topk = client.download_model_explanation(top_k=4)\n",
    "global_importance_values = global_explanation_topk.get_ranked_global_values()\n",
    "global_importance_names = global_explanation_topk.get_ranked_global_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('global importance values: {}'.format(global_importance_values))\n",
    "print('global importance names: {}'.format(global_importance_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model for visualization and deployment\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "original_model = Model(ws, 'model_explain_model_on_amlcomp')\n",
    "model_path = original_model.download(exist_ok=True)\n",
    "original_model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Download test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve x_test for visualization\n",
    "import joblib\n",
    "x_test_path = './x_test.pkl'\n",
    "run.download_file('x_test_higgs.pkl', output_file_path=x_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = joblib.load('x_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Load the visualization dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_community.widget import ExplanationDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "ExplanationDashboard(global_explanation, original_model,\n",
    "                     datasetX=cp.asnumpy(x_test.values[:50]))"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "mesameki"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
