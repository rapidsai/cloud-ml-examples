{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-node multi-GPU example on Azure using dask-cloudprovider\n",
    "\n",
    "[Dask Cloud Provider](https://cloudprovider.dask.org/en/latest/) is a native cloud intergration for dask. It helps manage Dask clusters on different cloud platforms. In this notebook, we will look at how we can use the package to set-up a Azure cluster and run a multi-node, multi-GPU example with [RAPIDS](https://rapids.ai/). RAPIDS provides a suite of libraries to accelerate data science pipelines on the GPU entirely. This can be scaled to multiple nodes using Dask as we will see through this notebook. \n",
    "\n",
    "For the purposes of this demo, we will use a part of the [NYC Taxi Dataset(Yellow Taxi) from Azure Open Datasets](https://docs.microsoft.com/en-us/azure/open-datasets/dataset-taxi-yellow?tabs=azureml-opendatasets). The goal is to predict the fare amount for a given trip given the times and coordinates of the taxi trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the notebook, run the following commands in the terminal to setup Azure CLI\n",
    "```\n",
    "pip install azure-cli\n",
    "az login\n",
    "```\n",
    "And follow the instructions on the prompt to finish setting up the account.\n",
    "\n",
    "The list of packages needed for this notebook is listed in the cell below - uncomment and run the cell to set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install \"dask-cloudprovider[azure]\"\n",
    "# !pip install azureml-core\n",
    "\n",
    "# # Run the statements below one after the other in order.\n",
    "# !pip install azureml-opendatasets\n",
    "# !pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime\n",
    "from math import asin, cos, pi, sin, sqrt\n",
    "\n",
    "import cudf\n",
    "import dask\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "\n",
    "# This is a package in preview.\n",
    "from azureml.opendatasets import NycTlcYellow\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from cuml.dask.ensemble import RandomForestRegressor\n",
    "from cuml.metrics import mean_squared_error\n",
    "from dask.distributed import Client, wait\n",
    "from dask_cloudprovider.azure import AzureVMCluster\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure cluster set up\n",
    "\n",
    "Let us now setup the [Azure cluster](https://cloudprovider.dask.org/en/latest/azure.html) using `AzureVMCluster` from Dask Cloud Provider. To do this, you'll first need to set up a Resource Group, a Virtual Network and a Security Group on Azure. [Learn more about how you can set this up](https://cloudprovider.dask.org/en/latest/azure.html#resource-groups). Note that you can also set it up using the Azure portal directly.\n",
    "\n",
    "Once you have set it up, you can now plug in the names of the entities you have created in the cell below. Finally note that we use the RAPIDS docker image to build the VM and use the `dask_cuda.CUDAWorker` to run within the VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"SOUTH CENTRAL US\"\n",
    "resource_group = \"RAPIDS-MNMG\"\n",
    "vnet = \"dask-vnet\"\n",
    "security_group = \"dask-nsg\"\n",
    "\n",
    "vm_size = \"Standard_NC12s_v3\"\n",
    "docker_image = \"rapidsai/rapidsai:21.06-cuda11.0-runtime-ubuntu18.04-py3.8\"\n",
    "docker_args = '--shm-size=256m'\n",
    "worker_class = \"dask_cuda.CUDAWorker\"\n",
    "worker_options = {'rmm-managed-memory':True}\n",
    " \n",
    "n_workers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Azure Marketplace VM\n",
    "\n",
    "We'll use [NVIDIA GPU-Optimized Image for AI and HPC](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/nvidia.ngc_azure_17_11?tab=overview) VM from the Azure Marketplace.  This is a customized image that has all the necessary dependencies and NVIDIA drivers preinstalled. \n",
    "\n",
    "This step might require user to accept [Azure Marketplace Image Terms](https://docs.microsoft.com/en-us/cli/azure/vm/image/terms?view=azure-cli-latest). To accept the Terms, run the following command in a cell\n",
    "```python\n",
    "! az vm image terms accept --urn \"nvidia:ngc_azure_17_11:ngc-base-version-21-02-2:21.02.2\" --verbose\n",
    "```\n",
    "\n",
    "_Note: This requires `dask-cloudprovider>=2021.6.0`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"logging.distributed\": \"info\",\n",
    "                 \"cloudprovider.azure.azurevm.marketplace_plan\": {\n",
    "                     \"publisher\": \"nvidia\",\n",
    "                     \"name\": \"ngc-base-version-21-02-2\",\n",
    "                     \"product\": \"ngc_azure_17_11\",\n",
    "                     \"version\": \"21.02.2\"\n",
    "                }})\n",
    "vm_image = \"\"\n",
    "config = dask.config.get(\"cloudprovider.azure.azurevm\", {})\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cluster = AzureVMCluster(\n",
    "    location=location,\n",
    "    resource_group=resource_group,\n",
    "    vnet=vnet,\n",
    "    security_group=security_group,\n",
    "    vm_image=vm_image,\n",
    "    vm_size=vm_size,\n",
    "    docker_image=docker_image,\n",
    "    worker_class=worker_class,\n",
    "    n_workers=n_workers,\n",
    "    security=True,\n",
    "    docker_args=docker_args,\n",
    "    worker_options=worker_options,\n",
    "    debug=False,\n",
    "    bootstrap=False, # This is to prevent the cloud init jinja2 script from running in the custom VM.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup\n",
    "\n",
    "The data needs to be cleaned up before it can be used in a meaningful way. We verify the columns we need are present in appropriate datatypes to make it ready for computation using cuML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    " 'tpepPickupDateTime': 'datetime64[ms]',\n",
    " 'tpepDropoffDateTime': 'datetime64[ms]',\n",
    " 'passengerCount': 'int32',\n",
    " 'tripDistance': 'float32',\n",
    " 'startLon': 'float32',\n",
    " 'startLat': 'float32',\n",
    " 'rateCodeId': 'int32',\n",
    " 'endLon': 'float32',\n",
    " 'endLat': 'float32',\n",
    " 'fareAmount': 'float32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df_part, must_haves):\n",
    "    \"\"\"\n",
    "    This function performs the various clean up tasks for the data\n",
    "    and returns the cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col, axis=1)\n",
    "            continue\n",
    "\n",
    "        # fixes datetime error found by Ty Mckercher and fixed by Paul Mahler\n",
    "        if df_part[col].dtype == 'object' and col in ['tpepPickupDateTime', 'tpepDropoffDateTime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "\n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Interesting Features\n",
    "\n",
    "We'll add new features by making use of \"uder defined functions\" on the dataframe. We'll make use of [apply_rows](https://docs.rapids.ai/api/cudf/stable/api.html#cudf.core.dataframe.DataFrame.apply_rows), which is similar to Pandas' apply funciton. `apply_rows` operation is [JIT compiled by numba](https://numba.pydata.org/numba-doc/dev/cuda/kernels.html) into GPU kernels. \n",
    "\n",
    "The kernels we define are - \n",
    "1. Haversine distance: This is used for calculating the total trip distance.\n",
    "\n",
    "2. Day of the week: This can be useful information for determining the fare cost.\n",
    "\n",
    "`add_features` function combined the two to produce a new dataframe that has the added features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance_kernel(startLat, startLon, endLat, endLon, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(startLat, startLon, endLat, endLon,)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] <3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m*2.6) + y + (y//4) + (c//4) -2*c)%7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['tpepPickupDateTime'].dt.hour\n",
    "    df['year'] = df['tpepPickupDateTime'].dt.year\n",
    "    df['month'] = df['tpepPickupDateTime'].dt.month\n",
    "    df['day'] = df['tpepPickupDateTime'].dt.day\n",
    "    df['diff'] = df['tpepDropoffDateTime'].astype('int32') - df['tpepPickupDateTime'].astype('int32')\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['startLat']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['startLon']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['endLat']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['endLon']//.01*.01\n",
    "    \n",
    "    df = df.drop('tpepDropoffDateTime', axis=1)\n",
    "    df = df.drop('tpepPickupDateTime', axis =1)\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                   incols=['startLat', 'startLon', 'endLat', 'endLon'],\n",
    "                   outcols=dict(h_distance=np.float32),\n",
    "                   kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                      incols=['day', 'month', 'year'],\n",
    "                      outcols=dict(day_of_week=np.float32),\n",
    "                      kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week']<2)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_workers(client, n_workers, n_gpus_per_worker, timeout=300):\n",
    "    import time\n",
    "    client.cluster.scale(n_workers)\n",
    "    m = len(client.has_what().keys())    \n",
    "    start = end = time.perf_counter_ns()\n",
    "    while ((m != n_workers*n_gpus_per_worker) and (((end - start) / 1e9) < timeout) ):\n",
    "        time.sleep(5)\n",
    "        m = len(client.has_what().keys())\n",
    "        end = time.perf_counter_ns()\n",
    "    if (((end - start) / 1e9) >= timeout):\n",
    "        raise RuntimeError(f\"Failed to rescale cluster in {timeout} sec.\"\n",
    "              \"Try increasing timeout for very large containers, and verify available compute resources.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client set up\n",
    "\n",
    "The cells below create a [Dask Client](https://distributed.dask.org/en/latest/client.html) with the cluster we defined earlier in the notebook accessing the Azure VM. Note that we have to scale the cluster and for doing that we'll use the `scale_workers` function. This is the step where the workers are allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "# Scale workers and wait for workers to be up and running\n",
    "# Number of GPUs per node for the VM we've spun up is 2\n",
    "scale_workers(client, n_workers, 2, timeout=600) # Run this just once per cluster\n",
    "client.wait_for_workers(n_workers)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Workflow\n",
    "\n",
    "Once workers become available, we can now run the rest of our workflow:\n",
    "\n",
    "- read and clean the data\n",
    "- add features\n",
    "- split into training and validation sets\n",
    "- fit a RF model\n",
    "- predict on the validation set\n",
    "- compute RMSE\n",
    "\n",
    "Note that for better performance we should perform HPO ideally. \n",
    "\n",
    "Refer to the notebooks in the repository for how to perform automated HPO [using RayTune](https://github.com/rapidsai/cloud-ml-examples/blob/main/ray/notebooks/Ray_RAPIDS_HPO.ipynb) and [using Optuna](https://github.com/rapidsai/cloud-ml-examples/blob/main/optuna/notebooks/optuna_rapids.ipynb).\n",
    "\n",
    "Let's get started by reading the data into the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = parser.parse('2018-06-01')\n",
    "start_date = parser.parse('2018-05-01')\n",
    "nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "nyc_tlc_df = nyc_tlc.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data locally to see what we're dealing with. We see that there are columns for pickup and dropoff times, distance, along with latitude, longitude, etc. These are the information we'll use to estimate the trip fare amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_tlc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pandas dataframe, we'll convert it into dask_cudf dataframe to distibute it across all available dask workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned before, our VMs each have 2 GPUs, so we will partition among n_workers*2\n",
    "df = dask_cudf.from_cudf(cudf.from_pandas(nyc_tlc_df), npartitions=n_workers * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step cleans up the data with the functions defined earlier, adds new features and split it for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the dataframe to clean up the outliers \n",
    "df = clean(df, must_haves)\n",
    "\n",
    "# Add new features\n",
    "taxi_df = df.map_partitions(add_features)\n",
    "\n",
    "taxi_df = taxi_df.dropna()\n",
    "taxi_df = taxi_df.astype(\"float32\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "X, y = taxi_df.drop([\"fareAmount\"], axis=1), taxi_df[\"fareAmount\"].astype('float32')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the RandomForestRegressor, we need to persist the data across available workers. There's a function in cuML (`cuml.dask.common.dask_utils.persist_across_workers`) that makes it easier to do this for different dask_cudf dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = client.has_what().keys()\n",
    "X_train, y_train = dask_utils.persist_across_workers(client, [X_train, y_train], workers=workers)\n",
    "\n",
    "cu_dask_rf = RandomForestRegressor(ignore_empty_partitions=True)\n",
    "cu_dask_rf = cu_dask_rf.fit(X_train, y_train)\n",
    "wait(cu_dask_rf.rfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Taxi Fares using the trained model and get the RMSE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cu_dask_rf.predict(X_test)\n",
    "score = mean_squared_error(y_pred.compute().to_array(), y_test.compute().to_array())\n",
    "print(\"Workflow Complete - RMSE: \", np.sqrt(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up\n",
    "\n",
    "Close out the client and cluster.\n",
    "\n",
    "Note: Do not forget to delete the Network Security Group and Virtual Network created too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
