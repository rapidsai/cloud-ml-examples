{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#8735fb; font-size:24pt\"> Multi Node Multi-GPU example on Kubernetes </span>\n",
    "\n",
    "[Dask Kubernetes](https://kubernetes.dask.org/en/latest/) is a library that extends a single machine multi-CPU [LocalCluster](https://docs.dask.org/en/latest/setup/single-distributed.html#localcluster) or a single machine multi-GPU [LocalCUDACluster](https://docs.rapids.ai/api/dask-cuda/nightly/api.html#cluster) for use in workloads that can leverage multiple nodes and GPUs using Kubernetes.    \n",
    "\n",
    "In this notebook, we explore how we can leverage multiple GPUs in multiple nodes using XGBoost and Kubernetes on Azure. \n",
    "We will further explore how we can use [Forest Inference Library (FIL)](https://docs.rapids.ai/api/cuml/stable/api.html?highlight=forestinference#cuml.ForestInference) of cuml to do inference. \n",
    "\n",
    "For the purposes of this demo, we will use a part of the NYC Taxi Dataset (only the files of 2014 calendar year will be used here). The goal is to predict the fare amount for a given trip given the times and coordinates of the taxi trip.\n",
    "We will download the data from [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/), where the dataset is publicly hosted by Microsoft. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:22pt\"> Step -1: Start a Kubernetes cluster in a cloud environment using you favourite cloud provider. We use AKS in this notebook.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going ahead with the rest of the notebook, please follow the steps provided in the [Detailed Setup Guide](./Detailed_setup_guide.md) to setup [Azure Kubernetes Services (AKS)](https://azure.microsoft.com/en-us/services/kubernetes-service/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following and install some libraries at the beginning. \n",
    "# # If azureml is not present install azureml-core. \n",
    "# # Further opendatasets is in preview mode hence it is not available in the main sdk and needs to be installed separately.\n",
    "# # Installing azureml-opendatasets with for some weird reason installs an old version of Pandas 1.0.0, which we will upgrade to 1.2.4 to work with RAPIDS. \n",
    "# # We do the same for the numpy and scipy libraries that are also downgraded by azureml-opendatasets.\n",
    "# ! pip install azureml-core\n",
    "# ! pip install azureml-opendatasets\n",
    "# ! pip install azureml-telemetry\n",
    "# ! pip install pandas==1.2.4 # reverting pandas to 1.2.4\n",
    "# ! pip install numpy==1.20.2 # reverting numpy to 1.20.2\n",
    "# ! pip install scipy==1.6.0 # reverting scipy to 1.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 0: Import necessary libraries </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, WorkerPlugin, wait, progress, get_worker\n",
    "from dask_kubernetes import KubeCluster, make_pod_from_dict\n",
    "import dask_cudf\n",
    "from azureml.opendatasets import NycTlcYellow\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from cuml.metrics import mean_squared_error\n",
    "from cuml import ForestInference\n",
    "import cudf\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import certifi\n",
    "import dask\n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 1: Set up the Kubernetes CUDA Cluster using dask-kubernetes </span>\n",
    "\n",
    "Spawn the workers and scheduler modules in the kubernetes cluster using customized pod specifications from `yaml` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTimer:\n",
    "    def __init__(self):\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.elapsed = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter_ns()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time.perf_counter_ns()\n",
    "        self.elapsed = self.end - self.start\n",
    "\n",
    "        \n",
    "def create_pod_from_yaml(yaml_file):\n",
    "    with open(yaml_file, 'r') as reader:\n",
    "        d = yaml.safe_load(reader)\n",
    "        d = dask.config.expand_environment_variables(d)\n",
    "    return make_pod_from_dict(d)\n",
    "\n",
    "\n",
    "def build_worker_and_scheduler_pods(sched_spec, worker_spec):\n",
    "    assert os.path.isfile(sched_spec)\n",
    "    assert os.path.isfile(worker_spec)\n",
    "\n",
    "    sched_pod = create_pod_from_yaml(sched_spec)\n",
    "    worker_pod = create_pod_from_yaml(worker_spec)\n",
    "\n",
    "    return sched_pod, worker_pod\n",
    "\n",
    "def scale_workers(client, n_workers, timeout=300):\n",
    "    client.cluster.scale(n_workers)\n",
    "    \n",
    "    m = len(client.has_what().keys())    \n",
    "    start = end = time.perf_counter_ns()\n",
    "    while ((m != n_workers) and (((end - start) / 1e9) < timeout) ):\n",
    "        time.sleep(5)\n",
    "        m = len(client.has_what().keys())\n",
    "        \n",
    "        end = time.perf_counter_ns()\n",
    "        \n",
    "    if (((end - start) / 1e9) >= timeout):\n",
    "        raise RuntimeError(f\"Failed to rescale cluster in {timeout} sec.\"\n",
    "              \"Try increasing timeout for very large containers, and verify available compute resources.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"logging.kubernetes\": \"info\",\n",
    "                 \"logging.distributed\": \"info\",\n",
    "                 \"kubernetes.scheduler-service-type\": \"LoadBalancer\",\n",
    "                 \"kubernetes.idle-timeout\": None,\n",
    "                 \"kubernetes.scheduler-service-wait-timeout\": 3600,\n",
    "                 \"kubernetes.deploy-mode\": \"remote\",\n",
    "                 \"kubernetes.logging\": \"info\",\n",
    "                 \"distributed.logging\": \"info\",\n",
    "                 \"distributed.scheduler.idle-timeout\": None,\n",
    "                 \"distributed.scheduler.locks.lease-timeout\": None,\n",
    "                 \"distributed.comm.timeouts.connect\": 3600,\n",
    "                 \"distributed.comm.tls.ca-file\": certifi.where()})\n",
    "\n",
    "sched_spec_path = \"./podspecs/azure/scheduler-specs.yml\"\n",
    "worker_spec_path = \"./podspecs/azure/cuda-worker-specs.yml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our custom pod specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched_pod, worker_pod = build_worker_and_scheduler_pods(sched_spec=sched_spec_path,\n",
    "                                                        worker_spec=worker_spec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KubeCluster(pod_template=worker_pod,\n",
    "                      scheduler_pod_template=sched_pod)\n",
    "\n",
    "client = Client(cluster)\n",
    "scheduler_address = cluster.scheduler_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will probably take around 3 to 4 minutes. Hang tight !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale up workers\n",
    "\n",
    "This may also take a few minutes. You may want to keep a liberal timeout to allow the system finish spawning the worker pods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_workers(client, 4, timeout=600)\n",
    "npartitions = len(client.has_what().keys())\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(scheduler_dict):\n",
    "    print(f\"All workers for scheduler id: {scheduler_dict['id']}, address: {scheduler_dict['address']}\")\n",
    "    for worker in scheduler_dict['workers']:\n",
    "        print(f\"Worker: {worker} , gpu_machines: {scheduler_dict['workers'][worker]['gpu']}\")\n",
    "\n",
    "pretty_print(client.scheduler_info()) # will show information on the len(CUDA_VISIBLE_DEVICES) partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 2: Data Setup, Cleanup and Enhancement </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 2.a: Set the path for downloading the data from Azureml Opendatasets </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()    \n",
    "start_date = parser.parse('2014-05-01') # lets start at 1st May 2014\n",
    "end_date = parser.parse('2014-05-31') # Lets stop at 31st May 2014\n",
    "nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nyc_tlc_df.shape) # Apprx. 14 Million rows\n",
    "print(type(nyc_tlc_df))\n",
    "print(nyc_tlc_df.head()) \n",
    "# since we are going to send the data to the server let's use the first 10million rows\n",
    "nyc_tlc_df = nyc_tlc_df[:10000000]\n",
    "print(nyc_tlc_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data locally to see what we're dealing with. We will make use of the data from 2014 for the purposes of the demo. We see that there are columns for pickup and dropoff times, distance, along with latitude, longitude, etc. These are the information we'll use to estimate the trip fare amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 2.b: Data Cleanup, Enhancement and Persisting Scripts </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to be cleaned up first. We remove some columns that we are not interested in. We also define the datatypes each of the columns need to be read as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add some new features to our dataframe via some custom functions, namely: \n",
    "1. Haversine distance: This is used for calculating the total trip distance.\n",
    "\n",
    "2. Day of the week: This can be useful information for determining the fare cost.\n",
    "\n",
    "`add_features` function combines the two to produce a new dataframe that has the added features.\n",
    "\n",
    "**NOTE:** We will also persist the test dataset in the workers. If the `X_infer` i.e. the test dataset is small enough, we can call `compute()` on it to bring the test dataset to the local machine and then perform predict on it. But in general, if the `X_infer` is large, it may not fit in the GPU(s) of the local machine. Moreover, moving around a large amount of data will also add to the prediction latency. Therefore it is better to persist the test dataset on the dask workers, and then call the predict functionality on the individual workers. Finally we collect the prediction results from the dask workers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding features functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "\n",
    "def haversine_distance_kernel(pickup_latitude_r, pickup_longitude_r, dropoff_latitude_r, dropoff_longitude_r, h_distance, radius):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude_r, pickup_longitude_r, dropoff_latitude_r, dropoff_longitude_r,)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        # radius = 6371 # Radius of earth in kilometers # currently passed as input arguments\n",
    "        \n",
    "        h_distance[i] = c * radius\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] <3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m*2.6) + y + (y//4) + (c//4) -2*c)%7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['tpepPickupDateTime'].dt.hour\n",
    "    df['year'] = df['tpepPickupDateTime'].dt.year\n",
    "    df['month'] = df['tpepPickupDateTime'].dt.month\n",
    "    df['day'] = df['tpepPickupDateTime'].dt.day\n",
    "    df['diff'] = (df['tpepPickupDateTime'] - df['tpepPickupDateTime']).dt.seconds #convert difference between pickup and dropoff into seconds\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['startLat']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['startLon']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['endLat']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['endLon']//.01*.01\n",
    "    \n",
    "    df = df.drop('tpepDropoffDateTime', axis=1)\n",
    "    df = df.drop('tpepPickupDateTime', axis =1)\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                   incols=['pickup_latitude_r', 'pickup_longitude_r', 'dropoff_latitude_r', 'dropoff_longitude_r'],\n",
    "                   outcols=dict(h_distance=np.float32),\n",
    "                   kwargs=dict(radius=6371))\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                      incols=['day', 'month', 'year'],\n",
    "                      outcols=dict(day_of_week=np.float32),\n",
    "                      kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week']<2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for cleaning and persisting the train and the test data on the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_train_infer_split(client, df, response_dtype, response_id, infer_frac=1.0, random_state=42, shuffle=True):\n",
    "    workers = client.has_what().keys()\n",
    "    X, y = df.drop([response_id], axis=1), df[response_id].astype('float32')\n",
    "    infer_frac = max(0, min(infer_frac, 1.0))\n",
    "    X_train, X_infer, y_train, y_infer = train_test_split(X, y, shuffle=True, random_state=random_state, test_size=infer_frac)\n",
    "    \n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        X_train, y_train = client.persist(\n",
    "            collections=[X_train, y_train]) \n",
    "    \n",
    "    if (infer_frac != 1.0):\n",
    "        with dask.annotate(workers=set(workers)):\n",
    "            X_infer, y_infer = client.persist(\n",
    "                collections=[X_infer, y_infer])\n",
    "\n",
    "        wait([X_train, y_train, X_infer, y_infer])\n",
    "    else:\n",
    "        X_infer = X_train\n",
    "        y_infer = y_train\n",
    "\n",
    "        wait([X_train, y_train])\n",
    "    \n",
    "    return X_train, y_train, X_infer, y_infer\n",
    "\n",
    "\n",
    "def clean(df_part, must_haves):\n",
    "    \"\"\"\n",
    "    This function performs the various clean up tasks for the data\n",
    "    and returns the cleaned dataframe.\n",
    "    \"\"\"    \n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col, axis=1)\n",
    "            continue\n",
    "\n",
    "        # fixes datetime error found by Ty Mckercher and fixed by Paul Mahler\n",
    "        if df_part[col].dtype == 'object' and col in ['tpepPickupDateTime', 'tpepDropoffDateTime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "\n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "            \n",
    "    return df_part\n",
    "\n",
    "    \n",
    "def taxi_data_loader(client, nyc_tlc_df, response_dtype=np.float32, infer_frac=1.0, random_state=0):\n",
    "    #create a list of columns & dtypes the df must have\n",
    "    must_haves = {\n",
    "     'tpepPickupDateTime': 'datetime64[ms]',\n",
    "     'tpepDropoffDateTime': 'datetime64[ms]',\n",
    "     'passengerCount': 'int32',\n",
    "     'tripDistance': 'float32',\n",
    "     'startLon': 'float32',\n",
    "     'startLat': 'float32',\n",
    "     'rateCodeId': 'int32',\n",
    "     'endLon': 'float32',\n",
    "     'endLat': 'float32',\n",
    "     'fareAmount': 'float32'\n",
    "    }\n",
    "\n",
    "    workers = client.has_what().keys()\n",
    "    response_id = 'fareAmount'\n",
    "\n",
    "    taxi_data = dask_cudf.from_cudf(cudf.from_pandas(nyc_tlc_df), npartitions=len(workers))\n",
    "    taxi_data = clean(taxi_data, must_haves)\n",
    "    taxi_data = taxi_data.map_partitions(add_features)\n",
    "    # Drop NaN values and convert to float32\n",
    "    taxi_data = taxi_data.dropna()\n",
    "    fields = ['passengerCount', 'tripDistance', 'startLon', 'startLat', 'rateCodeId',\n",
    "                 'endLon', 'endLat', 'fareAmount', 'diff', 'h_distance', 'day_of_week', 'is_weekend']\n",
    "    taxi_data = taxi_data.astype(\"float32\")\n",
    "    taxi_data = taxi_data[fields]\n",
    "    \n",
    "    return persist_train_infer_split(client, taxi_data, response_dtype, response_id, infer_frac, random_state)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 2.c: Get the Split Data and persist across workers </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a bit of time since the data has to be distributed across multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "X_train, y_train, X_infer, y_infer = taxi_data_loader(client, nyc_tlc_df, infer_frac=0.1, random_state=42)\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for ETL and persisting : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 3: Train a XGBoost Model </span>\n",
    "\n",
    "We are now ready to train a XGBoost model on the data and then predict the fare for each trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the training process, let us take a quick look at the details of the GPUs in the worker pods that we will be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(client.scheduler_info()) # will show information on the len(CUDA_VISIBLE_DEVICES) partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 3.a: Set the training Parameters </span>\n",
    "\n",
    "In this training example, we will use RMSE as the evaluation metric. It is also worth noting that performing HPO will lead to a set of more optimal hyperparameters.\n",
    "\n",
    "Refer to the notebook [HPO-RAPIDS](../notebooks/HPO-RAPIDS.ipynb) in this repository for how to perform HPO on Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.15,\n",
    "    'max_depth': 8,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 1,\n",
    "    'silent': True,\n",
    "    'verbose_eval': True,\n",
    "    'booster' : 'gbtree', # 'gblinear' not implemented in dask\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method':'gpu_hist',\n",
    "    'num_boost_rounds': 100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 3.b: Train the XGBoost Model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is already persisted in the dask workers in the Kubernetes Cluster, the next steps should not take a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "tic = timer()\n",
    "xgboost_output = xgb.dask.train(client, params,data_train, \n",
    "                                    num_boost_round=params['num_boost_rounds'])\n",
    "xgb_gpu_model = xgboost_output['booster']\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Step 3.c: Save the trained model to disk locally </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'trained-model_nyctaxi.xgb'\n",
    "xgb_gpu_model.save_model(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 4: Predict & Score using XGBoost Predict </span>\n",
    "\n",
    "Here we will use the `predict` and `inplace_predict` methods provided by the `xgboost.dask` library, out of the box. Later we will also use [Forest Inference Library (FIL)](https://docs.rapids.ai/api/cuml/stable/api.html?highlight=forestinference#cuml.ForestInference) to perform prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y_test = y_infer.compute()\n",
    "wait(_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = xgb.dask.DaskDMatrix(client, X_infer)\n",
    "tic = timer()\n",
    "y_pred = xgb.dask.predict(client, xgb_gpu_model, d_test)\n",
    "y_pred= y_pred.compute()\n",
    "wait(y_pred)\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for xgb.dask.predict : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference with the inplace predict method of dask XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "y_pred = xgb.dask.inplace_predict(client, xgb_gpu_model, X_infer)\n",
    "y_pred = y_pred.compute()\n",
    "wait(y_pred)\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for inplace inference : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "print(\"Calculating MSE\")\n",
    "score = mean_squared_error(y_pred, _y_test)\n",
    "print(\"Workflow Complete - RMSE: \", np.sqrt(score))\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:22pt\"> Step 5: Predict & Score using Forest Inference Library or FIL</span>\n",
    "\n",
    "[Forest Inference Library (FIL)](https://docs.rapids.ai/api/cuml/stable/api.html?highlight=forestinference#cuml.ForestInference) provides GPU accelerated inference capabilities for tree models. We will import the FIL functionality from [cuML](https://github.com/rapidsai/cuml) library.\n",
    "\n",
    "It accepts a **trained** tree model in a treelite format (currently LightGBM, XGBoost and SKLearn GBDT and random forest models\n",
    "are supported). In general, using FIL allows for faster inference while using a large number of workers, and the latency benefits are more pronounced as the size of the dataset grows large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import ForestInference\n",
    "from dask.distributed import get_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:18pt\"> Predict using `persist` on multiple workers in case the test dataset is large. </span>\n",
    "\n",
    "As noted in *Step 2.b*,  in case the test dataset is huge, it makes sense to call predict individually on the dask workers instead of bringing the entire test dataset to the local machine.\n",
    "\n",
    "To perform prediction individually on the dask workers, each dask worker needs to load the XGB model using FIL. However, the dask workers are remote and do not have access to the locally saved model. Hence we need to send the locally saved XGB model to the dask workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = client.has_what().keys()\n",
    "print(workers)\n",
    "n_workers = len(workers)\n",
    "n_partitions = n_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipFile(zipname):\n",
    "    worker = get_worker()\n",
    "    import zipfile\n",
    "    import os\n",
    "    with zipfile.ZipFile(os.path.join(worker.local_directory, zipname)) as zf:\n",
    "        zf.extractall(worker.local_directory)\n",
    "\n",
    "def checkOrMakeLocalDir():\n",
    "    worker = get_worker()\n",
    "    import os\n",
    "    if not os.path.exists(worker.local_directory):\n",
    "        os.makedirs(worker.local_directory)\n",
    "    \n",
    "def workerModelInit(model_file):   \n",
    "    # this function will run in each worker and initialize the worker \n",
    "    import os\n",
    "    worker = get_worker()\n",
    "    worker.data[\"fil_model\"] = ForestInference.load(filename=os.path.join(worker.local_directory, model_file),model_type='xgboost')\n",
    "    \n",
    "def predict(input_df):\n",
    "    # this function will run in each worker and predict \n",
    "    worker = get_worker()\n",
    "    return worker.data[\"fil_model\"].predict(input_df)\n",
    "\n",
    "def persistModelonWorkers(client, zip_file_name, model_file_name):\n",
    "    import zipfile\n",
    "    zf = zipfile.ZipFile(zip_file_name, mode='w')\n",
    "    zf.write(f\"./{model_file_name}\")\n",
    "    zf.close()\n",
    "    # check to see if local directory present in workers\n",
    "    # if not present make it\n",
    "    fut = client.run(checkOrMakeLocalDir)\n",
    "    wait(fut)\n",
    "    # upload the zip file in workers\n",
    "    fut = client.upload_file(f\"./{zip_file_name}\")\n",
    "    wait(fut)\n",
    "    # unzip file in the workers\n",
    "    fut = client.run(unzipFile, zip_file_name)\n",
    "    wait(fut)\n",
    "    # load model using FIL in workers\n",
    "    fut = client.run(workerModelInit, model_file_name)\n",
    "    wait(fut)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist the local model in the remote dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "persistModelonWorkers(client, \"zipfile_write.zip\", \"trained-model_nyctaxi.xgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference with distributed predict with FIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "predictions = X_infer.map_partitions(predict, meta=\"float\") # this is like MPI reduce\n",
    "y_pred = predictions.compute()\n",
    "wait(y_pred)\n",
    "toc = timer()\n",
    "print(f\"Wall clock time taken for this cell : {toc-tic} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_csv = X_infer.iloc[:,0].shape[0].compute()\n",
    "print(f\"It took {toc-tic} seconds to predict on {rows_csv} rows using FIL distributedly on each worker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timer()\n",
    "score = mean_squared_error(y_pred, _y_test)\n",
    "toc = timer()\n",
    "print(\"Final - RMSE: \", np.sqrt(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#8735fb; font-size:22pt\"> Step 6: Clean up </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
