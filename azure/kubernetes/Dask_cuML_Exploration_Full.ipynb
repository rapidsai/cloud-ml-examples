{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab730f7",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:24pt\"> Multi Node Multi-GPU performance sweeps on Kubernetes </span>\n",
    "\n",
    "Before going ahead with the rest of the notebook, please follow the steps provided in the [Detailed Setup Guide](./Detailed_setup_guide.md) to setup Azure Kubernetes Services (AKS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from functools import partial\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask_cudf\n",
    "\n",
    "from dask_kubernetes import KubeCluster, make_pod_from_dict\n",
    "from dask.distributed import Client, WorkerPlugin, wait, progress\n",
    "\n",
    "\n",
    "class SimpleTimer:\n",
    "    def __init__(self):\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.elapsed = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter_ns()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time.perf_counter_ns()\n",
    "        self.elapsed = self.end - self.start\n",
    "\n",
    "        \n",
    "def create_pod_from_yaml(yaml_file):\n",
    "    with open(yaml_file, 'r') as reader:\n",
    "        d = yaml.safe_load(reader)\n",
    "        d = dask.config.expand_environment_variables(d)\n",
    "    return make_pod_from_dict(d)\n",
    "\n",
    "\n",
    "def build_worker_and_scheduler_pods(sched_spec, worker_spec):\n",
    "    assert os.path.isfile(sched_spec)\n",
    "    assert os.path.isfile(worker_spec)\n",
    "\n",
    "    sched_pod = create_pod_from_yaml(sched_spec)\n",
    "    worker_pod = create_pod_from_yaml(worker_spec)\n",
    "\n",
    "    return sched_pod, worker_pod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"logging.kubernetes\": \"info\",\n",
    "                 \"logging.distributed\": \"info\",\n",
    "                 \"kubernetes.scheduler-service-type\": \"LoadBalancer\",\n",
    "                 \"kubernetes.idle-timeout\": None,\n",
    "                 \"kubernetes.scheduler-service-wait-timeout\": 3600,\n",
    "                 \"kubernetes.deploy-mode\": \"remote\",\n",
    "                 \"kubernetes.logging\": \"info\",\n",
    "                 \"distributed.logging\": \"info\",\n",
    "                 \"distributed.scheduler.idle-timeout\": None,\n",
    "                 \"distributed.scheduler.locks.lease-timeout\": None,\n",
    "                 \"distributed.comm.timeouts.connect\": 3600,\n",
    "                 \"distributed.comm.tls.ca-file\": certifi.where()})\n",
    "\n",
    "sched_spec_path = \"./podspecs/azure/scheduler-specs.yml\"\n",
    "worker_spec_path = \"./podspecs/azure/cuda-worker-specs.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d2bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sched_pod, worker_pod = build_worker_and_scheduler_pods(sched_spec=sched_spec_path,\n",
    "                                                        worker_spec=worker_spec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KubeCluster(pod_template=worker_pod,\n",
    "                      scheduler_pod_template=sched_pod)\n",
    "\n",
    "client = Client(cluster)\n",
    "scheduler_address = cluster.scheduler_address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22985b22",
   "metadata": {},
   "source": [
    "This will probably take around 3 to 4 minutes. Hang tight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6776ef",
   "metadata": {},
   "source": [
    "### Helper function to scale the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_workers(client, n_workers, timeout=300):\n",
    "    client.cluster.scale(n_workers)\n",
    "    \n",
    "    m = len(client.has_what().keys())    \n",
    "    start = end = time.perf_counter_ns()\n",
    "    while ((m != n_workers) and (((end - start) / 1e9) < timeout) ):\n",
    "        time.sleep(5)\n",
    "        m = len(client.has_what().keys())\n",
    "        \n",
    "        end = time.perf_counter_ns()\n",
    "        \n",
    "    if (((end - start) / 1e9) >= timeout):\n",
    "        raise RuntimeError(f\"Failed to rescale cluster in {timeout} sec.\"\n",
    "              \"Try increasing timeout for very large containers, and verify available compute resources.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c825d5f",
   "metadata": {},
   "source": [
    "### Scale the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99038a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_workers(client, 4, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a220e74",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53da5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_worker_pool(client, n_workers, auto_scale=False, timeout=300):\n",
    "    workers = [w for w in client.has_what().keys()]\n",
    "    if (len(workers) < n_workers):\n",
    "        if (auto_scale):\n",
    "            scale_workers(client=client, n_workers=n_workers, timeout=timeout)\n",
    "            workers = [w for w in client.has_what().keys()]\n",
    "        else:\n",
    "            print(\"Attempt to construct worker pool larger than available worker set, and auto_scale is False.\"\n",
    "                  \" Returning entire pool.\")\n",
    "    else:\n",
    "        workers = random.sample(population=workers, k=n_workers)\n",
    "        \n",
    "    return workers\n",
    "\n",
    "\n",
    "def estimate_df_rows(client, files, storage_opts={}, testpct=0.01):\n",
    "    workers = client.has_what().keys()\n",
    "    \n",
    "    est_size = 0\n",
    "    for file in files:\n",
    "        if (file.endswith('.csv')):\n",
    "            df = dask_cudf.read_csv(file, npartitions=len(workers), storage_options=storage_opts)\n",
    "        elif (file.endswith('.parquet')):\n",
    "            df = dask_cudf.read_parquet(file, npartitions=len(workers), storage_options=storage_opts)           \n",
    "        \n",
    "        # Select only the index column from our subsample\n",
    "        est_size += (df.sample(frac=testpct).iloc[:,0].shape[0] / testpct).compute()\n",
    "        del df\n",
    "    \n",
    "    return est_size\n",
    "\n",
    "def pretty_print(scheduler_dict):\n",
    "    print(f\"All workers for scheduler id: {scheduler_dict['id']}, address: {scheduler_dict['address']}\")\n",
    "    for worker in scheduler_dict['workers']:\n",
    "        print(f\"Worker: {worker} , gpu_machines: {scheduler_dict['workers'][worker]['gpu']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc0cc0e",
   "metadata": {},
   "source": [
    "### Taxi Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1a2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df_part, remap, must_haves):\n",
    "    \"\"\"\n",
    "    This function performs the various clean up tasks for the data\n",
    "    and returns the cleaned dataframe.\n",
    "    \"\"\"\n",
    "    tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
    "    df_part = df_part.rename(columns=tmp)\n",
    "    \n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(columns=remap)\n",
    "    \n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col, axis=1)\n",
    "            continue\n",
    "\n",
    "        # fixes datetime error found by Ty Mckercher and fixed by Paul Mahler\n",
    "        if df_part[col].dtype == 'object' and col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "\n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "            \n",
    "    return df_part\n",
    "\n",
    "def coalesce_taxi_data(fraction, random_state):\n",
    "    base_path = 'gs://anaconda-public-data/nyc-taxi/csv'\n",
    "\n",
    "    # list of column names that need to be re-mapped\n",
    "    remap = {}\n",
    "    remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "    remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "    remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "    #create a list of columns & dtypes the df must have\n",
    "    must_haves = {\n",
    "     'pickup_datetime': 'datetime64[ms]',\n",
    "     'dropoff_datetime': 'datetime64[ms]',\n",
    "     'passenger_count': 'int32',\n",
    "     'trip_distance': 'float32',\n",
    "     'pickup_longitude': 'float32',\n",
    "     'pickup_latitude': 'float32',\n",
    "     'rate_code': 'int32',\n",
    "     'dropoff_longitude': 'float32',\n",
    "     'dropoff_latitude': 'float32',\n",
    "     'fare_amount': 'float32'\n",
    "    }\n",
    "    \n",
    "    # apply a list of filter conditions to throw out records with missing or outlier values\n",
    "    query_frags = [\n",
    "        'fare_amount > 0 and fare_amount < 500',\n",
    "        'passenger_count > 0 and passenger_count < 6',\n",
    "        'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "        'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "        'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "        'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    valid_months_2016 = [str(x).rjust(2, '0') for x in range(1, 7)]\n",
    "    valid_files_2016 = [f'{base_path}/2016/yellow_tripdata_2016-{month}.csv' for month in valid_months_2016]\n",
    "    \n",
    "    df_2014_fractional = dask_cudf.read_csv(f'{base_path}/2014/yellow_*.csv', chunksize=25e6).sample(\n",
    "        frac=fraction, random_state=random_state)\n",
    "    df_2014_fractional = clean(df_2014_fractional, remap, must_haves)\n",
    "    \n",
    "    df_2015_fractional = dask_cudf.read_csv(f'{base_path}/2015/yellow_*.csv', chunksize=25e6).sample(\n",
    "        frac=fraction, random_state=random_state)\n",
    "    df_2015_fractional = clean(df_2015_fractional, remap, must_haves)\n",
    "    \n",
    "    df_2016_fractional = dask_cudf.read_csv(valid_files_2016, chunksize=25e6).sample(\n",
    "        frac=fraction, random_state=random_state)\n",
    "    df_2016_fractional = clean(df_2016_fractional, remap, must_haves)\n",
    "    \n",
    "    df_taxi = dask.dataframe.multi.concat([df_2014_fractional, df_2015_fractional, df_2016_fractional])\n",
    "    df_taxi = df_taxi.query(' and '.join(query_frags))\n",
    "    \n",
    "    return df_taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69dc6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_csv_data_loader(client, response_dtype=np.float32, fraction=1.0, random_state=0):\n",
    "    response_id = 'fare_amount'\n",
    "    workers = client.has_what().keys()\n",
    "    km_fields = ['passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code',\n",
    "                 'dropoff_longitude', 'dropoff_latitude', 'fare_amount']\n",
    "    \n",
    "    taxi_df = coalesce_taxi_data(fraction=fraction, random_state=random_state)\n",
    "    \n",
    "    taxi_df = taxi_df[km_fields]\n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        taxi_df = client.persist(collections=taxi_df)\n",
    "    \n",
    "    X = taxi_df[taxi_df.columns.difference([response_id])].astype(np.float32)\n",
    "    y = taxi_df[response_id].astype(response_dtype)\n",
    "    \n",
    "    wait(taxi_df)\n",
    "    \n",
    "    return taxi_df, X, y\n",
    "\n",
    "def taxi_parquet_data_loader(client, response_dtype=np.float32, fraction=1.0, random_state=0):\n",
    "    # list of column names that need to be re-mapped\n",
    "    remap = {}\n",
    "    remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "    remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "    remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "    #create a list of columns & dtypes the df must have\n",
    "    must_haves = {\n",
    "     'pickup_datetime': 'datetime64[ms]',\n",
    "     'dropoff_datetime': 'datetime64[ms]',\n",
    "     'passenger_count': 'int32',\n",
    "     'trip_distance': 'float32',\n",
    "     'pickup_longitude': 'float32',\n",
    "     'pickup_latitude': 'float32',\n",
    "     'rate_code': 'int32',\n",
    "     'dropoff_longitude': 'float32',\n",
    "     'dropoff_latitude': 'float32',\n",
    "     'fare_amount': 'float32'\n",
    "    }\n",
    "\n",
    "    # apply a list of filter conditions to throw out records with missing or outlier values\n",
    "    query_frags = [\n",
    "        'fare_amount > 0 and fare_amount < 500',\n",
    "        'passenger_count > 0 and passenger_count < 6',\n",
    "        'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "        'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "        'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "        'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "    ]\n",
    "\n",
    "    workers = client.has_what().keys()\n",
    "    taxi_parquet_path = \"gs://anaconda-public-data/nyc-taxi/nyc.parquet\"\n",
    "    response_id = 'fare_amount'\n",
    "    fields = ['passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code',\n",
    "                 'dropoff_longitude', 'dropoff_latitude', 'fare_amount']\n",
    "    \n",
    "    taxi_df = dask_cudf.read_parquet(taxi_parquet_path, npartitions=len(workers), chunksize=25e6).sample(\n",
    "        frac=fraction, random_state=random_state)\n",
    "    taxi_df = clean(taxi_df, remap, must_haves)\n",
    "    taxi_df = taxi_df.query(' and '.join(query_frags))\n",
    "    taxi_df = taxi_df[fields]\n",
    "    \n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        taxi_df = client.persist(collections=taxi_df)\n",
    "    \n",
    "    wait(taxi_df)\n",
    "\n",
    "    X = taxi_df[taxi_df.columns.difference([response_id])].astype(np.float32)\n",
    "    y = taxi_df[response_id].astype(response_dtype)\n",
    "        \n",
    "    return taxi_df, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cd72a",
   "metadata": {},
   "source": [
    "### Performance Validation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05338790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def record_elapsed_timings_to_df(df, timings, record_template, type, columns, write_to=None):\n",
    "    records = [dict(record_template, **{\"sample_index\": i,\n",
    "                                        \"elapsed\": elapsed,\n",
    "                                        \"type\": type})\n",
    "                  for i, elapsed in enumerate(timings)]\n",
    "\n",
    "    df = df.append(other=records, ignore_index=True)\n",
    "    \n",
    "    if (write_to):\n",
    "        df.to_csv(write_to, columns=columns) \n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def collect_load_time_samples(load_func, count, return_final_sample=True, verbose=False):\n",
    "    timings = []\n",
    "    for m in tqdm(range(count)):\n",
    "        with SimpleTimer() as timer:\n",
    "            df, X, y = load_func()\n",
    "        timings.append(timer.elapsed)\n",
    "    \n",
    "    if (return_final_sample):\n",
    "        return df, X, y, timings\n",
    "    \n",
    "    return None, None, None, timings\n",
    "\n",
    "\n",
    "def collect_func_time_samples(func, count, verbose=False):\n",
    "    timings = []\n",
    "    for k in tqdm(range(count)):\n",
    "        with SimpleTimer() as timer:\n",
    "            func()\n",
    "        timings.append(timer.elapsed)\n",
    "        \n",
    "    return timings\n",
    "\n",
    "\n",
    "def sweep_fit_func(model, func_id, require_compute, X, y, xy_fit, count):\n",
    "    _fit_func_attr = getattr(model, func_id)\n",
    "    if (require_compute):\n",
    "        if (xy_fit):\n",
    "            fit_func = partial(lambda X, y: _fit_func_attr(X, y).compute(), X, y)\n",
    "        else:\n",
    "            fit_func = partial(lambda X: _fit_func_attr(X).compute(), X)\n",
    "    else:\n",
    "        if (xy_fit):\n",
    "            fit_func = partial(_fit_func_attr, X, y)\n",
    "        else:\n",
    "            fit_func = partial(_fit_func_attr, X)                \n",
    "\n",
    "    return collect_func_time_samples(func=fit_func, count=count)\n",
    "\n",
    "\n",
    "def sweep_predict_func(model, func_id, require_compute, X, count):\n",
    "    _predict_func_attr = getattr(model, func_id)\n",
    "    predict_func = partial(lambda X: _predict_func_attr(X).compute(), X)\n",
    "    \n",
    "    return collect_func_time_samples(func=predict_func, count=count)\n",
    "    \n",
    "\n",
    "def performance_sweep(client, model, data_loader, hardware_type, worker_counts=[1], samples=1, load_samples=1, max_data_frac=1.0,\n",
    "                    predict_frac=0.05, scaling_type='weak', xy_fit=True, fit_requires_compute=False, update_workers_in_kwargs=True,\n",
    "                    response_dtype=np.float32, out_path='./perf_sweep.csv', append_to_existing=False, model_name=None,\n",
    "                    fit_func_id=\"fit\", predict_func_id=\"predict\", scaling_denom=None, model_args={}, model_kwargs={}):\n",
    "    \"\"\"\n",
    "    Primary performance sweep entrypoint.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    client: DASK client associated with the cluster we're interesting in collecting performance data for.\n",
    "    \n",
    "    model: Model object on which to gather performance data. This will be created and destroyed,\n",
    "        once for each element of 'worker_counts'\n",
    "    \n",
    "    data_loader: arbitrary data loading function that will be called to load the appropriate testing data.\n",
    "        Function that is responsible for loading and returning the data to be used for a given performance run. Function\n",
    "        signature must accept (client, fraction, and random_state). Client should be used to distribute data, and loaders\n",
    "        should utilize fraction and random_state with dask's dataframe.sample method to allow for control of how much data\n",
    "        is loaded.\n",
    "        \n",
    "        When called, its return value should be of the form: df, X, y, where df is the full dask_cudf dataframe, X is a\n",
    "        dask_cudf dataframe which contains all explanatory variables that will be passed to the 'fit' function, and y is a\n",
    "        dask_cudf series or dataframe that contains response variables which should be passed to fit/predict as fit(X, y)\n",
    "    \n",
    "    hardware_type: indicates the core hardware the current sweep is running on. ex. 'T4', 'V100', 'A100'\n",
    "    \n",
    "    worker_counts: List indicating the number of workers that should be swept. Ex [1, 2, 4]\n",
    "        worker counts must fit within the cluster associated with 'client', if the current DASK worker count is different\n",
    "        from what is requested on a given sweep, attempt to automatically scale the worker count. NOTE: this does not \n",
    "        mean we will scale the available cluster nodes, just the number of deployed worker pods.\n",
    "    \n",
    "    samples: number of fit/predict samples to record per worker count\n",
    "    \n",
    "    load_samples: number of times to sample data loads. This effectively times how long 'data_loader' runs.\n",
    "    \n",
    "    max_data_frac: maximum fraction of data to return.\n",
    "        Strong scaling: each run will utilize max_data_frac data.\n",
    "        Weak scaling: each run will utilize (current worker count) / (max worker count) * max_data_frac data.\n",
    "        \n",
    "    predict_frac: fraction of training data used to test inference\n",
    "    \n",
    "    scaling_type: values can be 'weak' or 'strong' indicating the type of scaling sweep to perform.\n",
    "    \n",
    "    xy_fit: indicates whether or not the model's 'fit' function is of the form (X, y), when xy_fit is False, we assume that\n",
    "        fit is of the form (X), as is the case with various unsupervised methods ex. KNN.\n",
    "    \n",
    "    fit_requires_compute: False generally, set this to True if the model's 'fit' function requires a corresponding '.compute()'\n",
    "        call to execute the required work.\n",
    "    \n",
    "    update_workers_in_kwargs: Some algorithms accept a 'workers' list, much like DASK, and will require their kwargs to have\n",
    "        workers populated. Setting this flag handles this automatically.\n",
    "        \n",
    "    response_dtype: defaults to np.float32, some algorithms require another dtype, such as int32\n",
    "    \n",
    "    out_path: path where performance data csv should be saved\n",
    "    \n",
    "    append_to_existing: When true, append results to an existing csv, otherwise overwrite.\n",
    "    \n",
    "    model_name: Override what we output as the model name\n",
    "    \n",
    "    fit_func_id: Defaults to 'fit', only set this if the model has a non-standard naming.\n",
    "    \n",
    "    predict_func_id: Defaults to 'predict', only set this if the model has a on-standard predict naming.\n",
    "    \n",
    "    scaling_denom: (weak scaling) defaults to max(workers) if unset. Specifies the maximum worker count that weak scaling\n",
    "        should scale against. For example, when using 1 worker in a weak scaling sweep, the worker will attempt to\n",
    "        process a fraction of the total data equal to 1/scaling_denom\n",
    "    \n",
    "    model_args: args that will be passed to the model's constructor\n",
    "    \n",
    "    model_kwargs: keyword args that will be passed to the model's constructor\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cols = ['n_workers', 'sample_index', 'elapsed', 'type', 'algorithm', 'scaling_type', 'data_fraction', 'hardware']\n",
    "    perf_df = cudf.DataFrame(columns=cols)\n",
    "    if (append_to_existing):\n",
    "        try:\n",
    "            perf_df = cudf.read_csv(out_path)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    model_name = model_name if model_name else str(model)\n",
    "    scaling_denom = scaling_denom if (scaling_denom is not None) else max(worker_counts)\n",
    "    max_data_frac = min(1.0, max_data_frac)\n",
    "\n",
    "    start_msg = f\"Starting {scaling_type}-scaling performance sweep for:\\n\"\n",
    "    start_msg += f\" model      : {model_name}\\n\"\n",
    "    start_msg += f\" data loader: {data_loader}.\\n\"\n",
    "    start_msg += f\"Configuration\\n\"\n",
    "    start_msg += \"==========================\\n\"\n",
    "    start_msg += f\"{'Worker counts':<25} : {worker_counts}\\n\"\n",
    "    start_msg += f\"{'Fit/Predict samples':<25} : {samples}\\n\"\n",
    "    start_msg += f\"{'Data load samples':<25} : {load_samples}\\n\"\n",
    "    start_msg += f\"- {'Max data fraction':<23} : {max_data_frac}\\n\"\n",
    "    start_msg += f\"{'Model fit':<25} : {'X ~ y' if xy_fit else 'X'}\\n\"\n",
    "    start_msg += f\"- {'Response DType':<23} : {response_dtype}\\n\"\n",
    "    start_msg += f\"{'Writing results to':<25} : {out_path}\\n\"\n",
    "    start_msg += f\"- {'Method':<23} : {'overwrite' if not append_to_existing else 'append'}\\n\"\n",
    "    print(start_msg, flush=True)\n",
    "    \n",
    "    for n in worker_counts:        \n",
    "        fraction = (n / scaling_denom) * max_data_frac if scaling_type == 'weak' else max_data_frac\n",
    "        record_template = {\"n_workers\": n, \"type\": \"predict\", \"algorithm\": model_name,\n",
    "               \"scaling_type\": scaling_type, \"data_fraction\": fraction, \"hardware\": hardware_type}\n",
    "        scale_workers(client, n)\n",
    "\n",
    "        print(f\"Sampling <{load_samples}> load times with {n} workers.\", flush=True)\n",
    "        \n",
    "        load_func = partial(data_loader, client=client, response_dtype=response_dtype, fraction=fraction, random_state=0)\n",
    "        df, X, y, load_timings = collect_load_time_samples(load_func=load_func, count=load_samples)\n",
    "        \n",
    "        perf_df = record_elapsed_timings_to_df(df=perf_df, timings=load_timings, type='load',\n",
    "                                                    record_template=record_template, columns=cols, write_to=out_path)\n",
    "\n",
    "        print(f\"Finished loading <{load_samples}>, samples, to <{n}> workers with a mean time of {np.mean(load_timings)/1e9:0.4f} sec.\", flush=True)\n",
    "        print(f\"Sweeping {model_name} '{fit_func_id}' with <{n}> workers. Sampling <{samples}> times.\", flush=True)\n",
    "\n",
    "        if (update_workers_in_kwargs and 'workers' in model_kwargs):\n",
    "            model_kwargs['workers'] = workers = list(client.has_what().keys())\n",
    "        \n",
    "        print(model_args, model_kwargs)\n",
    "        m = model(*model_args, **model_kwargs)       \n",
    "        \n",
    "        if (fit_func_id):\n",
    "            fit_timings = sweep_fit_func(model=m, func_id=fit_func_id,\n",
    "                                             require_compute=fit_requires_compute,\n",
    "                                             X=X, y=y, xy_fit=xy_fit, count=samples)\n",
    "\n",
    "            perf_df = record_elapsed_timings_to_df(df=perf_df, timings=fit_timings, type='fit',\n",
    "                                                        record_template=record_template, columns=cols, write_to=out_path)\n",
    "\n",
    "            print(f\"Finished gathering <{samples}>, 'fit' samples using <{n}> workers, with a mean time of {np.mean(fit_timings)/1e9:0.4f} sec.\",\n",
    "                  flush=True)\n",
    "        else:\n",
    "            print(f\"Skipping fit sweep, fit_func_id is None\")\n",
    "\n",
    "        if (predict_func_id):\n",
    "            print(f\"Sweeping {model_name} '{predict_func_id}' with <{n}> workers. Sampling <{samples}> times.\", flush=True)\n",
    "            predict_timings = sweep_predict_func(model=m, func_id=predict_func_id,\n",
    "                                                     require_compute=True, X=X, count=samples)\n",
    "\n",
    "            perf_df = record_elapsed_timings_to_df(df=perf_df, timings=predict_timings, type='predict',\n",
    "                                                        record_template=record_template, columns=cols, write_to=out_path)\n",
    "            \n",
    "            print(f\"Finished gathering <{samples}>, 'predict' samples using <{n}> workers, with a mean time of {np.mean(predict_timings)/1e9:0.4f} sec.\",\n",
    "                  flush=True)\n",
    "        else:\n",
    "            print(f\"Skipping inference sweep. predict_func_id is None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22e490",
   "metadata": {},
   "source": [
    "### Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d01751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ci(df, fields, groupby):\n",
    "    gbdf = df[fields].groupby(groupby).agg(['mean', 'std', 'count'])   \n",
    "    \n",
    "    ci = (1.96 + gbdf['elapsed']['std'] / np.sqrt(gbdf['elapsed']['count']))\n",
    "    \n",
    "    ci_df = ci.reset_index()\n",
    "    ci_df['ci.low'] = gbdf['elapsed'].reset_index()['mean'] - ci_df[0]\n",
    "    ci_df['ci.high'] = gbdf['elapsed'].reset_index()['mean'] + ci_df[0]\n",
    "    \n",
    "    return ci_df\n",
    "\n",
    "def visualize_csv_data(csv_path):\n",
    "    df = cudf.read_csv(csv_path)\n",
    "    \n",
    "    fields = ['elapsed', 'elapsed_sec', 'type', 'n_workers', 'hardware', 'scaling_type']\n",
    "    groupby = ['n_workers', 'type', 'hardware', 'scaling_type']\n",
    "    df['elapsed_sec'] = df['elapsed']/1e9\n",
    "\n",
    "    ci_df = simple_ci(df, fields, groupby=groupby)\n",
    "\n",
    "    # Rescale to seconds\n",
    "    ci_df[['ci.low', 'ci.high']] = ci_df[['ci.low', 'ci.high']]/1e9\n",
    "\n",
    "    # Print confidence intervals\n",
    "    print(ci_df[['hardware', 'n_workers', 'type', 'ci.low', 'ci.high']][ci_df['type'] != 'load'])\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    sns.set(rc={'figure.figsize':(20, 10)}, font_scale=2)\n",
    "\n",
    "    # Boxplots for elapsed time at each worker count.\n",
    "    plot_df = df[fields][df[fields].type != 'load'].to_pandas()\n",
    "    ax = sns.catplot(data=plot_df, x=\"n_workers\", y=\"elapsed_sec\",\n",
    "                     col=\"type\", row=\"scaling_type\", hue=\"hardware\", kind=\"box\",\n",
    "                     height=8, order=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe332dd",
   "metadata": {},
   "source": [
    "### Taxi Data Configuration (Medium Sized Dataset)\n",
    "We can use the parquet data from the Anaconda public repo here (hosted in google file system). THis will illustrate how much faster it is to read parquet, and gives us around 150 million rows of data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test with Taxi Dataset\n",
    "preload_data = False\n",
    "append_to_existing = True\n",
    "samples = 5\n",
    "load_samples = 1\n",
    "worker_counts = [4]\n",
    "scaling_denom = 4\n",
    "hardware_type = \"V100\"\n",
    "max_data_frac = 0.75\n",
    "scale_type = 'weak' # weak | strong\n",
    "out_prefix = 'taxi_medium'\n",
    "\n",
    "if (not preload_data):\n",
    "    data_loader = taxi_parquet_data_loader\n",
    "else:\n",
    "    data = taxi_parquet_data_loader(client, fraction=max_data_frac)\n",
    "    data_loader = lambda client, response_dtype, fraction, random_state: data\n",
    "    \n",
    "if (not hardware_type):\n",
    "    raise RuntimeError(\"Please specify the hardware type for this run! ex. (T4, V100, A100)\")\n",
    "\n",
    "sweep_kwargs = {\n",
    "    'append_to_existing': append_to_existing,\n",
    "    'samples': samples,\n",
    "    'load_samples': load_samples,\n",
    "    'worker_counts': worker_counts,\n",
    "    'scaling_denom': scaling_denom,\n",
    "    'hardware_type': hardware_type,\n",
    "    'data_loader': data_loader,\n",
    "    'max_data_frac': max_data_frac,\n",
    "    'scaling_type': scale_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c7867",
   "metadata": {},
   "source": [
    "#### It is worth noting that in this notebook we will be using much larger datasets compared to our other [notebook with XGBoost](./MNMG_XGBoost.ipynb) which uses a smaller dataset. The order of rows here is > 150 million. Now, with Azureml Opendatasets, we have to first download the data to the client machine and then distribute it over the workers. This would be slow and communication-wise inefficient for very large datasets. Hence we use the parquets and the csvs publicly hosted by Anaconda in google file system (GCFCS). With the latter endpoints, the workers can directly read the csv/parquet files without having to transfer them over the network from the client machine. You can choose any other public/private endpoints that directly allow reading from parquet or csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_parquet_path = [\"gs://anaconda-public-data/nyc-taxi/nyc.parquet/*.parquet\"]\n",
    "estimated_rows = estimate_df_rows(client, files=taxi_parquet_path, testpct=0.0001)\n",
    "print(estimated_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dcaa79",
   "metadata": {},
   "source": [
    "### Taxi Data Configuration (Large Dataset)\n",
    "The largest dataset we'll work with, contains up to 450 million rows of taxi data, stored as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728de308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to sweep with the large Taxi Dataset\n",
    "# preload_data = True\n",
    "# append_to_existing = True\n",
    "# samples = 5\n",
    "# load_samples = 1\n",
    "# worker_counts = [8]\n",
    "# scaling_denom = 8\n",
    "# hardware_type = \"V100\"\n",
    "# data_loader = taxi_csv_data_loader\n",
    "# max_data_frac = .5\n",
    "# scale_type = 'weak'\n",
    "# out_prefix = 'taxi_large'\n",
    "\n",
    "\n",
    "# if (not preload_data):\n",
    "#     data_loader = taxi_csv_data_loader\n",
    "# else:\n",
    "#     data = taxi_csv_data_loader(client, fraction=max_data_frac)\n",
    "#     data_loader = lambda client, response_dtype, fraction, random_state: data\n",
    "\n",
    "# if (not hardware_type):\n",
    "#     raise RuntimeError(\"Please specify the hardware type for this run! ex. (T4, V100, A100)\")\n",
    "\n",
    "# sweep_kwargs = {\n",
    "#     'append_to_existing': append_to_existing,\n",
    "#     'samples': samples,\n",
    "#     'load_samples': load_samples,\n",
    "#     'worker_counts': worker_counts,\n",
    "#     'scaling_denom': scaling_denom,\n",
    "#     'hardware_type': hardware_type,\n",
    "#     'data_loader': data_loader,\n",
    "#     'max_data_frac': max_data_frac,\n",
    "#     'scaling_type': scale_type\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a8591",
   "metadata": {},
   "source": [
    "### ETL Exploration CSV vs Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ca07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remap = {}\n",
    "remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    " 'pickup_datetime': 'datetime64[ms]',\n",
    " 'dropoff_datetime': 'datetime64[ms]',\n",
    " 'passenger_count': 'int32',\n",
    " 'trip_distance': 'float32',\n",
    " 'pickup_longitude': 'float32',\n",
    " 'pickup_latitude': 'float32',\n",
    " 'rate_code': 'int32',\n",
    " 'dropoff_longitude': 'float32',\n",
    " 'dropoff_latitude': 'float32',\n",
    " 'fare_amount': 'float32'\n",
    "}\n",
    "\n",
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]\n",
    "\n",
    "workers = client.has_what().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc46770",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'gcs://anaconda-public-data/nyc-taxi/csv'\n",
    "\n",
    "with SimpleTimer() as timer_csv:\n",
    "    df_csv_2014 = dask_cudf.read_csv(f'{base_path}/2014/yellow_*.csv', chunksize=25e6)\n",
    "    df_csv_2014 = clean(df_csv_2014, remap, must_haves)\n",
    "    df_csv_2014 = df_csv_2014.query(' and '.join(query_frags))\n",
    "    \n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        df_csv_2014 = client.persist(collections=df_csv_2014)\n",
    "        \n",
    "    wait(df_csv_2014)\n",
    "\n",
    "print(df_csv_2014.columns)\n",
    "rows_csv = df_csv_2014.iloc[:,0].shape[0].compute()\n",
    "print(f\"CSV load took {timer_csv.elapsed/1e9} sec. For {rows_csv} rows of data => {rows_csv/(timer_csv.elapsed/1e9)} rows/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7522f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(df_csv_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2402ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with SimpleTimer() as timer_parquet:\n",
    "    df_parquet = dask_cudf.read_parquet(f'gs://anaconda-public-data/nyc-taxi/nyc.parquet/*', chunksize=25e6)\n",
    "    df_parquet = clean(df_parquet, remap, must_haves)\n",
    "    df_parquet = df_parquet.query(' and '.join(query_frags))\n",
    "    \n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        df_parquet = client.persist(collections=df_parquet)\n",
    "    \n",
    "    wait(df_parquet)\n",
    "\n",
    "print(df_parquet.columns)\n",
    "rows_parquet = df_parquet.iloc[:,0].shape[0].compute()\n",
    "print(f\"Parquet load took {timer_parquet.elapsed/1e9} sec. For {rows_parquet} rows of data => {rows_parquet/(timer_parquet.elapsed/1e9)} rows/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(df_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a17a2",
   "metadata": {},
   "source": [
    "Speedup with 8 V100 nodes should be approximately 2.5-5x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup = (rows_parquet/(timer_parquet.elapsed/1e9))/(rows_csv/(timer_csv.elapsed/1e9))\n",
    "print(speedup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6829f7",
   "metadata": {},
   "source": [
    "## <span style=\"color:#8735fb; font-size:24pt\"> cuML Algorithms Performance Sweeps </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbaf027",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93302a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_kwargs = {\n",
    "    \"workers\": client.has_what().keys(),\n",
    "    \"n_estimators\": 10,\n",
    "    \"max_depth\": 12\n",
    "}\n",
    "rf_csv_path = f\"./{out_prefix}_random_forest_regression.csv\"\n",
    "\n",
    "performance_sweep(client=client, model=RandomForestRegressor,\n",
    "                **sweep_kwargs,\n",
    "                out_path=rf_csv_path,\n",
    "                response_dtype=np.int32,\n",
    "                model_kwargs=rf_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a44580",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_csv_path = f\"./{out_prefix}_random_forest_regression.csv\"\n",
    "visualize_csv_data(rf_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea67b55",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.cluster import KMeans\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    \"client\": client,\n",
    "    \"n_clusters\": 12,\n",
    "    \"max_iter\": 371,\n",
    "    \"tol\": 1e-5,\n",
    "    \"oversampling_factor\": 3,\n",
    "    \"max_samples_per_batch\": 32768/2,\n",
    "    \"verbose\": False,\n",
    "    \"init\": 'random'\n",
    "}\n",
    "kmeans_csv_path = f'./{out_prefix}_kmeans.csv'\n",
    "\n",
    "performance_sweep(client=client, model=KMeans,\n",
    "                **sweep_kwargs,\n",
    "                out_path=kmeans_csv_path,\n",
    "                xy_fit=False,\n",
    "                model_kwargs=kmeans_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_csv_data(kmeans_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d395c73",
   "metadata": {},
   "source": [
    "### Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.neighbors import NearestNeighbors\n",
    "\n",
    "nn_kwargs = {}\n",
    "nn_csv_path = f'./{out_prefix}_nn.csv'\n",
    "\n",
    "performance_sweep(client=client, model=NearestNeighbors,\n",
    "                **sweep_kwargs,\n",
    "                out_path=nn_csv_path,\n",
    "                xy_fit=False,\n",
    "                predict_func_id='get_neighbors',\n",
    "                model_kwargs=nn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d9dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_csv_path = f'./{out_prefix}_nn.csv'\n",
    "visualize_csv_data(nn_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e75e9",
   "metadata": {},
   "source": [
    "### TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ea698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.decomposition import TruncatedSVD\n",
    "\n",
    "tsvd_kwargs = {\n",
    "    \"client\": client,\n",
    "    \"n_components\": 5\n",
    "}\n",
    "tsvd_csv_path = f'./{out_prefix}_tsvd.csv'\n",
    "\n",
    "performance_sweep(client=client, model=TruncatedSVD,\n",
    "                **sweep_kwargs,\n",
    "                out_path=tsvd_csv_path,\n",
    "                xy_fit=False,\n",
    "                fit_requires_compute=True,\n",
    "                fit_func_id=\"fit_transform\",\n",
    "                predict_func_id=None,\n",
    "                model_kwargs=tsvd_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082304b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_csv_data(tsvd_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56003b",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71079df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.linear_model import Lasso as LassoRegression\n",
    "\n",
    "lasso_kwargs = {\n",
    "    \"client\": client\n",
    "}\n",
    "\n",
    "lasso_csv_path = f'./{out_prefix}_lasso_regression.csv'\n",
    "\n",
    "performance_sweep(client=client, model=LassoRegression,\n",
    "                **sweep_kwargs,\n",
    "                out_path=lasso_csv_path,\n",
    "                model_kwargs=lasso_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb48bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_csv_data(lasso_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d831973",
   "metadata": {},
   "source": [
    "### ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.linear_model import ElasticNet as ElasticNetRegression\n",
    "\n",
    "elastic_kwargs = {\n",
    "    \"client\": client,\n",
    "}\n",
    "\n",
    "enr_csv_path = f'./{out_prefix}_elastic_regression.csv'\n",
    "\n",
    "performance_sweep(client=client, model=ElasticNetRegression,\n",
    "                **sweep_kwargs,\n",
    "                out_path=enr_csv_path,\n",
    "                model_kwargs=elastic_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc28cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_csv_data(enr_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e784c2",
   "metadata": {},
   "source": [
    "### Model Parallel Multi-GPU Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a884f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.solvers import CD\n",
    "# This uses model parallel Coordinate Descent\n",
    "cd_kwargs = {\n",
    "}\n",
    "\n",
    "cd_csv_path = f'./{out_prefix}_mutli_gpu_linear_regression.csv'\n",
    "\n",
    "performance_sweep(client=client, model=CD,\n",
    "                **sweep_kwargs,\n",
    "                out_path=cd_csv_path,\n",
    "                model_kwargs=cd_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b7b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_csv_data(cd_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dab009",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e427b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xg_args = [client]\n",
    "xg_kwargs = {\n",
    "    'params': {\n",
    "        'tree_method': 'gpu_hist',\n",
    "    },\n",
    "    'num_boost_round': 100\n",
    "}\n",
    "\n",
    "xgb_csv_path = f'./{out_prefix}_xgb.csv'\n",
    "\n",
    "class XGBProxy():\n",
    "    \"\"\"\n",
    "    Create a simple API wrapper around XGBoost so that it supports the fit/predict workflow.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    data_loader: data loader object intended to be used by the performance sweep.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_loader):\n",
    "        self.args = []\n",
    "        self.kwargs = {}\n",
    "        self.data_loader = data_loader\n",
    "        self.trained_model = None\n",
    "        \n",
    "    def loader(self, client, response_dtype, fraction, random_state):\n",
    "        \"\"\"\n",
    "        Wrap the data loader method so that it creates a DMatrix from the returned data.\n",
    "        \"\"\"\n",
    "        df, X, y = self.data_loader(client, response_dtype, fraction, random_state)\n",
    "        dmatrix = xgb.dask.DaskDMatrix(client, X, y)\n",
    "        \n",
    "        return dmatrix, dmatrix, dmatrix\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Acts as a pseudo init function which initializes our model args.\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Wrap dask.train, and store the model on our proxy object.\n",
    "        \"\"\"\n",
    "        if (self.trained_model):\n",
    "            del self.trained_model\n",
    "            \n",
    "        self.trained_model = xgb.dask.train(*self.args,\n",
    "                              dtrain=X,\n",
    "                              evals=[(X, 'train')],\n",
    "                              **self.kwargs)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        assert(self.trained_model)\n",
    "        \n",
    "        return xgb.dask.predict(*self.args, self.trained_model, X)\n",
    "    \n",
    "\n",
    "xgb_proxy = XGBProxy(data_loader)\n",
    "performance_sweep(client=client, model=xgb_proxy, data_loader=xgb_proxy.loader, hardware_type=hardware_type,\n",
    "                worker_counts=worker_counts, \n",
    "                samples=samples,\n",
    "                load_samples=load_samples,\n",
    "                max_data_frac=max_data_frac, \n",
    "                scaling_type=scale_type,\n",
    "                out_path=xgb_csv_path,\n",
    "                append_to_existing=append_to_existing,\n",
    "                update_workers_in_kwargs=False,\n",
    "                xy_fit=False,\n",
    "                scaling_denom = scaling_denom,\n",
    "                model_args=xg_args,\n",
    "                model_kwargs=xg_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_csv_data(xgb_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a040706",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd5f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6361cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
