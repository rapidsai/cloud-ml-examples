{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving model performance with xfeat, RAPIDS and Optuna\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Feature Engineering is the processing of transforming raw data into features that can represent the underlying patterns of the data better. They can help boost the accuracy by a great deal and improve the ability of the model to generalise on unseen data. Every data scientist knows the importance feature engineering. Spending some time thinking about how best to apply and combine the available features can be very meaningful. \n",
    "\n",
    "Hyper parameter Optimisation is another such process which can help complement a good model by tuning it's hyperparameters, which can have a tremendous impact on the accuracy of the model. The time and resources required for these processes are generally the reason they're overlooked. \n",
    "\n",
    "With xfeat, RAPIDS and Optuna - we aim to bridge these gaps and elevate the performance. \n",
    "\n",
    "## What is Optuna?\n",
    "[Optuna](https://github.com/optuna/optuna) is a lightweight framework for automatic hyperparameter optimization. It provides a define-by-run API, which makes it easy to adapt to any already existing code that we have and enables high modularity and the flexibility to construct hyperparameter spaces dynamically. By simply wrapping the objective function with Optuna can help perform a parallel-distributed HPO search over a search space. As we'll see in this notebook.\n",
    "\n",
    "## What is xfeat?\n",
    "[xfeat](https://github.com/pfnet-research/xfeat) is a feature engineering & exploration library using GPUs and Optuna. It provides a scikit-learn-like API for feature engineering with support for Pandas, cuDF dataframes and cuPy arrays. \n",
    "\n",
    "## What is RAPIDS?\n",
    "[RAPIDS](https://rapids.ai/about.html) framework  provides a library suite that can execute end-to-end data science pipelines entirely on GPUs.  The libraries in the framework include [cuDF](https://github.com/rapidsai/cudf) - a GPU Dataframe with pandas-like API, [cuML](https://github.com/rapidsai/cuml) - implement machine learning algorithms that provide a scikit-learn-like API and many more. You can learn more [here](https://github.com/rapidsai).\n",
    "\n",
    "In this notebook, we'll show how one can use these tools together to develop and improve a machine learning model. We'll use Airlines dataset (20M rows) to predict if a flight will be delayed or not. We'll explore how to use Optuna with RAPIDS and the speedups that we can achieve with the integration of these, and to see the improvements with GPU speedups, we have included a pandas version to run on CPU. A table summarizing the results is available at the end of the notebook.\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "You need to have the following libraries installed - \n",
    "\n",
    "- Optuna, plotly, kaleido\n",
    "\n",
    "Run the following in a new cell to install these packages:\n",
    "\n",
    "```\n",
    "!pip install optuna\n",
    "!pip install plotly\n",
    "!pip install kaleido\n",
    "```\n",
    "\n",
    "- To install xfeat, follow the instructions from their [repository](https://github.com/pfnet-research/xfeat)\n",
    "\n",
    "Restart the kernel after you've installed the packages and then run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cupy\n",
    "import cudf\n",
    "import cuml\n",
    "from cuml import LogisticRegression\n",
    "from cuml.metrics import roc_auc_score\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna.study import StudyDirection\n",
    "from optuna.trial import TrialState\n",
    "from optuna import type_checking\n",
    "\n",
    "import xfeat\n",
    "from xfeat.pipeline import Pipeline\n",
    "from xfeat.num_encoder import SelectNumerical\n",
    "from xfeat.selector import ChiSquareKBest\n",
    "from xfeat.optuna_selector import KBestThresholdExplorer\n",
    "from functools import partial\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from cuml.preprocessing.LabelEncoder import LabelEncoder\n",
    "from cuml.preprocessing.TargetEncoder import TargetEncoder\n",
    "from xfeat import ArithmeticCombinations, Pipeline, SelectNumerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "# Helping time blocks of code\n",
    "@contextmanager\n",
    "def timed(txt):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print(\"%32s time:  %8.5f\" % (txt, t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "The following functions are defined to perform a few feature engineering tasks on the data. The `feature_engineering` function is called on the dataframe `df`, in this function we perform a simple Arithmetic Combinations on the numerical columns that adds two columns to create a new one. We specify the `operator` and `r` - r is used to indicate how many columns need to be combined.\n",
    "\n",
    "Then we call `categorical_encoding` which converts the categorical columns to numerical ones and then performs `target_encoding`. Target Encoding replaces the value with the target mean. This is helpful in classification problem to boost the model accuracy. Find more resources at the end of the notebook.\n",
    "\n",
    "You'll also notice we use `Pipeline` from xfeat to combine two or more feature engineering tasks together. This is useful to concatenate encoders sequentially.\n",
    "\n",
    "Read more about Feature Encoding and Pipelining with xfeat [here](https://github.com/pfnet-research/xfeat/blob/master/_docs/feature_encoding.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Perform feature engineering and return a new df with engineered features\n",
    "    \"\"\"\n",
    "    df_train, df_test, y_train, y_test = train_test_split(df, TARGET_COL,random_state=np.random.seed(0),\n",
    "                                                          shuffle=True)\n",
    " \n",
    "    # Xfeat's internal fold mechanism creates RangeIndex references, so we need to do an index reset on our data frames.\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    # Need to do this to ensure we are appropriately assigning the split values\n",
    "    df_train[TARGET_COL] = y_train\n",
    "    df_test[TARGET_COL] = y_test\n",
    "    \n",
    "    \n",
    "    for col in CAT_COLS:\n",
    "        out_col = f'{col}_TE'\n",
    "        lbl_enc = LabelEncoder(handle_unknown='ignore')\n",
    "        tar_enc = TargetEncoder(n_folds=5, smooth=TARGET_ENC_SMOOTH, split_method=TARGET_ENC_SPLIT)\n",
    "        \n",
    "        df_train[col] = lbl_enc.fit_transform(df_train[col])\n",
    "        df_train[out_col] = tar_enc.fit_transform(df_train[col], df_train[TARGET_COL])\n",
    "        \n",
    "        df_test[out_col] = tar_enc.transform(df_test[col])\n",
    "        df_test[col] = lbl_enc.transform(df_test[col]).fillna(0)\n",
    "        del lbl_enc, tar_enc\n",
    "\n",
    "\n",
    "\n",
    "    encoder = Pipeline([\n",
    "                        SelectNumerical(),\n",
    "                        ArithmeticCombinations(exclude_cols=[TARGET_COL],\n",
    "                                               drop_origin=False,\n",
    "                                               operator=\"+\",\n",
    "                                               r=2,\n",
    "                                               output_suffix=\"_plus\")\n",
    "                    ])\n",
    "\n",
    "    df_train = encoder.fit_transform(df_train)\n",
    "    df_test = encoder.transform(df_test)\n",
    "    df = cudf.concat([df_train, df_test], sort=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Hyper parameter Optimisation\n",
    "\n",
    "Now that we have some new features, how do we know they are relevant for the task or represent anything meaningful? We use the feature selection process to do this. This helps in selection of a subset of features that are  most informative. This helps in simplifying the problem and ensures that we aren't overloading the system with unimportant features. Optuna provides a way to choose a `selector` which accepts a `Pipeline` object from xfeat. You can see in the `feature_selection` function we define a Pipeline that takes in an Explorer and a Selection Algorithm (`ChiSquareKBest`). We pass this to an Optuna Study object, along with an Objective function\n",
    "\n",
    "Chi squared tests are used to test the independence of two events. For Feature Selection, we aim to select feature, which are highly dependent on the response. This way, we can get features that will best determine the outcome.\n",
    "\n",
    "### Objective Function\n",
    "The objective function will be the one we optimize in Optuna Study. Objective funciton tries out different values for the parameters that we are tuning and saving the results in `study.trials_dataframes()`.\n",
    "\n",
    "Let's define the objective function for this HPO task by making use of the `train_and_eval()`. You can see that we simply choose a value for the parameters and call the `train_and_eval` method, making Optuna very easy to use in an existing workflow.\n",
    "\n",
    "The objective remains constant over different samplers, which are built-in options in Optuna to enable the selection of different sampling algorithms that optuna provides. Some of the available ones include - GridSampler, RandomSampler, TPESampler, etc. We'll use TPESampler for this demo, but feel free to try different samplers to notice the chnages in performance.\n",
    "\n",
    "\n",
    "### HPO Trials and Study\n",
    "Optuna uses [study](https://optuna.readthedocs.io/en/stable/reference/study.html) and [trials](https://optuna.readthedocs.io/en/stable/reference/trial.html) to keep track of the HPO experiments. Put simply, a trial is a single call of the objective function while a set of trials make up a study. We will pick the optimal performing trial from a study to get the best parameters that were used in that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(df, penalty='l2', C=1.0, l1_ratio=None, fit_intercept=True, selector=None):\n",
    "    # Splitting data and prepping for selector fit\n",
    "    X_train,  X_test, y_train, y_test = train_test_split(df,\n",
    "                                                         TARGET_COL,\n",
    "                                                         random_state=np.random.seed(0),\n",
    "                                                         shuffle=True)\n",
    "\n",
    "    # Xfeat's internal fold mechanism creates RangeIndex references, so we need to do an index reset on our data frames.\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    if selector:\n",
    "        # For the selector, the label also needs to be in the DF\n",
    "        X_train[TARGET_COL] = y_train\n",
    "        X_test[TARGET_COL] = y_test\n",
    "\n",
    "        X_train = selector.fit_transform(X_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "\n",
    "    # Train and get accuracy\n",
    "    classifier = LogisticRegression(penalty=penalty,\n",
    "                                    C=C,\n",
    "                                    l1_ratio=l1_ratio,\n",
    "                                    fit_intercept=fit_intercept)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict_proba(X_test.values)[:, 1]\n",
    "    y_pred = y_pred.astype(y_test.dtype)\n",
    "    score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    return score, classifier\n",
    "\n",
    "\n",
    "def objective(df, selector, trial):\n",
    "    \"\"\"\n",
    "    Performs the training and evaluation of the set of parameters and subset of features using selector.\n",
    "    \"\"\"\n",
    "    selector.set_trial(trial)\n",
    "    \n",
    "    # Select Params\n",
    "    C = trial.suggest_uniform(\"C\", 0 , 7.0)\n",
    "    penalty = trial.suggest_categorical(\"penalty\", ['l1', 'none', 'l2'])\n",
    "    l1_ratio = trial.suggest_uniform(\"l1_ratio\", 0 , 1.0)\n",
    "    fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "    \n",
    "    score, _ = train_and_eval(df,\n",
    "                           penalty=penalty,\n",
    "                           C=C,\n",
    "                           l1_ratio=l1_ratio,\n",
    "                           fit_intercept=fit_intercept,\n",
    "                           selector=selector)\n",
    "    return score\n",
    "\n",
    "def feature_selection(df, experiment_name):\n",
    "    \"\"\"\n",
    "    Defines the Pipeline and performs the optuna opt\n",
    "    \"\"\"\n",
    "    selector = Pipeline(\n",
    "        [\n",
    "            SelectNumerical(),\n",
    "            KBestThresholdExplorer(ChiSquareKBest(target_col=TARGET_COL)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "    study.optimize(partial(objective, df, selector), n_trials=N_TRIALS)\n",
    "\n",
    "    selector.from_trial(study.best_trial)\n",
    "    selected_cols = selector.get_selected_cols()\n",
    "\n",
    "    df_select = df[selected_cols]\n",
    "    df_select[TARGET_COL] = df[TARGET_COL]\n",
    "\n",
    "    params = study.best_params\n",
    "    score, classifier = train_and_eval(df_select,\n",
    "                  C=params['C'],\n",
    "                  penalty=params['penalty'],\n",
    "                  l1_ratio=params['l1_ratio'],\n",
    "                  fit_intercept=params['fit_intercept'])\n",
    "\n",
    "    return study, df_select.reset_index(drop=True), classifier, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Experiment Variables\n",
    "\n",
    "Change the `INPUT_FILE` to correspond to the path in your local system and select the number of rows and trials to run the experiment for. \n",
    "\n",
    "NOTE: It is not recommended to run the CPU version for more than 100,000 rows. To avoid this, we set a glad `cpu_run` based on `N_ROWS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"/home/data/airline_data/airline_small.parquet\"\n",
    "\n",
    "N_ROWS = 10000000\n",
    "N_TRIALS = 10\n",
    "\n",
    "TARGET_ENC_SMOOTH = 0.001\n",
    "TARGET_ENC_SPLIT = 'interleaved'\n",
    "\n",
    "CAT_COLS = [\"Dest\", \"Origin\", \"UniqueCarrier\"]\n",
    "TARGET_COL = \"ArrDelayBinary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Run\n",
    "\n",
    "Now, let's run the RAPIDS version by first reading in the data as cudf DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default performance:  (0.5941567420959473, LogisticRegression(penalty='l2', tol=0.0001, C=1.0, fit_intercept=True, max_iter=1000, linesearch_max_iter=50, verbose=4, l1_ratio=None, solver='qn', handle=<cuml.common.handle.Handle object at 0x7fbd42bbf6b0>, output_type='cudf'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArrDelayBinary</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayofWeek</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1846.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>987.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArrDelayBinary    Year  Month  DayofMonth  DayofWeek  CRSDepTime  \\\n",
       "0             1.0  1987.0   10.0         1.0        4.0         1.0   \n",
       "1             0.0  1987.0   10.0         1.0        4.0         5.0   \n",
       "2             1.0  1987.0   10.0         1.0        4.0         5.0   \n",
       "3             0.0  1987.0   10.0         1.0        4.0         5.0   \n",
       "4             1.0  1987.0   10.0         1.0        4.0         8.0   \n",
       "\n",
       "   CRSArrTime  UniqueCarrier  FlightNum  Origin   Dest  Distance  Diverted  \n",
       "0       556.0            0.0      190.0   220.0  175.0    1846.0       0.0  \n",
       "1       114.0            4.0       57.0   132.0  219.0     337.0       0.0  \n",
       "2        35.0            5.0      351.0   116.0  130.0     987.0       0.0  \n",
       "3        40.0            3.0      251.0   148.0  179.0     142.0       0.0  \n",
       "4       517.0           12.0      500.0   131.0  175.0    1515.0       0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = cudf.read_parquet(INPUT_FILE)[:N_ROWS]\n",
    "# Can't handle nagative values, yet\n",
    "df_ = df_.drop([\"ActualElapsedTime\"], axis=1)\n",
    "print(\"Default performance: \", train_and_eval(df_)[0])\n",
    "\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/join/join.py:368: UserWarning: can't safely cast column from right with type uint8 to object, upcasting to None\n",
      "  \"right\", dtype_r, dtype_l, libcudf_join_type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After feature eng:  (0.5000408291816711, LogisticRegression(penalty='l2', tol=0.0001, C=1.0, fit_intercept=True, max_iter=1000, linesearch_max_iter=50, verbose=4, l1_ratio=None, solver='qn', handle=<cuml.common.handle.Handle object at 0x7fbd3bff5590>, output_type='cudf'))\n",
      "                              FE time:  50.28020\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayofWeek</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>...</th>\n",
       "      <th>DistanceDiverted_plus</th>\n",
       "      <th>DistanceDest_TE_plus</th>\n",
       "      <th>DistanceOrigin_TE_plus</th>\n",
       "      <th>DistanceUniqueCarrier_TE_plus</th>\n",
       "      <th>DivertedDest_TE_plus</th>\n",
       "      <th>DivertedOrigin_TE_plus</th>\n",
       "      <th>DivertedUniqueCarrier_TE_plus</th>\n",
       "      <th>Dest_TEOrigin_TE_plus</th>\n",
       "      <th>Dest_TEUniqueCarrier_TE_plus</th>\n",
       "      <th>Origin_TEUniqueCarrier_TE_plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1988.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1714.0</td>\n",
       "      <td>0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>37</td>\n",
       "      <td>130</td>\n",
       "      <td>...</td>\n",
       "      <td>337.0</td>\n",
       "      <td>337.274796</td>\n",
       "      <td>337.188048</td>\n",
       "      <td>337.162193</td>\n",
       "      <td>0.274796</td>\n",
       "      <td>0.188048</td>\n",
       "      <td>0.162193</td>\n",
       "      <td>0.462844</td>\n",
       "      <td>0.436989</td>\n",
       "      <td>0.350241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1988.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>4</td>\n",
       "      <td>974.0</td>\n",
       "      <td>84</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235.0</td>\n",
       "      <td>235.187393</td>\n",
       "      <td>235.240799</td>\n",
       "      <td>235.211942</td>\n",
       "      <td>0.187393</td>\n",
       "      <td>0.240799</td>\n",
       "      <td>0.211942</td>\n",
       "      <td>0.428192</td>\n",
       "      <td>0.399335</td>\n",
       "      <td>0.452740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1855.0</td>\n",
       "      <td>2141.0</td>\n",
       "      <td>4</td>\n",
       "      <td>693.0</td>\n",
       "      <td>84</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>925.0</td>\n",
       "      <td>925.196435</td>\n",
       "      <td>925.241073</td>\n",
       "      <td>925.212574</td>\n",
       "      <td>0.196435</td>\n",
       "      <td>0.241073</td>\n",
       "      <td>0.212574</td>\n",
       "      <td>0.437508</td>\n",
       "      <td>0.409009</td>\n",
       "      <td>0.453647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>814.0</td>\n",
       "      <td>5</td>\n",
       "      <td>278.0</td>\n",
       "      <td>206</td>\n",
       "      <td>149</td>\n",
       "      <td>...</td>\n",
       "      <td>298.0</td>\n",
       "      <td>298.177675</td>\n",
       "      <td>298.188746</td>\n",
       "      <td>298.198605</td>\n",
       "      <td>0.177675</td>\n",
       "      <td>0.188746</td>\n",
       "      <td>0.198605</td>\n",
       "      <td>0.366421</td>\n",
       "      <td>0.376280</td>\n",
       "      <td>0.387351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>1341.0</td>\n",
       "      <td>3</td>\n",
       "      <td>651.0</td>\n",
       "      <td>213</td>\n",
       "      <td>143</td>\n",
       "      <td>...</td>\n",
       "      <td>440.0</td>\n",
       "      <td>440.190919</td>\n",
       "      <td>440.210602</td>\n",
       "      <td>440.212036</td>\n",
       "      <td>0.190919</td>\n",
       "      <td>0.210602</td>\n",
       "      <td>0.212036</td>\n",
       "      <td>0.401522</td>\n",
       "      <td>0.402955</td>\n",
       "      <td>0.422639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Month  DayofMonth  DayofWeek  CRSDepTime  CRSArrTime  \\\n",
       "0  1988.0   10.0        16.0        7.0      1600.0      1714.0   \n",
       "1  1988.0    1.0        19.0        2.0      1135.0      1340.0   \n",
       "2  1990.0    3.0        13.0        2.0      1855.0      2141.0   \n",
       "3  1990.0    4.0        22.0        7.0       710.0       814.0   \n",
       "4  1987.0   11.0        15.0        7.0      1305.0      1341.0   \n",
       "\n",
       "   UniqueCarrier  FlightNum  Origin  Dest  ...  DistanceDiverted_plus  \\\n",
       "0              0      247.0      37   130  ...                  337.0   \n",
       "1              4      974.0      84   210  ...                  235.0   \n",
       "2              4      693.0      84    17  ...                  925.0   \n",
       "3              5      278.0     206   149  ...                  298.0   \n",
       "4              3      651.0     213   143  ...                  440.0   \n",
       "\n",
       "   DistanceDest_TE_plus  DistanceOrigin_TE_plus  \\\n",
       "0            337.274796              337.188048   \n",
       "1            235.187393              235.240799   \n",
       "2            925.196435              925.241073   \n",
       "3            298.177675              298.188746   \n",
       "4            440.190919              440.210602   \n",
       "\n",
       "   DistanceUniqueCarrier_TE_plus  DivertedDest_TE_plus  \\\n",
       "0                     337.162193              0.274796   \n",
       "1                     235.211942              0.187393   \n",
       "2                     925.212574              0.196435   \n",
       "3                     298.198605              0.177675   \n",
       "4                     440.212036              0.190919   \n",
       "\n",
       "   DivertedOrigin_TE_plus  DivertedUniqueCarrier_TE_plus  \\\n",
       "0                0.188048                       0.162193   \n",
       "1                0.240799                       0.211942   \n",
       "2                0.241073                       0.212574   \n",
       "3                0.188746                       0.198605   \n",
       "4                0.210602                       0.212036   \n",
       "\n",
       "   Dest_TEOrigin_TE_plus  Dest_TEUniqueCarrier_TE_plus  \\\n",
       "0               0.462844                      0.436989   \n",
       "1               0.428192                      0.399335   \n",
       "2               0.437508                      0.409009   \n",
       "3               0.366421                      0.376280   \n",
       "4               0.401522                      0.402955   \n",
       "\n",
       "   Origin_TEUniqueCarrier_TE_plus  \n",
       "0                        0.350241  \n",
       "1                        0.452740  \n",
       "2                        0.453647  \n",
       "3                        0.387351  \n",
       "4                        0.422639  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We cast to objects for categorical  and target encoding\n",
    "# Can't pass categorical directly to LR\n",
    "for col in CAT_COLS:\n",
    "    df_[col] = df_[col].astype(\"object\")\n",
    "\n",
    "with timed(\"FE\"):\n",
    "    df_feature_eng = feature_engineering(df_)\n",
    "    df_feature_eng[TARGET_COL] = df_feature_eng[TARGET_COL].astype('int64')\n",
    "    score, _ = train_and_eval(df_feature_eng)\n",
    "    print(\"After feature eng: \", score)\n",
    "\n",
    "df_feature_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-09 22:23:46,640] A new study created in memory with name: no-name-6e129246-e1b4-41b6-8760-abec2e1e0331\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "exp_name = 'Optuna-SingleGPU' + str(random.randint(0,100))\n",
    "with timed(\"FS + Optuna\"):\n",
    "    # Disable Alembic driver, used by MLflow, from logging INFO messages to the command line.\n",
    "    logging.getLogger('alembic').setLevel(logging.CRITICAL)\n",
    "    study, df_select, best_clf, score = feature_selection(df_feature_eng, experiment_name=exp_name)\n",
    "    print(\"Best score after Feature Selection + Optuna: \", score)\n",
    "df_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(\"Complete workflow \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The details of the best trial \", study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summarization\n",
    "\n",
    "We noticed that Feature Engineering alone takes 28.47 seconds on CPU vs 7.78 seconds on GPU, yieling a 4x speed up.\n",
    "\n",
    "Performing Feature engineering and Selection boosts the AUC score from 0.61 to 0.72. Byy repeating this task on a larger portion of the data, a wider search space, we would be able to achieve a better improvement.\n",
    "\n",
    "From our experiemnts, GPU runs are faster for 100,000 rows (and 10 trials) and we are able to obtain <b>5.x</b> speedups. For more performance improvements, you are encouraged to increased the number of rows and/or number of trials. This will result in a big boost in accuracy. Keep in mind, you do not want to run the experiment on CPU with a larger number of rows, as this will result in the kernel crashing.\n",
    "\n",
    "|Number of rows| Trials| CPU | GPU|\n",
    "|-|-|-|-|\n",
    "|100K|10|64.83|12.97|\n",
    "|1M|10|-|60.14|\n",
    "|10M|10|-|464.03|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's look at some graphs to understand and visualize what we achieved in this notebook. \n",
    "\n",
    "The graph below shows the importance of a feature for the performance. We see that the `penalty` set in Logistic Regression and `K` from the Chi-sqaure test have the highest importance in performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "f = optuna.visualization.plot_param_importances(study)\n",
    "Image(f.to_image(format=\"png\", engine='kaleido'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a slice plot to better understand the parameter relationships. We see how the change in the parameter affects the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = optuna.visualization.plot_slice(study, params=['l1_ratio', 'C', 'KBestThresholdExplorer.k', 'penalty', 'fit_intercept'])\n",
    "Image(f.to_image(format=\"png\", engine='kaleido'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of all trials in the study to see how the performance improvements took place within the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = optuna.visualization.plot_optimization_history(study)\n",
    "Image(f.to_image(format=\"png\", engine='kaleido'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have seen the performance improvement, let's look at how we can retrieve this model with MLFlow.\n",
    "\n",
    "### Launch our optimized model within the MLflow framework.\n",
    "Run the code block below to identify the most recently registered model, with the 'rapids-optuna-airline' tag; after identifying the latest model version, run the code below in a separate terminal, and wait for it to fully load your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Run the command below in a terminal, and wait for it to load your model:\\n\\n  \\\n",
    "      {get_latest_mlflow_model(MLFLOW_TRACKING_URI, MLFLOW_MODEL_ID)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to the following:\n",
    "\n",
    "```shell\n",
    "2020/07/27 13:59:49 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n",
    "2020/07/27 13:59:49 INFO mlflow.pyfunc.backend: === Running command 'source /anaconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-3335621df6011b1847d2555b195418d4496e5ffd 1>&2 && gunicorn --timeout=60 -b 127.0.0.1:5000 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n",
    "[2020-07-27 13:59:50 -0600] [23779] [INFO] Starting gunicorn 20.0.4\n",
    "[2020-07-27 13:59:50 -0600] [23779] [INFO] Listening at: http://127.0.0.1:5000 (23779)\n",
    "[2020-07-27 13:59:50 -0600] [23779] [INFO] Using worker: sync\n",
    "[2020-07-27 13:59:50 -0600] [23788] [INFO] Booting worker with pid: 23788\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host='localhost'\n",
    "port='56767'\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"format\": \"pandas-split\"\n",
    "}\n",
    "\n",
    "data = { \n",
    "    \"columns\": [\"Year\", \"Month\", \"DayofMonth\", \"DayofWeek\", \"CRSDepTime\", \"CRSArrTime\", \"UniqueCarrier\",\n",
    "                \"FlightNum\", \"ActualElapsedTime\", \"Origin\", \"Dest\", \"Distance\", \"Diverted\"],\n",
    "    \"data\": [[1987, 10, 1, 4, 1, 556, 0, 190, 247, 202, 162, 1846, 0]]\n",
    "}\n",
    "\n",
    "while (True):\n",
    "    try:\n",
    "        resp = requests.post(url=\"http://%s:%s/invocations\" % (host, port), data=json.dumps(data), headers=headers)\n",
    "        print('Classification: %s' % (\"ON-Time\" if resp.text == \"[0.0]\" else \"LATE\"))\n",
    "        break\n",
    "    except Exception as e:\n",
    "        errmsg = \"Caught exception attempting to call model endpoint: %s\" % e\n",
    "        print(errmsg)\n",
    "        print(\"... Sleeping ...\")\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "[How to Win a DS Kaggle competition](https://www.coursera.org/learn/competitive-data-science)\n",
    "\n",
    "[Target Encoding and Bayesian Target Encoding](https://towardsdatascience.com/target-encoding-and-bayesian-target-encoding-5c6a6c58ae8c)\n",
    "\n",
    "[Learn more about using MLFlow with RAPIDS](https://github.com/mlflow/mlflow/tree/master/examples/rapids/mlflow_project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
