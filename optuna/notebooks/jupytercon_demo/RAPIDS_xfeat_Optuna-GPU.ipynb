{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving model performance with xfeat, RAPIDS and Optuna\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Feature Engineering is the processing of transforming raw data into features that can represent the underlying patterns of the data better. They can help boost accuracy by a great deal and improve the ability of the model to generalise on unseen data. Every data scientist knows the importance of feature engineering. Spending some time thinking about how best to apply and combine the available features can be very meaningful.\n",
    "\n",
    "Hyper parameter Optimisation is another such process which can help complement a good model by tuning it's hyperparameters, which can have a tremendous impact on the accuracy of the model.\n",
    "\n",
    "The time and resources required for these processes are generally the reason they're overlooked. With xfeat, RAPIDS and Optuna - we aim to bridge these gaps and elevate the performance.\n",
    "\n",
    "## What is Optuna?\n",
    "[Optuna](https://github.com/optuna/optuna) is a lightweight framework for automatic hyperparameter optimization. It provides a define-by-run API, which makes it easy to adapt to any already existing code that we have and enables high modularity and the flexibility to construct hyperparameter spaces dynamically. By simply wrapping the objective function with Optuna can help perform a parallel-distributed HPO search over a search space. As we'll see in this notebook.\n",
    "\n",
    "## What is xfeat?\n",
    "[xfeat](https://github.com/pfnet-research/xfeat) is a feature engineering & exploration library using GPUs and Optuna. It provides a scikit-learn-like API for feature engineering with support for Pandas, cuDF dataframes and cuPy arrays. \n",
    "\n",
    "## What is MLflow?\n",
    "[MLflow](https://mlflow.org/) is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry.\n",
    "\n",
    "## What is RAPIDS?\n",
    "[RAPIDS](https://rapids.ai/about.html) framework  provides a library suite that can execute end-to-end data science pipelines entirely on GPUs.  The libraries in the framework include [cuDF](https://github.com/rapidsai/cudf) - a GPU Dataframe with pandas-like API, [cuML](https://github.com/rapidsai/cuml) - implement machine learning algorithms that provide a scikit-learn-like API and many more. You can learn more [here](https://github.com/rapidsai).\n",
    "\n",
    "In this notebook, we'll show how one can use these tools together to develop and improve a machine learning model. We'll use Airlines dataset (20M rows) to predict if a flight will be delayed or not. We'll explore how to use Optuna with RAPIDS and the speedups that we can achieve with the integration of these, and to see the improvements with GPU speedups. There's a CPU version of this notebook available in the repository here - `optuna/notebooks/jupytercon\\ demo/RAPIDS_xfeat_Optuna-CPU.ipynb`.\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "You need to have the following libraries installed - \n",
    "\n",
    "- MLFlow, Optuna, plotly, kaleido\n",
    "\n",
    "Uncomment and run the following cell to install these packages. Restart the kernel after you've installed the packages and then run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlflow\n",
    "# !pip install optuna\n",
    "# !pip install plotly\n",
    "# !pip install kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cupy\n",
    "import cudf\n",
    "import cuml\n",
    "from cuml import LogisticRegression\n",
    "from cuml.metrics import roc_auc_score\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from  mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import optuna\n",
    "import sklearn\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "from optuna.study import StudyDirection\n",
    "from optuna.trial import TrialState\n",
    "from optuna import type_checking\n",
    "\n",
    "import xfeat\n",
    "from xfeat.pipeline import Pipeline\n",
    "from xfeat.num_encoder import SelectNumerical\n",
    "from xfeat.selector import ChiSquareKBest\n",
    "from xfeat.optuna_selector import KBestThresholdExplorer\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from cuml.preprocessing.LabelEncoder import LabelEncoder\n",
    "from cuml.preprocessing.TargetEncoder import TargetEncoder\n",
    "from xfeat import ArithmeticCombinations, Pipeline, SelectNumerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "# Helping time blocks of code\n",
    "@contextmanager\n",
    "def timed(txt):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print(\"%32s time:  %8.5f\" % (txt, t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "We'll use the cell below to download the data. The file_name specifies which of the two available files we want to use. The two files are - `airline_small.parquet` (smaller file) and `airline_20000000.parquet`. By default, we use the smaller file. You are free to change it for experimentation.\n",
    "\n",
    "Set the `download_data=True` and `data_dir` to a local path in your system to download the data into the path of `data_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "download_data = True\n",
    "\n",
    "file_name = 'airline_small.parquet' # NOTE: Change to airline_20000000.parquet to use a larger dataset\n",
    "\n",
    "data_dir = \"data/\" # NOTE: Change to a local path where you want to save the file\n",
    "INPUT_FILE = os.path.join(data_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "    if os.path.isfile(INPUT_FILE):\n",
    "            print(f\" > File already exists. Ready to load at {INPUT_FILE}\")\n",
    "    else:\n",
    "        # Ensure folder exists\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    url = \"https://rapidsai-cloud-ml-sample-data.s3-us-west-2.amazonaws.com/\" + file_name\n",
    "\n",
    "    urlretrieve(url= url,filename=INPUT_FILE)\n",
    "\n",
    "    print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow Configuration\n",
    "\n",
    "For tracking the hyperparameter optimisation expriments, we will use MLFlow. In the next cell, the required variables are set up along with the callback class `RAPIDSMLflowCallback` that we will pass to Optuna for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_URI='sqlite:////tmp/mlflow-db.sqlite'\n",
    "MLFLOW_MODEL_ID = \"rapids-optuna-airline\"\n",
    "\n",
    "def get_latest_mlflow_model(tracking_uri, model_id):\n",
    "    client = MlflowClient(tracking_uri=tracking_uri, registry_uri=tracking_uri)\n",
    "    model = client.get_registered_model(model_id)\n",
    "    latest_model = model.latest_versions[0]\n",
    "\n",
    "    return f\"MLFLOW_TRACKING_URI={tracking_uri} mlflow models serve --no-conda -m models:/{model_id}/{latest_model.version} -p 56767\"\n",
    "\n",
    "## Custom callback, for additional flexibility, based on MLflowCallback\n",
    "class RAPIDSMLflowCallback(object):\n",
    "    def __init__(self, tracking_uri: str = \"sqlite:////tmp/mlflow-db.sqlite\",\n",
    "                 experiment_name: str = \"RAPIDS-Optuna\",\n",
    "                 metric_name=\"value\"):\n",
    "        self._tracking_uri = tracking_uri\n",
    "        self._experiment_name = experiment_name\n",
    "        self._metric_name = metric_name\n",
    "        \n",
    "    def __call__(self, study, trial):\n",
    "        if (self._tracking_uri is not None):\n",
    "            mlflow.set_tracking_uri(self._tracking_uri)\n",
    "        \n",
    "        eid = mlflow.set_experiment(self._experiment_name)\n",
    "        with mlflow.start_run(run_name=f\"Trial: {trial.number}\", experiment_id=eid, nested=True):\n",
    "            trial_value = trial.value if trial.value is not None else float(\"nan\")\n",
    "            mlflow.log_metric(self._metric_name, trial_value)\n",
    "            \n",
    "            mlflow.log_params(trial.params)\n",
    "\n",
    "            tags = {}\n",
    "            tags[\"number\"] = str(trial.number)\n",
    "            tags[\"datetime_start\"] = str(trial.datetime_start)\n",
    "            tags[\"datetime_complete\"] = str(trial.datetime_complete)\n",
    "            tags['RAPIDS cuDF Version'] = str(cudf.__version__)\n",
    "            tags['RAPIDS cuML Version'] = str(cuml.__version__)\n",
    "            tags['SKlearn Version'] = str(sklearn.__version__)\n",
    "\n",
    "            trial_state = trial.state\n",
    "            if (isinstance(trial_state, TrialState)):\n",
    "                tags['state'] = str(trial_state).split('.')[-1]\n",
    "            \n",
    "            # Set direction and convert it to str and remove the common prefix.\n",
    "            study_direction = study.direction\n",
    "            if isinstance(study_direction, StudyDirection):\n",
    "                tags[\"direction\"] = str(study_direction).split(\".\")[-1]\n",
    "\n",
    "            tags.update(trial.user_attrs)\n",
    "            distributions = {\n",
    "                (k + \"_distribution\"): str(v) for (k, v) in trial.distributions.items()\n",
    "            }\n",
    "            tags.update(distributions)\n",
    "\n",
    "            # This is a temporary fix on Optuna side. It avoids an error with user\n",
    "            # attributes that are too long. It should be fixed on MLflow side later.\n",
    "            # When it is fixed on MLflow side this codeblock can be removed.\n",
    "            # see https://github.com/optuna/optuna/issues/1340\n",
    "            # see https://github.com/mlflow/mlflow/issues/2931\n",
    "            max_mlflow_tag_length = 5000\n",
    "            for key, value in tags.items():\n",
    "                value = str(value)  # make sure it is a string\n",
    "                if len(value) > max_mlflow_tag_length:\n",
    "                    tags[key] = textwrap.shorten(value, max_mlflow_tag_length)\n",
    "\n",
    "            mlflow.set_tags(tags) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "The following functions are defined to perform a few feature engineering tasks on the data. The feature_engineering function is called on the dataframe df, in this function we use cuML's [LabelEncoder](https://docs.rapids.ai/api/cuml/stable/api.html#cuml.preprocessing.LabelEncoder.LabelEncoder) and [TargetEncoder](https://docs.rapids.ai/api/cuml/stable/api.html#cuml.preprocessing.TargetEncoder) to encode the categorical columns. Arithmetic Combinations on the numerical columns using xfeat. The Arithmetic Operation performed here is addition on two columns at once. We specify the operator and r, which is used to indicate how many columns need to be combined.\n",
    "\n",
    "You'll also notice we use `Pipeline` from xfeat to combine two or more feature engineering tasks together. This is useful to concatenate encoders sequentially.\n",
    "\n",
    "In this step, we will be creating a lot of features without much manual optimization. This is part of the charm of automated feature engineering - the next steps will ensure that only the most useful generated features are retained.\n",
    "\n",
    "Read more about Feature Encoding and Pipelining with xfeat [here](https://github.com/pfnet-research/xfeat/blob/master/_docs/feature_encoding.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Perform feature engineering and return a new df with engineered features\n",
    "    \"\"\"\n",
    "    df_train, df_test, y_train, y_test = train_test_split(df,\n",
    "                                                          TARGET_COL,\n",
    "                                                          random_state=np.random.seed(0),\n",
    "                                                          shuffle=True)\n",
    "\n",
    "    # Xfeat's internal fold mechanism creates RangeIndex references, so we need to do an index reset on our data frames.\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    # Need to do this to ensure we are appropriately assigning the split values\n",
    "    df_train[TARGET_COL] = y_train\n",
    "    df_test[TARGET_COL] = y_test\n",
    "    \n",
    "    for col in CAT_COLS:\n",
    "        out_col = f'{col}_TE'\n",
    "        lbl_enc = LabelEncoder(handle_unknown='ignore')\n",
    "        tar_enc = TargetEncoder(n_folds=5, smooth=TARGET_ENC_SMOOTH, split_method=TARGET_ENC_SPLIT)\n",
    "\n",
    "        df_train[col] = lbl_enc.fit_transform(df_train[col])\n",
    "        df_train[out_col] = tar_enc.fit_transform(df_train[col], df_train[TARGET_COL])\n",
    "\n",
    "        df_test[out_col] = tar_enc.transform(df_test[col])\n",
    "        df_test[col] = lbl_enc.transform(df_test[col]).fillna(0)\n",
    "        del lbl_enc, tar_enc\n",
    "\n",
    "    encoder = Pipeline([\n",
    "                        SelectNumerical(),\n",
    "                        ArithmeticCombinations(exclude_cols=[TARGET_COL],\n",
    "                                               drop_origin=False,\n",
    "                                               operator=\"+\",\n",
    "                                               r=2,\n",
    "                                               output_suffix=\"_plus\")\n",
    "                    ])\n",
    "\n",
    "    df_train = encoder.fit_transform(df_train)\n",
    "    df_test = encoder.transform(df_test)\n",
    "    df = cudf.concat([df_train, df_test], sort=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Hyper parameter Optimisation\n",
    "\n",
    "Now that we have some new features, how do we know they are relevant for the task or represent anything meaningful? We use the feature selection process to do this. This helps in selection of a subset of features that are most informative. This helps in simplifying the problem and ensures that we aren't overloading the system with unimportant features.\n",
    "\n",
    "Optuna provides a way to choose a selector which accepts a Pipeline object from xfeat. You can see in the `feature_selection` function we define a `Pipeline` that takes in an Explorer and a Selection Algorithm (`ChiSquareKBest`). We pass this to an Optuna Study object, along with an Objective function. \n",
    "\n",
    "Chi squared tests are used to test the independence of two events. For Feature Selection using Chi-squared test, we aim to select features, which are highly dependent on the response. This way, we can get features that will best determine the outcome.\n",
    "\n",
    "#### Objective Function\n",
    "The objective function will be the one we optimize in Optuna Study. Objective function tries out different values for the parameters that we are tuning and saving the results in `study.trials_dataframes()`.\n",
    "\n",
    "Let's define the objective function for this HPO task by making use of the `train_and_eval()`. You can see that we simply choose a value for the parameters and call the train_and_eval method, making Optuna very easy to use in an existing workflow.\n",
    "\n",
    "The objective function does not need to be changed for different samplers. Samplers are built-in options in Optuna to enable the selection of different sampling algorithms that optuna provides. Some of the available ones include - GridSampler, RandomSampler, TPESampler, etc. We'll use TPESampler for this demo, but feel free to try different samplers to notice the changes in performance. Tree-structured Parzen Estimator or TPE is based on independent sampling. It fits a Gaussian Mixture Model for each parameter in a trial `l(x)` and another GMM `g(x)` for other parameters. And it optimizes the `l(x)/g(x)` ratio to yield the best parameters.\n",
    "\n",
    "### HPO Trials and Study\n",
    "Optuna uses [study](https://optuna.readthedocs.io/en/stable/reference/study.html) and [trials](https://optuna.readthedocs.io/en/stable/reference/trial.html) to keep track of the HPO experiments. Put simply, a trial is a single call of the objective function while a set of trials make up a study. We will pick the best performing trial from a study to get the best parameters that were used in that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(df, penalty='l2', C=1.0, l1_ratio=None, fit_intercept=True, selector=None, return_model=False):\n",
    "    \"\"\"\n",
    "        Split the dataframe based on TARGET_COL\n",
    "        Accepts the parameters to be set for the Logistic Regression model\n",
    "        \n",
    "        Evaluates on the split data and returns AUC score.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df: DataFrame to use for training and validation\n",
    "        \n",
    "        penalty, C, l1_ratio, fit_intercept: Parameters for the LogisticRegression Model\n",
    "            For details refer to the documentation\n",
    "\n",
    "        selector: xfeat selector passed via Optuna (default=None)\n",
    "            When set, the slector is used to fit and transform the data\n",
    " \n",
    "        return_model: Returns the fit model back to the calling fucntion (default=False)\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        score: AUC score of the validation set\n",
    "        classifier, signature: Returned only when return_model is set to True\n",
    "    \"\"\"\n",
    "    # Splitting data and prepping for selector fit\n",
    "    X_train,  X_test, y_train, y_test = train_test_split(df,\n",
    "                                                         TARGET_COL,\n",
    "                                                         random_state=np.random.seed(0),\n",
    "                                                         shuffle=True)\n",
    "    # Xfeat's internal fold mechanism creates RangeIndex references, so we need to do an index reset on our data frames.\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    if selector:\n",
    "        # For the selector, the label also needs to be in the DF\n",
    "        X_train[TARGET_COL] = y_train\n",
    "        X_test[TARGET_COL] = y_test\n",
    "\n",
    "        X_train = selector.fit_transform(X_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "\n",
    "    # Train and get accuracy\n",
    "    classifier = LogisticRegression(penalty=penalty,\n",
    "                                    C=C,\n",
    "                                    l1_ratio=l1_ratio,\n",
    "                                    fit_intercept=fit_intercept)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict_proba(X_test.values)[:, 1]\n",
    "    y_pred = y_pred.astype(y_test.dtype)\n",
    "    score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    if (return_model):\n",
    "        return score, classifier, infer_signature(X_test.to_pandas(), cupy.asnumpy(y_pred))\n",
    "\n",
    "    return score\n",
    "\n",
    "def objective(df, selector, trial):\n",
    "    \"\"\"\n",
    "    Performs the training and evaluation of the set of parameters and subset of features using selector.\n",
    "    \"\"\"\n",
    "    selector.set_trial(trial)\n",
    "    \n",
    "    # Select Params\n",
    "    C = trial.suggest_uniform(\"C\", 0 , 7.0)\n",
    "    penalty = trial.suggest_categorical(\"penalty\", ['l1', 'none', 'l2'])\n",
    "    l1_ratio = trial.suggest_uniform(\"l1_ratio\", 0 , 1.0)\n",
    "    fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "\n",
    "    score = train_and_eval(df,\n",
    "                           penalty=penalty,\n",
    "                           C=C,\n",
    "                           l1_ratio=l1_ratio,\n",
    "                           fit_intercept=fit_intercept,\n",
    "                           selector=selector)\n",
    "    return score\n",
    "\n",
    "def feature_selection(df, experiment_name):\n",
    "    \"\"\"\n",
    "    Defines the Pipeline and performs the optuna opt\n",
    "    \"\"\"\n",
    "    artifact_path = \"rapids-optuna-airline\"\n",
    "    selector = Pipeline(\n",
    "        [\n",
    "            SelectNumerical(),\n",
    "            # Select features according to the k highest scores from the ChiSquared test\n",
    "            KBestThresholdExplorer(ChiSquareKBest(target_col=TARGET_COL)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    mlfcb = RAPIDSMLflowCallback(\n",
    "        tracking_uri=MLFLOW_TRACKING_URI,\n",
    "        experiment_name=experiment_name,\n",
    "        metric_name='auc')\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=f\"Optuna-HPO:{study.study_name}\"):\n",
    "        study.optimize(partial(objective, df, selector), n_trials=N_TRIALS, callbacks=[mlfcb])\n",
    "        \n",
    "        selector.from_trial(study.best_trial)\n",
    "        selected_cols = selector.get_selected_cols()\n",
    "        \n",
    "        df_select = df[selected_cols]\n",
    "        df_select[TARGET_COL] = df[TARGET_COL]\n",
    "        \n",
    "        params = study.best_params\n",
    "        score, classifier, signature = train_and_eval(df_select,\n",
    "                      C=params['C'],\n",
    "                      penalty=params['penalty'],\n",
    "                      l1_ratio=params['l1_ratio'],\n",
    "                      fit_intercept=params['fit_intercept'],\n",
    "                      return_model=True)\n",
    "        \n",
    "        with mlflow.start_run(run_name='Final Classifier', nested=True):\n",
    "            mlflow.log_metric('auc', score)\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.sklearn.log_model(classifier,\n",
    "                                 signature=signature,\n",
    "                                 artifact_path=artifact_path,\n",
    "                                 registered_model_name=\"rapids-optuna-airline\",\n",
    "                                 conda_env='conda/conda.yaml')\n",
    "\n",
    "    return study, df_select.reset_index(drop=True), classifier, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Experiment Variables\n",
    "\n",
    "Change the `INPUT_FILE` to correspond to the path in your local system and select the number of rows and trials to run the experiment for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"/home/data/airline_data/airline_small.parquet\"\n",
    "\n",
    "N_ROWS = 1000000  # Number of rows to use for this experiment run\n",
    "N_TRIALS = 50 # Number of trials for the HPO study\n",
    "\n",
    "TARGET_COL = \"ArrDelayBinary\" \n",
    "CAT_COLS = [\"Dest\", \"Origin\", \"UniqueCarrier\"]\n",
    "\n",
    "# Parameters for TagetEncoder\n",
    "TARGET_ENC_SMOOTH = 0.001\n",
    "TARGET_ENC_SPLIT = 'interleaved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Run\n",
    "\n",
    "Let's get the time to GPU performance. \n",
    "\n",
    "Here, we will read the data and get the default AUC score first, before optimizing our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_ = cudf.read_parquet(INPUT_FILE)[:N_ROWS]\n",
    "# Can't handle nagative values, yet\n",
    "df_ = df_.drop([\"ActualElapsedTime\"], axis=1)\n",
    "print(\"Default performance: \", train_and_eval(df_))\n",
    "\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the feature engineering tasks and see the performance. We'll also take a look at the dataframe after feature engineering. You can see the new features with `_LE` for LabelEncoder, `_TE` for TargetEncoder and `_plus` for additive features. Our 13 column dataset has grown to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cast to objects for categorical  and target encoding\n",
    "# Can't pass categorical directly to LR\n",
    "for col in CAT_COLS:\n",
    "    df_[col] = df_[col].astype(\"object\")\n",
    "\n",
    "with timed(\"Feature Engineering\"):\n",
    "    df_feature_eng = feature_engineering(df_)\n",
    "    df_feature_eng[TARGET_COL] = df_feature_eng[TARGET_COL].astype('float64')\n",
    "    score = train_and_eval(df_feature_eng)\n",
    "    print(\"After feature eng: \", score)\n",
    "\n",
    "df_feature_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "exp_name = 'Optuna-SingleGPU' + str(random.randint(0,1000))\n",
    "with timed(\"Feature Selection + Optuna\"):\n",
    "    # Disable Alembic driver, used by MLflow, from logging INFO messages to the command line.\n",
    "    logging.getLogger('alembic').setLevel(logging.CRITICAL)\n",
    "    study, df_select, best_clf, score = feature_selection(df_feature_eng, experiment_name=exp_name)\n",
    "    print(\"Best score after Feature Selection + Optuna: \", score)\n",
    "df_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(\"Complete workflow \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The details of the best trial \", study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summarization\n",
    "\n",
    "We noticed that Feature Engineering alone takes 28.47 seconds on CPU vs 7.78 seconds on GPU, yielding a 4x speed up. Performing Feature engineering and Selection boosts the AUC score from 0.61 to 0.72. By repeating this task on a larger portion of the data, a wider search space, we would be able to achieve a better improvement. \n",
    "\n",
    "From our experiments, GPU runs are faster for 100,000 rows (and 10 trials) and we are able to obtain <b>5x</b> speedups. For more performance improvements, you are encouraged to increase the number of rows and/or number of trials. This will result in a big boost in accuracy. Keep in mind, you do not want to run the experiment on the CPU with a larger number of rows, as this will result in the kernel crashing.\n",
    "\n",
    "Below is a tablet summarizing the results from our experiments.\n",
    "\n",
    "|Number of rows| Trials| CPU | GPU|\n",
    "|-|-|-|-|\n",
    "|100K|10|64.83|12.97|\n",
    "|1M|10|-|60.14|\n",
    "|10M|10|-|464.03|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's look at some graphs to understand and visualize what we achieved in this notebook. \n",
    "\n",
    "The graph below shows the importance of a feature for the performance. We see that the `penalty` set in Logistic Regression and `K` from the Chi-sqaure test have the highest importance in performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "f = optuna.visualization.plot_param_importances(study)\n",
    "Image(f.to_image(format=\"png\", engine='kaleido'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a slice plot to better understand the parameter relationships. We see how the change in the parameter affects the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = optuna.visualization.plot_slice(study, params=['l1_ratio', 'C', 'KBestThresholdExplorer.k', 'penalty', 'fit_intercept'])\n",
    "Image(f.to_image(format=\"png\", engine='kaleido'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of all trials in the study to see how the performance improvements took place within the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = optuna.visualization.plot_optimization_history(study)\n",
    "Image(f.to_image(format=\"png\", engine='kaleido'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have seen the performance improvement, let's look at how we can retrieve this model with MLFlow.\n",
    "\n",
    "### Launch our optimized model within the MLflow framework.\n",
    "Run the code block below to identify the most recently registered model, with the 'rapids-optuna-airline' tag; after identifying the latest model version, run the code below in a separate terminal, and wait for it to fully load your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Run the command below in a terminal, and wait for it to load your model:\\n\\n  \\\n",
    "      {get_latest_mlflow_model(MLFLOW_TRACKING_URI, MLFLOW_MODEL_ID)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to the following:\n",
    "\n",
    "```shell\n",
    "2020/07/27 13:59:49 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n",
    "2020/07/27 13:59:49 INFO mlflow.pyfunc.backend: === Running command 'source /anaconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-3335621df6011b1847d2555b195418d4496e5ffd 1>&2 && gunicorn --timeout=60 -b 127.0.0.1:5000 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n",
    "[2020-07-27 13:59:50 -0600] [23779] [INFO] Starting gunicorn 20.0.4\n",
    "[2020-07-27 13:59:50 -0600] [23779] [INFO] Listening at: http://127.0.0.1:5000 (23779)\n",
    "[2020-07-27 13:59:50 -0600] [23779] [INFO] Using worker: sync\n",
    "[2020-07-27 13:59:50 -0600] [23788] [INFO] Booting worker with pid: 23788\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host='localhost'\n",
    "port='56767'\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"format\": \"pandas-split\"\n",
    "}\n",
    "\n",
    "data = { \n",
    "    \"columns\": [\"Year\", \"Month\", \"DayofMonth\", \"DayofWeek\", \"CRSDepTime\", \"CRSArrTime\", \"UniqueCarrier\",\n",
    "                \"FlightNum\", \"ActualElapsedTime\", \"Origin\", \"Dest\", \"Distance\", \"Diverted\"],\n",
    "    \"data\": [[1987, 10, 1, 4, 1, 556, 0, 190, 247, 202, 162, 1846, 0]]\n",
    "}\n",
    "\n",
    "while (True):\n",
    "    try:\n",
    "        resp = requests.post(url=\"http://%s:%s/invocations\" % (host, port), data=json.dumps(data), headers=headers)\n",
    "        print('Classification: %s' % (\"ON-Time\" if resp.text == \"[0.0]\" else \"LATE\"))\n",
    "        break\n",
    "    except Exception as e:\n",
    "        errmsg = \"Caught exception attempting to call model endpoint: %s\" % e\n",
    "        print(errmsg)\n",
    "        print(\"... Sleeping ...\")\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Thoughts\n",
    "\n",
    "Weâ€™ve seen how to use RAPIDS alongside xfeat and Optuna to get performance boosts both in terms of run-time and AUC score. The notebook can serve as a starting point for anyone looking to get started on using GPUs to run Hyperparameter Optimization and Feature Engineering experiments. These can add value by yielding better performing models much faster than CPU runs. MLflow helps in retaining the model information and makes it easier to save and load our best model, thus making it an useful choice for production. The combination of these libraries can save a lot of time and provide significant improvements in performance. \n",
    "\n",
    "To learn more:\n",
    "\n",
    "[Target Encoding and Bayesian Target Encoding](https://towardsdatascience.com/target-encoding-and-bayesian-target-encoding-5c6a6c58ae8c)\n",
    "\n",
    "[Learn more about using MLFlow with RAPIDS](https://github.com/mlflow/mlflow/tree/master/examples/rapids/mlflow_project)\n",
    "\n",
    "[Algorithms for Hyper-Parameter Optimization](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)\n",
    "\n",
    "Find the libraries: [RAPIDS](https://rapids.ai/), [xfeat](https://github.com/pfnet-research/xfeat/tree/master/xfeat), [Optuna](https://optuna.readthedocs.io/en/stable/), [MLFlow](https://mlflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
