## Inferencing with Triton Inferencing Server with RAPIDS cuML FIL backend on Kubernetes

This folder contains examples to run the Triton Inferencing Server on Kubernetes with a RAPIDS cuML FIL backend, which is available as a fully integrated part of the Triton Server.

1. The directory [GCP](./GCP/FIL) contains examples on Google Kubernetes Engine. 
2. The directory [AWS](./AWS/FIL) contains examples on Amazon Elastic Kubernetes Services.

