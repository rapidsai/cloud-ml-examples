## Inferencing with Triton Inferencing Server with RAPIDS cuML FIL backend on Kubernetes

This folder contains examples to run the Triton Inferencing Server on a Kubernetes Server with a custom RAPIDS cuML FIL backend.

1. The directory [GCP](./GCP) contains examples on Google Kubernetes Engine. 
2. The directory [AWS](./AWS) contains examples on Amazon Elastic Kubernetes Services.
