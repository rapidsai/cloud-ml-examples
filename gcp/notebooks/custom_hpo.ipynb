{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "### Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from ax import ParameterType, optimize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# os\n",
    "import sys, os, time, logging\n",
    "\n",
    "# CPU DS stack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# GPU DS stack [ rapids ]\n",
    "import gcsfs\n",
    "\n",
    "# scaling library\n",
    "import dask\n",
    "\n",
    "# data ingestion [ CPU ]\n",
    "from pyarrow import orc as pyarrow_orc\n",
    "\n",
    "# ML models\n",
    "from sklearn import ensemble\n",
    "import xgboost\n",
    "\n",
    "# data set splits\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "\n",
    "# device query\n",
    "##hack\n",
    "try:\n",
    "    import cudf, cuml\n",
    "    from cuml.model_selection import train_test_split as cuml_train_test_split\n",
    "    import pynvml\n",
    "    import cupy\n",
    "except Exception as e:\n",
    "    print(\"Caught import failures -- probably missing GPU:\")\n",
    "    print(e)\n",
    "\n",
    "# memory query\n",
    "import psutil\n",
    "\n",
    "# i/o\n",
    "import logging, json, pprint\n",
    "\n",
    "default_sagemaker_paths = {\n",
    "    'base': '/opt/ml',\n",
    "    'code': '/opt/ml/code',\n",
    "    'data': '/opt/ml/input',\n",
    "    'train_data': '/opt/ml/input/data/training',\n",
    "    'hyperparams': '/opt/ml/input/config/hyperparameters.json',\n",
    "    'model': '/opt/ml/model',\n",
    "    'output': '/opt/ml/output',\n",
    "}\n",
    "\n",
    "\n",
    "class RapidsCloudML(object):\n",
    "\n",
    "    def __init__(self, cloud_type='AWS',\n",
    "                 model_type='XGBoost',\n",
    "                 data_type='ORC',\n",
    "                 compute_type='single-GPU',\n",
    "                 n_workers=-1,\n",
    "                 verbose_estimator=False,\n",
    "                 CSP_paths=default_sagemaker_paths):\n",
    "\n",
    "        self.CSP_paths = CSP_paths\n",
    "        self.cloud_type = cloud_type\n",
    "        self.model_type = model_type\n",
    "        self.data_type = data_type\n",
    "        self.compute_type = compute_type\n",
    "        self.verbose_estimator = verbose_estimator\n",
    "        self.n_workers = self.parse_compute(n_workers)\n",
    "        self.query_memory()\n",
    "\n",
    "    def _read_orc(self, filename):\n",
    "        if ('CPU' in self.compute_type):\n",
    "            if (filename.startswith('gs://')):\n",
    "                fs = gcsfs.GCSFileSystem()\n",
    "                with fs.open(filename, mode='rb') as file:\n",
    "                    dataset = pyarrow_orc.ORCFile(file).read().to_pandas()\n",
    "            else:\n",
    "                with open(filename, mode='rb') as file:\n",
    "                    dataset = pyarrow_orc.ORCFile(file).read().to_pandas()\n",
    "\n",
    "        elif ('GPU' in self.compute_type):\n",
    "            dataset = cudf.read_orc(filename)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _read_csv(self, filename, col_labels):\n",
    "        if ('CPU' in self.compute_type):\n",
    "            dataset = pd.read_csv(filename, names=col_labels)\n",
    "        elif ('GPU' in self.compute_type):\n",
    "            dataset = cudf.read_csv(filename, names=col_labels)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def load_data(self, filename='dataset.orc', col_labels=None, y_label='ArrDelayBinary'):\n",
    "        target_filename = self.CSP_paths['train_data'] + '/' + filename\n",
    "        self.log_to_file(f'\\n> loading dataset from {target_filename}...\\n')\n",
    "\n",
    "        with PerfTimer() as ingestion_timer:\n",
    "            if 'ORC' in self.data_type:\n",
    "                dataset = self._read_orc(target_filename)\n",
    "            elif 'CSV' in self.data_type:\n",
    "                dataset = self._read_csv(target_filename, names=col_labels)\n",
    "\n",
    "        self.log_to_file(f'ingestion completed in {ingestion_timer.duration}')\n",
    "        self.log_to_file(f'dataset descriptors: {dataset.shape}\\n {dataset.dtypes}\\n {dataset.columns}\\n')\n",
    "\n",
    "        return dataset, col_labels, y_label, ingestion_timer.duration\n",
    "\n",
    "    def split_data(self, dataset, y_label, train_size=.8, random_state=0, shuffle=True):\n",
    "        \"\"\"\n",
    "        split dataset into train and test subset\n",
    "        NOTE: assumes the first column of the dataset is the classification labels\n",
    "            ! in the case of sklearn, we manually filter this column in the split call\n",
    "            ! in the case of cuml, the filtering happens internally\n",
    "        \"\"\"\n",
    "        self.log_to_file('\\tsplitting train and test data')\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        with PerfTimer() as split_timer:\n",
    "            if 'CPU' in self.compute_type:\n",
    "                X_train, X_test, y_train, y_test = sklearn_train_test_split(dataset.loc[:, dataset.columns != y_label],\n",
    "                                                                            dataset[y_label], train_size=train_size,\n",
    "                                                                            shuffle=shuffle, random_state=random_state)\n",
    "            elif 'GPU' in self.compute_type:\n",
    "                X_train, X_test, y_train, y_test = cuml_train_test_split(X=dataset, y=y_label, train_size=train_size,\n",
    "                                                                         shuffle=shuffle, random_state=random_state)\n",
    "        self.log_to_file(f'\\t> split completed in {split_timer.duration}')\n",
    "        return X_train, X_test, y_train, y_test, split_timer.duration\n",
    "\n",
    "    def train_model(self, X_train, y_train, model_params):\n",
    "        self.log_to_file(f'\\ttraining {self.model_type} estimator w/ hyper-params')\n",
    "        pprint.pprint(model_params, indent=10)\n",
    "        print(f\"model type: {self.model_type}\\n compute type: {self.compute_type}\\n dataset dtype: {type(X_train)}\")\n",
    "\n",
    "        try:\n",
    "            if self.model_type == 'XGBoost':\n",
    "                trained_model, training_time = self.fit_xgboost(X_train, y_train, model_params)\n",
    "            elif self.model_type == 'RandomForest':\n",
    "                trained_model, training_time = self.fit_random_forest(X_train, y_train, model_params)\n",
    "\n",
    "        except Exception as error:\n",
    "            self.log_to_file('!error during model training: ' + str(error))\n",
    "            raise\n",
    "\n",
    "        self.log_to_file(f'\\t> finished training in {training_time:.4f} s')\n",
    "        return trained_model, training_time\n",
    "\n",
    "    # train dlmc.xgboost model\n",
    "    def fit_xgboost(self, X_train, y_train, model_params):\n",
    "        with PerfTimer() as train_timer:\n",
    "            train_DMatrix = xgboost.DMatrix(data=X_train, label=y_train)\n",
    "            trained_model = xgboost.train(dtrain=train_DMatrix,\n",
    "                                          params=model_params,\n",
    "                                          num_boost_round=model_params['num_boost_round'],\n",
    "                                          verbose_eval=self.verbose_estimator)\n",
    "        return trained_model, train_timer.duration\n",
    "\n",
    "    # fit_xgboost_multi_GPU ()\n",
    "    # fit_random_forest_multi_GPU ()\n",
    "\n",
    "    # train cuml.random-forest model\n",
    "    def fit_random_forest(self, X_train, y_train, model_params):\n",
    "        if 'CPU' in self.compute_type:\n",
    "            rf_model = sklearn.ensemble.RandomForestClassifier(n_estimators=model_params['n_estimators'],\n",
    "                                                               max_depth=model_params['max_depth'],\n",
    "                                                               max_features=model_params['max_features'],\n",
    "                                                               n_jobs=int(self.n_workers),\n",
    "                                                               verbose=self.verbose_estimator)\n",
    "        elif 'GPU' in self.compute_type:\n",
    "            rf_model = cuml.ensemble.RandomForestClassifier(n_estimators=model_params['n_estimators'],\n",
    "                                                            max_depth=model_params['max_depth'],\n",
    "                                                            n_bins=model_params['n_bins'],\n",
    "                                                            max_features=model_params['max_features'],\n",
    "                                                            verbose=self.verbose_estimator)\n",
    "        with PerfTimer() as train_timer:\n",
    "            trained_model = rf_model.fit(X_train, y_train)\n",
    "\n",
    "        return trained_model, train_timer.duration\n",
    "\n",
    "    def evaluate_test_perf(self, trained_model, X_test, y_test):\n",
    "        self.log_to_file(f'\\tinferencing on test set')\n",
    "        with PerfTimer() as inference_timer:\n",
    "            try:\n",
    "                if self.model_type == 'XGBoost':\n",
    "                    test_DMatrix = xgboost.DMatrix(data=X_test, label=y_test)\n",
    "                    test_accuracy = 1 - float(trained_model.eval(test_DMatrix).split(':')[1])\n",
    "\n",
    "                elif self.model_type == 'RandomForest':\n",
    "                    # y_test = cudf.DataFrame({'label': y_test.astype('int32') })\n",
    "                    test_accuracy = trained_model.score(X_test, y_test.astype('int32'))\n",
    "\n",
    "            except Exception as error:\n",
    "                self.log_to_file('!error during inference: ' + str(error))\n",
    "                raise\n",
    "\n",
    "        self.log_to_file(f'\\t> finished inference in {inference_timer.duration:.4f} s')\n",
    "        return test_accuracy, inference_timer.duration\n",
    "\n",
    "    # TODO: FIL inference [ ? ]\n",
    "    # evaluate_perf_FIL(self, trained_model, X_test, y_test ):\n",
    "\n",
    "    # TODO: global_best_model.save()\n",
    "    def save_best_model(self, global_best_model=None):\n",
    "        pass\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # end of data science logic\n",
    "    # ------------------------------------------------------\n",
    "\n",
    "    def parse_compute(self, n_workers=None):\n",
    "        if 'CPU' in self.compute_type or 'GPU' in self.compute_type:\n",
    "            available_devices = self.query_compute()\n",
    "            if n_workers == -1:\n",
    "                n_workers = available_devices\n",
    "            assert (n_workers <= available_devices)\n",
    "            self.log_to_file(f'compute type: {self.compute_type}, n_workers: {n_workers}')\n",
    "        else:\n",
    "            raise Exception('unsupported compute type')\n",
    "        return n_workers\n",
    "\n",
    "    def query_compute(self):\n",
    "        available_devices = None\n",
    "        if 'CPU' in self.compute_type:\n",
    "            available_devices = os.cpu_count()\n",
    "            self.log_to_file(f'detected {available_devices} CPUs')\n",
    "        elif 'GPU' in self.compute_type:\n",
    "            available_devices = cupy.cuda.runtime.getDeviceCount()\n",
    "            self.log_to_file(f'detected {available_devices} GPUs')\n",
    "        return available_devices\n",
    "\n",
    "    # TODO: enumerate all visible GPUs [ ? ]\n",
    "    def query_memory(self):\n",
    "        def print_device_memory(memory, device_ID=-1):\n",
    "            memory_free_GB = np.array(memory.free) / np.array(10e8)\n",
    "            memory_used_GB = np.array(memory.used) / np.array(10e8)\n",
    "            memory_total_GB = np.array(memory.total) / np.array(10e8)\n",
    "            if device_ID != -1:\n",
    "                self.log_to_file(f'device ID = {device_ID}')\n",
    "            self.log_to_file(f'memory free, used, total: {memory_free_GB}, {memory_used_GB}, {memory_total_GB}')\n",
    "\n",
    "        if 'CPU' in self.compute_type:\n",
    "            print_device_memory(psutil.virtual_memory())\n",
    "\n",
    "        elif 'GPU' in self.compute_type:\n",
    "            pynvml.nvmlInit()\n",
    "            for iGPU in range(self.n_workers):\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(iGPU)\n",
    "                print_device_memory(pynvml.nvmlDeviceGetMemoryInfo(handle))\n",
    "\n",
    "    def set_up_logging(self):\n",
    "        logging_path = self.CSP_paths['output'] + '/log.txt'\n",
    "        logging.basicConfig(filename=logging_path,\n",
    "                            level=logging.INFO)\n",
    "\n",
    "    def log_to_file(self, text):\n",
    "        logging.info(text)\n",
    "        print(text)\n",
    "\n",
    "    def environment_check(self):\n",
    "        self.check_dirs()\n",
    "\n",
    "        if self.cloud_type == 'AWS':\n",
    "            try:\n",
    "                self.list_files('/opt/ml')\n",
    "                self.log_to_file(os.environ['SM_NUM_GPUS'])\n",
    "                self.log_to_file(os.environ['SM_TRAINING_ENV'])\n",
    "                self.log_to_file(os.environ['SM_CHANNEL_TRAIN'])\n",
    "                self.log_to_file(os.environ['SM_HPS'])\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def check_dirs(self):\n",
    "        self.log_to_file('\\n> checking for sagemaker paths...\\n')\n",
    "\n",
    "        directories_to_check = self.CSP_paths\n",
    "        for iDir, val in directories_to_check.items():\n",
    "            self.log_to_file(f'{val}, exists : {os.path.exists(val)}')\n",
    "\n",
    "        self.log_to_file(f'working directory = {os.getcwd()}')\n",
    "\n",
    "    def list_files(self, startpath):\n",
    "        print(f'\\n> listing contents of {startpath}\\n')\n",
    "        for root, dirs, files in os.walk(startpath):\n",
    "            level = root.replace(startpath, '').count(os.sep)\n",
    "            indent = ' ' * 4 * (level)\n",
    "            print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "            subindent = ' ' * 4 * (level + 1)\n",
    "            for f in files:\n",
    "                print('{}{}'.format(subindent, f))\n",
    "\n",
    "\n",
    "# perf_counter = highest available timer resolution\n",
    "class PerfTimer:\n",
    "    def __init__(self):\n",
    "        self.start = None\n",
    "        self.duration = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.duration = time.perf_counter() - self.start\n",
    "\n",
    "\n",
    "'''\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit\n",
    "\n",
    "n_estimators=100,\n",
    "criterion='gini',\n",
    "max_depth=None,\n",
    "min_samples_split=2,\n",
    "min_samples_leaf=1,\n",
    "min_weight_fraction_leaf=0.0,\n",
    "max_features='auto',\n",
    "max_leaf_nodes=None,\n",
    "min_impurity_decrease=0.0,\n",
    "min_impurity_split=None,\n",
    "bootstrap=True,\n",
    "oob_score=False,\n",
    "n_jobs=None,\n",
    "random_state=None,\n",
    "verbose=0,\n",
    "warm_start=False,\n",
    "class_weight=None,\n",
    "ccp_alpha=0.0,\n",
    "max_samples=None\n",
    "\n",
    "'''\n",
    "\n",
    "### Setup paths, model, and configuration parameters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gcp_path_setup(input_path, output_path):\n",
    "    paths = {\n",
    "        'train_data': input_path,\n",
    "        'model': f'{output_path}/model',\n",
    "        'output': f'{output_path}/output',\n",
    "    }\n",
    "\n",
    "    return paths\n",
    "\n",
    "def ax_train_proxy(model_params, config_params, ax_params):\n",
    "    rcml = RapidsCloudML(cloud_type=config_params['cloud_type'],\n",
    "                         model_type=config_params['model_type'],\n",
    "                         compute_type=f\"single-{config_params['compute']}\",\n",
    "                         CSP_paths=config_params['paths'])\n",
    "\n",
    "    # environment check\n",
    "    rcml.environment_check()\n",
    "\n",
    "    # ingest data [ post pre-processing ]\n",
    "    dataset, col_labels, y_label, ingest_time = rcml.load_data(filename=config_params['dataset_filename'])\n",
    "    rcml.query_memory()\n",
    "\n",
    "    # classification objective requires int32 label for cuml random forest\n",
    "    dataset[y_label] = dataset[y_label].astype('int32')\n",
    "\n",
    "    accuracy_per_fold = []\n",
    "    train_time_per_fold = []\n",
    "    infer_time_per_fold = []\n",
    "    split_time_per_fold = []\n",
    "    global_best_model = None\n",
    "    global_best_test_accuracy = 0\n",
    "\n",
    "    model_params[\"max_depth\"] = ax_params[\"max_depth\"]\n",
    "    model_params[\"max_features\"] = ax_params[\"max_features\"]\n",
    "    model_params[\"n_estimators\"] = ax_params[\"n_estimators\"]\n",
    "\n",
    "    # optional cross-validation w/ model_params['n_train_folds'] > 1\n",
    "    for i_train_fold in range(config_params['CV_folds']):\n",
    "        print(f\"STARTING TRAINING FOLD {i_train_fold}\", flush=True)\n",
    "        rcml.log_to_file(f\"\\n CV fold {i_train_fold} of {config_params['CV_folds']}\\n\")\n",
    "\n",
    "        # split data\n",
    "        X_train, X_test, y_train, y_test, split_time = rcml.split_data(dataset=dataset,\n",
    "                                                                       y_label=y_label,\n",
    "                                                                       random_state=i_train_fold,\n",
    "                                                                       shuffle=True)\n",
    "        split_time_per_fold += [round(split_time, 4)]\n",
    "\n",
    "        # train model\n",
    "        trained_model, training_time = rcml.train_model(X_train, y_train, model_params)\n",
    "        train_time_per_fold += [round(training_time, 4)]\n",
    "\n",
    "        # evaluate perf\n",
    "        test_accuracy, infer_time = rcml.evaluate_test_perf(trained_model, X_test, y_test)\n",
    "        accuracy_per_fold += [round(test_accuracy, 4)]\n",
    "        infer_time_per_fold += [round(infer_time, 4)]\n",
    "\n",
    "        # update best model [ assumes maximization of perf metric ]\n",
    "        if test_accuracy > global_best_test_accuracy:\n",
    "            global_best_test_accuracy = test_accuracy\n",
    "            global_best_model = trained_model\n",
    "\n",
    "        rcml.log_to_file(f'\\n accuracy per fold    : {accuracy_per_fold} \\n')\n",
    "        rcml.log_to_file(f'\\n train-time per fold  : {train_time_per_fold} \\n')\n",
    "        rcml.log_to_file(f'\\n infer-time per fold  : {infer_time_per_fold} \\n')\n",
    "        rcml.log_to_file(f'\\n split-time per fold  : {split_time_per_fold} \\n')\n",
    "        \n",
    "    return global_best_test_accuracy\n",
    "\n",
    "\n",
    "def ax_train(model_params: dict, config_params: dict):\n",
    "    depth = [int(d) for d in config_params['ht_depth_range'].split(',')]\n",
    "    features = [float(d) for d in config_params['ht_features_range'].split(',')]\n",
    "    estimators = [int(d) for d in config_params['ht_est_range'].split(',')]\n",
    "    experiments = config_params['ht_experiments']\n",
    "\n",
    "    parameters=[\n",
    "            {\"name\": \"max_depth\", \"type\": \"range\", \"bounds\": depth, \"parameter_type\": ParameterType.INT},\n",
    "            {\"name\": \"max_features\", \"type\": \"range\", \"bounds\": features, \"parameter_type\": ParameterType.FLOAT},\n",
    "            {\"name\": \"n_estimators\", \"type\": \"range\", \"bounds\": estimators, \"parameter_type\": ParameterType.INT}\n",
    "        ]\n",
    "    \n",
    "    best_parameters, best_values, experiment, model = optimize(\n",
    "        parameters=parameters,\n",
    "        evaluation_function=lambda params: ax_train_proxy(model_params=model_params,\n",
    "                                                          config_params=config_params,\n",
    "                                                          ax_params=params),\n",
    "        minimize=False,\n",
    "        total_trials=experiments,\n",
    "        objective_name='accuracy',\n",
    "    )\n",
    "\n",
    "    print(\"Ax Optimization Results:\")\n",
    "    print(best_parameters)\n",
    "    print(best_values)\n",
    "\n",
    "    return best_values['accuracy']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paths = gcp_path_setup(\"gs://[PATH TO YOUR DATA BUCKET]\", \"gs://[PATH TO YOUR TRAINING DATA]\")\n",
    "\n",
    "# dataset_filename should be contained in your data bucket.\n",
    "\n",
    "config_params = {}\n",
    "config_params['CV_folds'] = 1\n",
    "config_params['cloud_type'] = 'GCP'\n",
    "config_params['compute'] = 'GPU' \n",
    "config_params['dataset'] = 'airline'\n",
    "config_params['dataset_filename'] = 'airline_10000000.orc'\n",
    "config_params['model_type'] = \"RandomForest\"\n",
    "config_params['num_samples'] = 4\n",
    "config_params['paths'] = paths\n",
    "config_params['ht_est_range'] = \"100,200\"\n",
    "config_params['ht_depth_range'] = \"9,17\" \n",
    "config_params['ht_features_range'] = \"0.2,0.6\" \n",
    "config_params['ht_experiments'] = 10\n",
    "\n",
    "model_params = {\n",
    "    'seed': random.random(),\n",
    "    'n_bins': 64\n",
    "    # 'seed': 0\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy = ax_train(model_params, config_params)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (aws-sagemaker-gtc-2020)",
   "language": "python",
   "name": "pycharm-6f8a357a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}