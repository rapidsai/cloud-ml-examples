{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/header.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hyper Parameter Optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization) (HPO) improves model quality by searching over hyperparameters, parameters not typically learned during the training process but rather values that control the learning process itself (e.g., model size/capacity). This search can significantly boost model quality relative to default settings and non-expert tuning; however, HPO can take a very long time on a non-accelerated platform. In this notebook, we containerize a RAPIDS workflow and run Bring-Your-Own-Container SageMaker HPO to show how we can overcome the computational complexity of model search. \n",
    "\n",
    "We accelerate HPO in two key ways: \n",
    "* by *scaling within a node* (e.g., multi-GPU where each GPU brings a magnitude higher core count relative to CPUs), and \n",
    "* by *scaling across nodes* and running parallel trials on cloud instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we cover step 2 of the workflow - building a container - which must then be published to the Amazon Elastic Container Registry (ECR) to be used for our Estimator object. This notebook is intended to set up a container in the ECR to be used in the sagemaker_studio/rapids_studio_hpo.ipynb example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/hpo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:22pt\"> **Preamble** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get things rolling let's make sure we can query our AWS SageMaker execution role and session as well as our account ID and AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "account=!(aws sts get-caller-identity --query Account --output text)\n",
    "region=!(aws configure get region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['561241433344'], ['us-east-1'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account, region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:22pt\"> **Key Choices** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and choose the configuration options for our HPO run.\n",
    "\n",
    "Below are two reference configurations showing a small and a large scale HPO (sized in terms of total experiments/compute). \n",
    "\n",
    "The default values in the notebook are set for the small HPO configuration, however you are welcome to scale them up.\n",
    "\n",
    "> **small HPO**: 1_year, XGBoost, 3 CV folds, singleGPU, max_jobs = 10, max_parallel_jobs = 2\n",
    "\n",
    "> **large HPO**: 10_year, XGBoost, 10 CV folds, multiGPU, max_jobs = 100, max_parallel_jobs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Dataset ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We offer free hosting for several demo datasets that you can try running HPO with, or alternatively you can bring your own dataset (BYOD). \n",
    "\n",
    "By default we leverage the `Airline` dataset, which is a large public tracker of US domestic flight logs which we offer in various sizes (1 year, 3 year, and 10 year) and in <a href='https://parquet.apache.org/'>Parquet</a> (compressed column storage) format. The machine learning objective with this dataset is to predict whether flights will be more than 15 minutes late arriving to their destination ([dataset link](https://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&DB_URL=), additional details in <a href='#dataset'>Section 1.1</a>). \n",
    "\n",
    "As an alternative we also offer the `NYC Taxi` dataset which captures yellow cab trip details in Ney York in January 2020, stored in <a href='https://en.wikipedia.org/wiki/Comma-separated_values'>CSV </a> format without any compression. The machine learning objective with this dataset is to predict whether a trip had an above average tip (>$2.20).\n",
    "\n",
    "We host the demo datasets in public S3 demo buckets in both the **us-east-1** (N. Virginia) or **us-west-2** (Oregon) regions (i.e., `sagemaker-rapids-hpo-us-east-1`, and `sagemaker-rapids-hpo-us-west-2`). You should run the SageMaker HPO workflow in either of these two regions if you wish to leverage the demo datasets since SageMaker requires that the S3 dataset and the compute you'll be renting are co-located. \n",
    "\n",
    "Lastly, if you plan to use your own dataset refer to the <a href='#byod'>BYOD checklist in the Appendix</a> to help integrate into the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| dataset | data_bucket | dataset_directory | # samples | storage type | time span |\n",
    "|---|---|---|---|---|---|\n",
    "| Airline Stats Small    | demo    | 1_year   | 6.3M   | Parquet     | 2019         |\n",
    "| Airline Stats Medium   | demo    | 3_year   | 18M    | Parquet     | 2019-2017    |\n",
    "| Airline Stats Large    | demo    | 10_year  | 63M    | Parquet     | 2019-2010    |\n",
    "| NYC Taxi               | demo    | NYC_taxi | 6.3M   | CSV         | 2020 January |\n",
    "| Bring Your Own Dataset | custom  | custom   | custom | Parquet/CSV | custom       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose dataset directory\n",
    "dataset_directory = '3_year' # '1_year', '3_year', '10_year', 'NYC_taxi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Algorithm ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a ML/algorithm perspective, we offer [XGBoost](https://xgboost.readthedocs.io/en/latest/#), [RandomForest](https://docs.rapids.ai/api/cuml/stable/cuml_blogs.html#tree-and-forest-models) and [KMeans](https://docs.rapids.ai/api/cuml/stable/api.html?highlight=kmeans#cuml.KMeans). You are free to switch between these algorithm choices and everything in the example will continue to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose learning algorithm\n",
    "algorithm_choice = 'XGBoost'\n",
    "\n",
    "assert (algorithm_choice in ['XGBoost', 'RandomForest', 'KMeans'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also optionally increase robustness via reshuffles of the train-test split (i.e., [cross-validation folds](https://scikit-learn.org/stable/modules/cross_validation.html)). Typical values here are between 3 and 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose cross-validation folds\n",
    "cv_folds = 3\n",
    "\n",
    "assert (cv_folds >= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ ML Workflow Compute Choice ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable the option of running different code variations that unlock increasing amounts of parallelism in the compute workflow. \n",
    "\n",
    "* `singleCPU`** = [pandas](https://pandas.pydata.org/) + [sklearn](https://scikit-learn.org/stable/)\n",
    "* `multiCPU`   = [dask](https://dask.org/) + [pandas](https://pandas.pydata.org/) + [sklearn](https://scikit-learn.org/stable/)\n",
    "\n",
    "* <span style=\"color:#8735fb; font-size:14pt\"> RAPIDS </span> `singleGPU` = [cudf](https://github.com/rapidsai/cudf) + [cuml](https://github.com/rapidsai/cuml)\n",
    "* <span style=\"color:#8735fb; font-size:14pt\"> RAPIDS </span> `multiGPU`  = [dask](https://dask.org/) + [cudf](https://github.com/rapidsai/cudf) + [cuml](https://github.com/rapidsai/cuml) \n",
    "\n",
    "All of these code paths are available in the `/code/workflows` directory for your reference. \n",
    "\n",
    "> **Note that the single-CPU option will leverage multiple cores in the model training portion of the workflow; however, to unlock full parallelism in each stage of the workflow we use [Dask](https://dask.org/). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please choose code variant\n",
    "ml_workflow_choice = 'singleGPU' \n",
    "\n",
    "assert (ml_workflow_choice in ['singleCPU', 'singleGPU', 'multiCPU', 'multiGPU'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: block; text-align: center; color:#8735fb; font-size:30pt\"> **1. ML Workflow** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/ml_workflow.png' width='800'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 1.1 - Dataset </span>\n",
    "<a id ='dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default settings for this demo are built to utilize the Airline dataset (Carrier On-Time Performance 1987-2020, available from the [Bureau of Transportation Statistics](https://transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline%20On-Time%20Performance%20Data&DB_Short_Name=On-Time#)). Below are some additional details about this dataset, we plan to offer a companion notebook that does a deep dive on the data science behind this dataset. Note that if you are using an alternate dataset (e.g., NYC Taxi or BYOData) these details are not relevant.\n",
    "\n",
    "The public dataset contains logs/features about flights in the United States (17 airlines) including:\n",
    "\n",
    "* Locations and distance  ( `Origin`, `Dest`, `Distance` )\n",
    "* Airline / carrier ( `Reporting_Airline` )\n",
    "* Scheduled departure and arrival times ( `CRSDepTime` and `CRSArrTime` )\n",
    "* Actual departure and arrival times ( `DpTime` and `ArrTime` )\n",
    "* Difference between scheduled & actual times ( `ArrDelay` and `DepDelay` )\n",
    "* Binary encoded version of late, aka our target variable ( `ArrDelay15` )\n",
    "\n",
    "Using these features we will build a classifier model to predict whether a flight is going to be more than 15 minutes late on arrival as it prepares to depart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 1.2 - Python ML Workflow </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a RAPIDS enabled SageMaker HPO we first need to build a [SageMaker Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html). An Estimator is a container image that captures all the software needed to run an HPO experiment. The container is augmented with entrypoint code that will be trggered at runtime by each worker. The entrypoint code enables us to write custom models and hook them up to data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with SageMaker HPO, the entrypoint logic should parse hyperparameters (supplied by AWS SageMaker), load and split data, build and train a model, score/evaluate the trained model, and emit an output representing the final score for the given hyperparameter setting. We've already built multiple variations of this code.\n",
    "\n",
    "If you would like to make changes by adding your custom model logic feel free to modify the **train.py** and/or the specific workflow files in the `code/workflows` directory. You are also welcome to uncomment the cells below to load the read/review the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's switch our working directory to the location of the Estimator entrypoint and library code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/updated3/cloud-ml-examples/aws/code\n"
     ]
    }
   ],
   "source": [
    "%cd code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "aws/code\n",
    "├── Dockerfile\n",
    "├── entrypoint.sh\n",
    "├── HPOConfig.py\n",
    "├── HPODatasets.py\n",
    "├── MLWorkflow.py\n",
    "├── serve.py\n",
    "├── train.py\n",
    "└── workflows\n",
    "    ├── MLWorkflowMultiCPU.py\n",
    "    ├── MLWorkflowMultiGPU.py\n",
    "    ├── MLWorkflowSingleCPU.py\n",
    "    └── MLWorkflowSingleGPU.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load workflows/MLWorkflowSingleGPU.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: block; text-align: center; color:#8735fb; font-size:30pt\"> **2. Build Estimator** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/estimator.png' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've already mentioned, the SageMaker Estimator represents the containerized software stack that AWS SageMaker will replicate to each worker node.\n",
    "\n",
    "The first step to building our Estimator, is to augment a RAPIDS container with our ML Workflow code from above, and push this image to Amazon Elastic Cloud Registry so it is available to SageMaker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 2.1 - Containerize and Push to ECR </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn to building our container so that it can integrate with the AWS SageMaker HPO API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our container can either be built on top of the latest RAPIDS [ nightly ] image as a starting layer or the RAPIDS stable image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rapids_base_container = 'rapidsai/rapidsai-cloud-ml:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also decide on the full name of our container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base = 'cloud-ml-sagemaker'\n",
    "image_tag  = rapids_base_container.split(':')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_fullname = f\"{account[0]}.dkr.ecr.{region[0]}.amazonaws.com/{image_base}:{image_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'561241433344.dkr.ecr.us-east-1.amazonaws.com/cloud-ml-sagemaker:latest'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecr_fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> 2.1.1 - Write Dockerfile </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write out the Dockerfile to disk, and in a few cells execute the docker build command. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write our selected RAPDIS image layer as the first FROM statement in the the Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dockerfile', 'w') as dockerfile: \n",
    "    dockerfile.writelines( f'FROM {rapids_base_container} \\n\\n'\n",
    "                           f'ENV AWS_DATASET_DIRECTORY=\"{dataset_directory}\"\\n'\n",
    "                           f'ENV AWS_ALGORITHM_CHOICE=\"{algorithm_choice}\"\\n'\n",
    "                           f'ENV AWS_ML_WORKFLOW_CHOICE=\"{ml_workflow_choice}\"\\n'\n",
    "                           f'ENV AWS_CV_FOLDS=\"{cv_folds}\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's append write the remaining pieces of the Dockerfile, namely adding the sagemaker-training-toolkit, flask, dask-ml, and copying our python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a Dockerfile\n",
    "\n",
    "# ensure printed output/log-messages retain correct order\n",
    "ENV PYTHONUNBUFFERED=True\n",
    "\n",
    "# path where SageMaker looks for code when container runs in the cloud\n",
    "ENV CLOUD_PATH=\"/opt/ml/code\"\n",
    "\n",
    "# copy our latest [local] code into the container \n",
    "COPY . $CLOUD_PATH\n",
    "\n",
    "# make the entrypoint script executable\n",
    "RUN chmod +x $CLOUD_PATH/entrypoint.sh\n",
    "\n",
    "WORKDIR $CLOUD_PATH\n",
    "ENTRYPOINT [\"./entrypoint.sh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's ensure that our Dockerfile correctly captured our base image selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM rapidsai/rapidsai-cloud-ml:latest \n",
      "\n",
      "ENV AWS_DATASET_DIRECTORY=\"3_year\"\n",
      "ENV AWS_ALGORITHM_CHOICE=\"XGBoost\"\n",
      "ENV AWS_ML_WORKFLOW_CHOICE=\"singleGPU\"\n",
      "ENV AWS_CV_FOLDS=\"3\"\n",
      "\n",
      "# ensure printed output/log-messages retain correct order\n",
      "ENV PYTHONUNBUFFERED=True\n",
      "\n",
      "# path where SageMaker looks for code when container runs in the cloud\n",
      "ENV CLOUD_PATH=\"/opt/ml/code\"\n",
      "\n",
      "# copy our latest [local] code into the container \n",
      "COPY . $CLOUD_PATH\n",
      "\n",
      "# make the entrypoint script executable\n",
      "RUN chmod +x $CLOUD_PATH/entrypoint.sh\n",
      "\n",
      "WORKDIR $CLOUD_PATH\n",
      "ENTRYPOINT [\"./entrypoint.sh\"]\n"
     ]
    }
   ],
   "source": [
    "validate_dockerfile(rapids_base_container)\n",
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> 2.1.2 Build and Tag </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The build step will be dominated by the download of the RAPIDS image (base layer). If it's already been downloaded the build will take less than 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: Pulling from rapidsai/rapidsai-cloud-ml\n",
      "Digest: sha256:d7e654e302cf1c3d0784533875963fb6d4c92a2ba286a665c5c907768314edb1\n",
      "Status: Image is up to date for rapidsai/rapidsai-cloud-ml:latest\n",
      "docker.io/rapidsai/rapidsai-cloud-ml:latest\n"
     ]
    }
   ],
   "source": [
    "!docker pull $rapids_base_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  148.5kB\n",
      "Step 1/11 : FROM rapidsai/rapidsai-cloud-ml:latest\n",
      " ---> 02d8a62f6d2c\n",
      "Step 2/11 : ENV AWS_DATASET_DIRECTORY=\"3_year\"\n",
      " ---> Using cache\n",
      " ---> cc635c1758c8\n",
      "Step 3/11 : ENV AWS_ALGORITHM_CHOICE=\"XGBoost\"\n",
      " ---> Using cache\n",
      " ---> 4be2669ce87e\n",
      "Step 4/11 : ENV AWS_ML_WORKFLOW_CHOICE=\"singleGPU\"\n",
      " ---> Using cache\n",
      " ---> fc30a90dbb86\n",
      "Step 5/11 : ENV AWS_CV_FOLDS=\"3\"\n",
      " ---> Using cache\n",
      " ---> e4d57f0893f0\n",
      "Step 6/11 : ENV PYTHONUNBUFFERED=True\n",
      " ---> Using cache\n",
      " ---> fba4d538bb57\n",
      "Step 7/11 : ENV CLOUD_PATH=\"/opt/ml/code\"\n",
      " ---> Using cache\n",
      " ---> f323001ce6dd\n",
      "Step 8/11 : COPY . $CLOUD_PATH\n",
      " ---> Using cache\n",
      " ---> e4ccba1d9ed2\n",
      "Step 9/11 : RUN chmod +x $CLOUD_PATH/entrypoint.sh\n",
      " ---> Using cache\n",
      " ---> 0f9d2cf570af\n",
      "Step 10/11 : WORKDIR $CLOUD_PATH\n",
      " ---> Using cache\n",
      " ---> db881b5a4eeb\n",
      "Step 11/11 : ENTRYPOINT [\"./entrypoint.sh\"]\n",
      " ---> Using cache\n",
      " ---> 6917417b835a\n",
      "Successfully built 6917417b835a\n",
      "Successfully tagged 561241433344.dkr.ecr.us-east-1.amazonaws.com/cloud-ml-sagemaker:latest\n",
      "CPU times: user 4.56 ms, sys: 8.1 ms, total: 12.7 ms\n",
      "Wall time: 322 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!docker build . -t $ecr_fullname -f Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> 2.1.3 - Publish to Elastic Cloud Registry (ECR) </span>\n",
    "\n",
    "Now that we've built and tagged our container its time to push it to Amazon's container registry (ECR). Once in ECR, AWS SageMaker will be able to leverage our image to build Estimators and run experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker Login to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_login_str = !(aws ecr get-login --region {region[0]} --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!{docker_login_str[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ECR repository [ if it doesn't already exist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_query = !(aws ecr describe-repositories --repository-names $image_base)\n",
    "if repository_query[0] == '':\n",
    "    !(aws ecr create-repository --repository-name $image_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now actually push the container to ECR\n",
    "> Note the first push to ECR may take some time (hopefully less than 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [561241433344.dkr.ecr.us-east-1.amazonaws.com/cloud-ml-sagemaker]\n",
      "\n",
      "\u001b[1B18701c81: Preparing \n",
      "\u001b[1B59dad267: Preparing \n",
      "\u001b[1B3ffe903b: Preparing \n",
      "\u001b[1B32fbf55b: Preparing \n",
      "\u001b[1B189ca3a3: Preparing \n",
      "\u001b[1Bb97c445b: Preparing \n",
      "\u001b[1B78e3bf48: Preparing \n",
      "\u001b[1B8b120579: Preparing \n",
      "\u001b[1B87e0621d: Preparing \n",
      "\u001b[1B7ad6008c: Preparing \n",
      "\u001b[1Bdd8ed907: Preparing \n",
      "\u001b[1B872b888e: Preparing \n",
      "\u001b[1B512fd434: Preparing \n",
      "\u001b[1B31fc0e08: Preparing \n",
      "\u001b[1B8308da3d: Layer already exists \u001b[10A\u001b[2K\u001b[8A\u001b[2K\u001b[4A\u001b[2Klatest: digest: sha256:29a7a49d0a8499fbbd9bd68b988b5ec7647045036f3baa77c330d35ad3ac1d25 size: 3475\n"
     ]
    }
   ],
   "source": [
    "!docker push $ecr_fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 2.2 - Create Estimator </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built our container [ +custom logic] and pushed it to ECR, we can finally compile all of efforts into an Estimator instance in SageMaker Studio. Navigate to sagemaker_studio/rapids_studio_hpo.ipynb to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:25pt\"> **Appendix: Bring Your Own Dataset Checklist** </span>\n",
    "<a id ='byod'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan to use your own dataset (BYOD) here is a checklist to help you integrate into the workflow:\n",
    "\n",
    "> - [ ] Dataset should be in either CSV or Parquet format.\n",
    "> - [ ] Dataset is already pre-processed (and all feature-engineering is done).\n",
    "> - [ ] Dataset is uploaded to S3 and `data_bucket` and `dataset_directory` have been set to the location of your data.\n",
    "> - [ ] Dataset feature and target columns have been enumerated in `/code/HPODataset.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:25pt\"> **Rapids References** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [cloud-ml-examples](http://github.com/rapidsai/cloud-ml-examples)\n",
    "\n",
    "> [RAPIDS HPO](https://rapids.ai/hpo)\n",
    "\n",
    "> [cuML Documentation](https://docs.rapids.ai/api/cuml/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:25pt\"> **SageMaker References** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [SageMaker Training Toolkit](https://github.com/aws/sagemaker-training-toolkit)\n",
    "\n",
    "> [Estimator Parameters](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)\n",
    "\n",
    "> Spot Instances [docs](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html), and [blog]()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
