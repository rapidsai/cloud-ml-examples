{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Optimization with NVIDIA RAPIDS + AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying domain knowledge, intuition, and experimentation to build a successful model, data scientists typically run hyper-parameter-optimization (HPO) to find a champion model and reach highest performance before deploying to production. \n",
    "\n",
    "HPO searches over models by trying different settings of 'architecture parameters,' parameters not usually optimized by the learning algorithm -- i.e., *maximum depth* and *number-of-trees* in a random forest model, or the *number-of-layers* and *neurons-per-layer* of a neural network. \n",
    "\n",
    "Often HPO can improve the generalization quality of a model by 5-15% relative to hand tuned or default model parameters. But there is a problem, HPO is very computationally expensive (we are searching over model architectures not just individual parameters) and can be very slow.\n",
    "\n",
    "In this notebook we show how we can overcome the computational complexity of HPO by combining two superpowers -- the *scaling power* of the cloud, and the *speed* of the GPU. By using these two super-powers we can vastly accelerate HPO, and best of all you can use these superpowers too! Once you've gone through this content you should be able to plug in custom code and data so you can accelerate HPO on **your ML problem**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it Works: HPO on AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS SageMaker provides a work orchestrator for HPO. Given an Estimator object ( essentially containerized model code -- more on this soon), data, and hyper-parameter ranges SageMaker will use a search strategy to try various combinations of hyper-parameters (i.e., experiment) within the admissable ranges and report back on their performance, ultimately reporting on the best performing combination.\n",
    "\n",
    "While we expect the search strategy choices to grow, currently AWS SageMaker only supports **Random** and **Bayesian** search. \n",
    "\n",
    "- The **Random** strategy is as its name implies, randomly sampling in the possible ranges with no concern for past experiments.\n",
    "\n",
    "- The **Bayesian** strategy tries several parallel experiments and then uses regression to pick the next batch of hyper-parameters.\n",
    "\n",
    "In this notebook we'll be using the Random strategy, though you are welcome to switch by changing the .\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/miroenev/aws-rapids/master/figures/HPO_motivation.png' width='1000px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize AWS SageMaker Account & Session Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get things rolling lets make sure we can query our AWS SageMaker execution role and session as well as our account ID and AWS region [ we'll need this info later on ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sm_execution_role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "account=!(aws sts get-caller-identity --query Account --output text)\n",
    "region=!(aws configure get region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification of Airline Delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll be leveraging the RAPIDS **cuml.RandomForest** classifier model to try to predict airline arrival delays (see the Dataset section below for more details). To find the best performing model we'll search across three hyper-parameters that control the architecture of the Random Forest \n",
    "\n",
    "- **maximum_depth**: the maximum possible depth of any tree\n",
    "- **n_estimators**: the number of trees in the forest\n",
    "- **max_features**: the fraction of features used to determine splits in the trees\n",
    "<!-- <img src='https://raw.githubusercontent.com/miroenev/aws-rapids/master/figures/tree_depth.png' width='75%'>\n",
    "<center>Sample Decision Tree of max_depth = 7</center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we'll utilize the Airline dataset (Carrier On-Time Performance 1987-2020, available from the [Bureau of Transportation Statistics](https://transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline%20On-Time%20Performance%20Data&DB_Short_Name=On-Time#)). \n",
    "\n",
    "Specifically we'll try to classify whether a flight is going to be more than 15 minutes late on arrival, for the last 10years of data [ 2019-2009 ]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each flight the features in the data include information about time, the airline, source and destination airports, distance, and departure delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a cleaned version of our dataset on a public S3 bucket, which we specify here and will subsequently use as an input to our HPO Estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bucket = 'cloud-ml-examples'\n",
    "target_bucket_prefix = '10_years'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_training = 's3://{}/{}'.format(target_bucket, target_bucket_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://cloud-ml-examples/10_years'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_input_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - BYOContainer / Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a RAPIDS enabled SageMaker HPO we first need to build an Estimator. \n",
    "\n",
    "An Estimator is a docker container image that captures all the software needed to run an HPO experiment.\n",
    "\n",
    "The container is augmented with special **entrypoint code** that will be triggered at runtime by each worker. \n",
    "\n",
    "The entrypoint code enables us to write custom models and hook them up to data. \n",
    "\n",
    "In order to work with SageMaker HPO, the entrypoint logic should parse hyper-parameters (supplied by AWS SageMaker), load and split data, build and train a model, score/evaluate the trained model, and emit an output representing the final score for the given hyper-parameter setting.\n",
    "\n",
    "We've already built sample entrypoint code leveraging the cuml.RandomForest classifier model. If you would like to make changes by adding your custom model logic feel free to modify the **train.py** file.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Build Custom Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/miroenev/aws-rapids/master/figures/estimator.png'>\n",
    "\n",
    "If you want to dig into the custom code, check out the train.py script as well as its supporting library rapids_cloud_ml.py.\n",
    "\n",
    "By default we'll run with 10 years of the airline dataset, however if you would like to point the code at your own data, just modify the top few lines of train.py and be sure that the `dataset_columns` (columns/features of you dataset) and `target_variable` (the label column which will be the classification target) match your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Containerize Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets turn to building our container so that it can integrate with the AWS SageMaker HPO API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our container takes the latest RAPIDS [ nightly ] image as a starting layer, adds some bits to inter-operate with AWS SageMaker (i.e., github.com/aws/sagemaker-containers), and copies in custom entypoint code that will run when the Estimator is spawned. We'll discuss the custom logic in the section below, for now lets actually build our container and push it to the Amazon Elastic Container Registry (ECR). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Define Container Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide on the full name of our container `image_base:image_tag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base = 'cloud-ml-sagemaker'\n",
    "image_tag = 'runtime-0.14-10.1-18.04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_fullname=f\"{account[0]}.dkr.ecr.{region[0]}.amazonaws.com/{image_base}:{image_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'611520507156.dkr.ecr.us-west-2.amazonaws.com/cloud-ml-sagemaker:runtime-0.14-10.1-18.04'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecr_fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Kick-off image download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be sure we have the latest bits by pulling the nightly RAPDIS build.\n",
    "> Note: This may take a few minutes since we are downloading the latest stable rapids image/container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14-cuda10.1-runtime-ubuntu18.04-py3.7: Pulling from rapidsai/rapidsai-nightly\n",
      "Digest: sha256:b3861b13d8229388a1f8b8d8380d14f25becfe44d9e64240b99101ceec3793b2\n",
      "Status: Image is up to date for rapidsai/rapidsai-nightly:0.14-cuda10.1-runtime-ubuntu18.04-py3.7\n"
     ]
    }
   ],
   "source": [
    "!docker pull rapidsai/rapidsai-nightly:0.14-cuda10.1-runtime-ubuntu18.04-py3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 - Write Dockerfile\n",
    "We write out the Dockerfile in this cell, write it to disk, and in the next cell execute the docker build command.\n",
    "> Note that we're copying in custom logic [ train.py, rapids_csp. py ] that we'll be defining shortly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "%%writefile container/Dockerfile\n",
    "#FROM rapidsai/rapidsai-nightly:0.13-cuda10.1-base-ubuntu18.04-py3.7\n",
    "FROM rapidsai/rapidsai:0.13-cuda10.1-base-ubuntu18.04-py3.7\n",
    "    \n",
    "ENV PYTHONUNBUFFERED=TRUE \\\n",
    "    PYTHONDONTWRITEBYTECODE=TRUE \\\n",
    "    CLOUD_PATH=\"/opt/ml/code\"\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends build-essential\n",
    "RUN source activate rapids && pip install sagemaker-containers\n",
    "\n",
    "COPY container/rapids_csp.py $CLOUD_PATH/rapids_csp.py\n",
    "COPY container/train.py $CLOUD_PATH/train.py\n",
    "ENV SAGEMAKER_PROGRAM $CLOUD_PATH/train.py\n",
    "\n",
    "WORKDIR $CLOUD_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting container/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile container/Dockerfile\n",
    "FROM rapidsai/rapidsai-nightly:0.14-cuda10.1-runtime-ubuntu18.04-py3.7\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE \\\n",
    "    PYTHONDONTWRITEBYTECODE=TRUE \\\n",
    "    CLOUD_PATH=\"/opt/ml/code\"\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends build-essential\n",
    "RUN source activate rapids && pip install sagemaker-containers\n",
    "\n",
    "COPY container/rapids_cloud_ml.py $CLOUD_PATH/rapids_cloud_ml.py\n",
    "COPY container/train.py $CLOUD_PATH/train.py\n",
    "ENV SAGEMAKER_PROGRAM $CLOUD_PATH/train.py\n",
    "\n",
    "WORKDIR $CLOUD_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Build and Tag\n",
    "The build usually take less than 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   3.65MB\n",
      "Step 1/8 : FROM rapidsai/rapidsai-nightly:0.14-cuda10.1-runtime-ubuntu18.04-py3.7\n",
      " ---> c53141e217b9\n",
      "Step 2/8 : ENV PYTHONUNBUFFERED=TRUE     PYTHONDONTWRITEBYTECODE=TRUE     CLOUD_PATH=\"/opt/ml/code\"\n",
      " ---> Using cache\n",
      " ---> 715fc2e3a8ed\n",
      "Step 3/8 : RUN apt-get update && apt-get install -y --no-install-recommends build-essential\n",
      " ---> Using cache\n",
      " ---> 3f2f34119deb\n",
      "Step 4/8 : RUN source activate rapids && pip install sagemaker-containers\n",
      " ---> Using cache\n",
      " ---> 29fb094be5c7\n",
      "Step 5/8 : COPY container/rapids_cloud_ml.py $CLOUD_PATH/rapids_cloud_ml.py\n",
      " ---> Using cache\n",
      " ---> 2e51b9998921\n",
      "Step 6/8 : COPY container/train.py $CLOUD_PATH/train.py\n",
      " ---> Using cache\n",
      " ---> b9a5d4904758\n",
      "Step 7/8 : ENV SAGEMAKER_PROGRAM $CLOUD_PATH/train.py\n",
      " ---> Using cache\n",
      " ---> 8d5ed55ce194\n",
      "Step 8/8 : WORKDIR $CLOUD_PATH\n",
      " ---> Using cache\n",
      " ---> b7233f6ef7cf\n",
      "Successfully built b7233f6ef7cf\n",
      "Successfully tagged 611520507156.dkr.ecr.us-west-2.amazonaws.com/cloud-ml-sagemaker:runtime-0.14-10.1-18.04\n",
      "CPU times: user 3.6 ms, sys: 7.86 ms, total: 11.5 ms\n",
      "Wall time: 271 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!docker build . -t $ecr_fullname -f container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Publish Container to Elastic Cloud Registry (ECR)\n",
    "Now that we've built and tagged our container its time to push it to Amazon's container registry (ECR). Once in ECR, AWS SageMaker will be able to leverage our image to build Estimators and run experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Docker Login to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_login_str = !(aws ecr get-login --region {region[0]} --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!{docker_login_str[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Create ECR repository [ if it doesn't already exist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_query = !(aws ecr describe-repositories --repository-names $image_base)\n",
    "if repository_query[0] == '':\n",
    "    !(aws ecr create-repository --repository-name $image_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Push to ECR\n",
    "> Note the first push to ECR may take some time (hopefully less than 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'611520507156.dkr.ecr.us-west-2.amazonaws.com/cloud-ml-sagemaker:runtime-0.14-10.1-18.04'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecr_fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [611520507156.dkr.ecr.us-west-2.amazonaws.com/cloud-ml-sagemaker]\n",
      "\n",
      "\u001b[1B876fbbfd: Preparing \n",
      "\u001b[1Bdf249d74: Preparing \n",
      "\u001b[1B6607916d: Preparing \n",
      "\u001b[1B62af22b0: Preparing \n",
      "\u001b[1B295fcef4: Preparing \n",
      "\u001b[1B8835dada: Preparing \n",
      "\u001b[1Bbd9a1af1: Preparing \n",
      "\u001b[1B489c106b: Preparing \n",
      "\u001b[1B74f76be4: Preparing \n",
      "\u001b[1Bd332a58a: Preparing \n",
      "\u001b[1Bf11cbf29: Preparing \n",
      "\u001b[1Ba4b22186: Preparing \n",
      "\u001b[1Bafb09dc3: Preparing \n",
      "\u001b[1Bb5a53aac: Preparing \n",
      "\u001b[1Bc8e5063e: Preparing \n",
      "\u001b[1B7c529ced: Layer already exists \u001b[16A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[Kruntime-0.14-10.1-18.04: digest: sha256:5fbe69f23047cb46f64391b3c2a4d27351efc04f8cc8de34f5322f1cd9b1bdf3 size: 3689\n"
     ]
    }
   ],
   "source": [
    "!docker push $ecr_fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - Map Container to Estimator using SageMaker Python SDK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built our container [ +custom logic] and pushed it to ECR, we can finally compile all of efforts into an **Estimator** object -- you can think of the Estimator as the software stack that AWS SageMaker will replicate to each worker node.\n",
    "\n",
    "We'll build the Estimator using our SageMaker execution role, the ECR image we built/tagged, and add an output path to [optionally] save models trained during the HPO experimentation.\n",
    "\n",
    "For additional options and details see the [Estimator documentation](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator) (e.g., to change the size in GB of the EBS volume to use for storing input data during training, default = 30GB )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type_GPU = 'ml.p3.8xlarge' # #'ml.g4dn.xlarge' # 'ml.p3.2xlarge' # 'ml.g4dn.4xlarge'\n",
    "train_instance_type_CPU = 'ml.c5.4xlarge'\n",
    "\n",
    "train_instance_type = train_instance_type_GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ml.p3.8xlarge'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_estimator = sagemaker.estimator.Estimator( sagemaker_session = sm_session, \n",
    "                                              role = sm_execution_role,\n",
    "                                              image_name = ecr_fullname,\n",
    "                                              train_instance_count = 1, \n",
    "                                              train_instance_type = train_instance_type,                                               \n",
    "                                              input_mode = 'File', \n",
    "                                              output_path = f's3://{target_bucket}/{target_bucket_prefix}/output' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Test the Estimator [ optional ]\n",
    "Now that we have an AWS SageMaker Estimator built up, we can feed it data and ask it to train. \n",
    "\n",
    "This is a useful step if you've made changes to your custom logic and are interested in making sure everything works before launching a large HPO search. \n",
    "\n",
    "To trigger this debugging logic  just uncomment and run the cell below.\n",
    "> Note: This verification step will use the default hyper-parameter values declared in our custom train code, as SageMaker HPO will not be orchestrating this single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-13 23:03:47 Starting - Starting the training job...\n",
      "2020-05-13 23:03:48 Starting - Launching requested ML instances......\n",
      "2020-05-13 23:04:49 Starting - Preparing the instances for training......\n",
      "2020-05-13 23:06:10 Downloading - Downloading input data\n",
      "2020-05-13 23:06:10 Training - Downloading the training image..................\n",
      "2020-05-13 23:09:09 Training - Training image download completed. Training in progress..\u001b[34m2020-05-13 23:09:10,625 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"estimator-mgpu-CV-1-3\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"/opt/ml/code/train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"/opt/ml/code/train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=/opt/ml/code/train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=/opt/ml/code/train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"estimator-mgpu-CV-1-3\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"/opt/ml/code/train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"/opt/ml/code/train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/envs/rapids/bin:/opt/conda/envs/rapids/lib/python37.zip:/opt/conda/envs/rapids/lib/python3.7:/opt/conda/envs/rapids/lib/python3.7/lib-dynload:/opt/conda/envs/rapids/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/envs/rapids/bin/python3.7 /opt/ml/code/train.py\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mparsing job config from job filename...\n",
      "\u001b[0m\n",
      "\u001b[34m{    'CV_folds': 1,\n",
      "     'compute': 'multi-GPU',\n",
      "     'dataset': 'airline',\n",
      "     'dataset_filename': '*.parquet',\n",
      "     'model_type': 'RandomForest'}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mparsing model hyper-parameters from command line arguments...\n",
      "\u001b[0m\n",
      "\u001b[34m{'max_depth': 6, 'max_features': 0.5, 'n_estimators': 100, 'seed': 0}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mdask multi-GPU cluster with 4 workers \n",
      "\u001b[0m\n",
      "\u001b[34m> loading dataset from /opt/ml/input/data/training/*.parquet...\n",
      "\u001b[0m\n",
      "\u001b[34m|-> ingestion_timer : 5.0436\n",
      "\u001b[0m\n",
      "\u001b[34mdataset len : 61390973\n",
      " CV fold : 0 \n",
      "\u001b[0m\n",
      "\u001b[34m|-> split_timer : 0.0323\n",
      "\u001b[0m\n",
      "\u001b[34m|-> persist_timer : 6.4586\n",
      "\u001b[0m\n",
      "\u001b[34m|-> train_timer : 5.1406\n",
      "\u001b[0m\n",
      "\u001b[34m|-> score_timer : 0.5091\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#011 test-accuracy: 0.9297811388969421; \n",
      "\u001b[0m\n",
      "\u001b[34m2020-05-13 23:09:45,851 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-05-13 23:09:56 Uploading - Uploading generated training model\n",
      "2020-05-13 23:09:56 Completed - Training job completed\n",
      "Training seconds: 242\n",
      "Billable seconds: 242\n"
     ]
    }
   ],
   "source": [
    "sm_estimator.fit(inputs = s3_input_training, job_name = 'estimator-mgpu-CV-1-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - HPO\n",
    "With a working AWS SageMaker Estimator in hand, the hardest part is behind us!\n",
    "\n",
    "Now all we have to do is tell SageMaker about the space of hyper-parameters in which to search for the best model.\n",
    "\n",
    "For more documentation check out the AWS SageMaker [HyperParameter Tuner documentation](https://sagemaker.readthedocs.io/en/stable/tuner.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Define Search Ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important choices when running HPO is to choose the bounds of the hyper-parameter search process. \n",
    "\n",
    "Below we've set the ranges of the hyper-parameters to allow for significant variation in all of the different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "from sagemaker.parameter import ContinuousParameter, IntegerParameter, ParameterRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_hyperparameter_ranges = {\n",
    "    'max_depth'    : IntegerParameter    ( 3,  15  ),\n",
    "    'n_estimators' : IntegerParameter    ( 100, 500 ),\n",
    "    'max_features' : ContinuousParameter ( 0.2, 1.0 ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Define Metric\n",
    "The definitions below specify a regular expressions (i.e., string parsing rules) to find the metrics which we are using to evalaute performance in the output log of each worker/Estimator. In this case we are case we are onyl interested in the performance of our model on the test data (i.e., `test-accuracy`), so we have a single metric to track.\n",
    "\n",
    "For additional details on metrics refer to the [AWS SageMaker documentation on Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[{'Name': 'test-accuracy', 'Regex': 'test-accuracy: (.*);'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'test-accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Build HPO Tuner using SageMaker Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are setting up the parameters that will define the HPO job. By default (to avoid accidently spawning large compute jobs), we have limited the number of HPO experiments to run to 2.\n",
    "\n",
    "To run a more realistic large-scale HPO, change `max_jobs` to 100 and `max_parallel_jobs` to 10 (or as high as your instance limit permits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPO_experiment = {\n",
    "    'model_type' : 'rf', \n",
    "    'dataset' : 'airline',\n",
    "    'dataset_samples' : 20000000,\n",
    "    'compute_type': 'mGPU',\n",
    "    'strategy': 'Random',\n",
    "    'sm_estimator' : sm_estimator,\n",
    "    'metric_definitions' : metric_definitions,\n",
    "    'objective_metric_name' : objective_metric_name,\n",
    "    'hyperparameter_ranges' : random_forest_hyperparameter_ranges,\n",
    "    's3_input_training' : s3_input_training,    \n",
    "    'objective_type': 'Maximize', \n",
    "    'max_jobs': 4,\n",
    "    'max_parallel_jobs': 2,\n",
    "    'CV_folds' : 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = sagemaker.tuner.HyperparameterTuner( estimator = HPO_experiment['sm_estimator'],\n",
    "                                           metric_definitions = HPO_experiment['metric_definitions'], \n",
    "                                           objective_metric_name = HPO_experiment['objective_metric_name'],\n",
    "                                           objective_type = HPO_experiment['objective_type'],\n",
    "                                           hyperparameter_ranges = HPO_experiment['hyperparameter_ranges'],\n",
    "                                           strategy = HPO_experiment['strategy'],  \n",
    "                                           max_jobs = HPO_experiment['max_jobs'],\n",
    "                                           max_parallel_jobs = HPO_experiment['max_parallel_jobs'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/miroenev/aws-rapids/master/figures/max_jobs.png' width='800px'>\n",
    "<img src='https://raw.githubusercontent.com/miroenev/aws-rapids/master/figures/max_parallel.png' width='500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Build HPO Job Name \n",
    "Using these HPO parameters we'll build up a unique name for this HPO job. \n",
    "> Note that the entrypoint script relies on the job name to determine some configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tag = '4gpu'\n",
    "HPO_experiment['experiment_name'] = f\"{HPO_experiment['model_type']}-{HPO_experiment['compute_type']}-CV-{HPO_experiment['CV_folds']}-{HPO_experiment['dataset_samples']}-{custom_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_name = HPO_experiment['experiment_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rf-mGPU-CV-1-20000000-4gpu'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 - Run HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................................................................................................!\n",
      "839.910438818999\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "hpo.fit( inputs = HPO_experiment['s3_input_training'], \n",
    "         job_name = HPO_experiment['experiment_name'], wait = True, logs = 'All')    \n",
    "hpo.wait() # block until the .fit call above is completed\n",
    "\n",
    "HPO_job_total_time = time.perf_counter() - start_time\n",
    "print(HPO_job_total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name).dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.928403</td>\n",
       "      <td>236.0</td>\n",
       "      <td>2020-05-13 23:35:17+00:00</td>\n",
       "      <td>rf-mGPU-CV-1-20000000-4gpu-004-ccc2e1e6</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-05-13 23:31:21+00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.445147</td>\n",
       "      <td>251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.925242</td>\n",
       "      <td>283.0</td>\n",
       "      <td>2020-05-13 23:36:04+00:00</td>\n",
       "      <td>rf-mGPU-CV-1-20000000-4gpu-003-0068929f</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-05-13 23:31:21+00:00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.660389</td>\n",
       "      <td>322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.929506</td>\n",
       "      <td>227.0</td>\n",
       "      <td>2020-05-13 23:28:44+00:00</td>\n",
       "      <td>rf-mGPU-CV-1-20000000-4gpu-002-d17393b7</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-05-13 23:24:57+00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.363640</td>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.929304</td>\n",
       "      <td>244.0</td>\n",
       "      <td>2020-05-13 23:28:52+00:00</td>\n",
       "      <td>rf-mGPU-CV-1-20000000-4gpu-001-6901c70a</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-05-13 23:24:48+00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.375529</td>\n",
       "      <td>473.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FinalObjectiveValue  TrainingElapsedTimeSeconds           TrainingEndTime  \\\n",
       "0             0.928403                       236.0 2020-05-13 23:35:17+00:00   \n",
       "1             0.925242                       283.0 2020-05-13 23:36:04+00:00   \n",
       "2             0.929506                       227.0 2020-05-13 23:28:44+00:00   \n",
       "3             0.929304                       244.0 2020-05-13 23:28:52+00:00   \n",
       "\n",
       "                           TrainingJobName TrainingJobStatus  \\\n",
       "0  rf-mGPU-CV-1-20000000-4gpu-004-ccc2e1e6         Completed   \n",
       "1  rf-mGPU-CV-1-20000000-4gpu-003-0068929f         Completed   \n",
       "2  rf-mGPU-CV-1-20000000-4gpu-002-d17393b7         Completed   \n",
       "3  rf-mGPU-CV-1-20000000-4gpu-001-6901c70a         Completed   \n",
       "\n",
       "          TrainingStartTime  max_depth  max_features  n_estimators  \n",
       "0 2020-05-13 23:31:21+00:00        4.0      0.445147         251.0  \n",
       "1 2020-05-13 23:31:21+00:00       13.0      0.660389         322.0  \n",
       "2 2020-05-13 23:24:57+00:00        7.0      0.363640         167.0  \n",
       "3 2020-05-13 23:24:48+00:00        4.0      0.375529         473.0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Gains with RAPIDS & GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are seeing greater than 40x acceleration of model training on the GPU (cuml vs sklearn Random Forest).\n",
    "Stay tuned for more performance numbers coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "AWS SageMaker + NVIDIA RAPIDS HPO FTW!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
