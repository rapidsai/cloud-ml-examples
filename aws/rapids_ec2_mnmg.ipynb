{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa952e5-d969-4681-ab29-e8c681007bd7",
   "metadata": {},
   "source": [
    "# Multi-node multi-GPU example on AWS using dask-cloudprovider\n",
    "\n",
    "[Dask Cloud Provider](https://cloudprovider.dask.org/en/latest/) is a native cloud integration for dask. It helps manage Dask clusters on different cloud platforms. In this notebook, we will look at how we can use the package to set-up a AWS cluster and run a multi-node multi-GPU (MNMG) example with [RAPIDS](https://rapids.ai/). RAPIDS provides a suite of libraries to accelerate data science pipelines on the GPU entirely. This can be scaled to multiple nodes using Dask as we will see through this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05249d57-7b73-45fc-b0a6-25b3cd8a9f78",
   "metadata": {},
   "source": [
    "### Local Environment Setup\n",
    "\n",
    "1. Before running the notebook, ensure you have installed RAPIDS locally. Follow the instructions in [README.md](https://github.com/rapidsai/cloud-ml-examples/blob/main/aws/README.md) to install RAPIDS locally.\n",
    "\n",
    "2. Install and set up AWS CLI using the following commands:\n",
    "\n",
    "```bash\n",
    "pip install awscli\n",
    "aws configure\n",
    "\n",
    "```\n",
    "The list of packages needed for this notebook is listed in the cell below - uncomment and run the cell to set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d1b0e0-8d6a-4f49-a527-982230487e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"dask-cloudprovider[aws]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93975bb-5529-428b-bfe5-27b394f7d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import cudf\n",
    "import dask\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from cuml.dask.ensemble import RandomForestRegressor\n",
    "from cuml.metrics import mean_squared_error\n",
    "from dask_cloudprovider.aws import EC2Cluster\n",
    "from dask.distributed import Client\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dateutil import parser\n",
    "import configparser, os, contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1920f-af3e-4e14-901c-3f81a68d1acc",
   "metadata": {},
   "source": [
    "## AWS Cluster Set-up\n",
    "We'll now setup the [EC2Cluster](https://cloudprovider.dask.org/en/latest/aws.html#elastic-compute-cloud-ec2) from Dask Cloud Provider. To do this, you'll first need to run `aws configure` and ensure the credentials are updated. [Learn more about the setup](https://cloudprovider.dask.org/en/latest/aws.html#authentication). The API also expects a security group that allows access to ports 8786-8787 and all traffic between instances in the security group. If you do not pass a group here, dask cloud provider will create one for you.\n",
    "\n",
    "Note: Make sure you have permissions in the account to create the necessary resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc86186-4208-4b4b-8024-9e4ac7404646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_aws_credentials():\n",
    "    parser = configparser.RawConfigParser()\n",
    "    parser.read(os.path.expanduser('~/.aws/config'))\n",
    "    config = parser.items('default')\n",
    "    parser.read(os.path.expanduser('~/.aws/credentials'))\n",
    "    credentials = parser.items('default')\n",
    "    all_credentials = {key.upper(): value for key, value in [*config, *credentials]}\n",
    "    with contextlib.suppress(KeyError):\n",
    "        all_credentials[\"AWS_REGION\"] = all_credentials.pop(\"REGION\")\n",
    "    return all_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1328a4-481a-4651-81d1-6ee4aa8cbc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 2\n",
    "n_gpus_per_worker = 4\n",
    "security_group = \"sg-dask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fd1a2-f842-429b-881e-4338e4250ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = EC2Cluster(env_vars=get_aws_credentials(),\n",
    "                     instance_type=\"p3.8xlarge\", # 4 V100 GPUs\n",
    "                     docker_image=\"rapidsai/rapidsai:21.08-cuda11.0-runtime-ubuntu18.04-py3.8\",\n",
    "                     worker_class=\"dask_cuda.CUDAWorker\",\n",
    "                     worker_options = {'rmm-managed-memory':True},\n",
    "                     security_groups=[security_group],\n",
    "                     docker_args = '--shm-size=256m',\n",
    "                     n_workers=n_workers,\n",
    "                     security=False,\n",
    "                     availability_zone=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eaf4f0-85f8-4b1a-9a81-a311eee8492a",
   "metadata": {},
   "source": [
    "## Client set up\n",
    "\n",
    "The cells below create a [Dask Client](https://distributed.dask.org/en/latest/client.html) with the cluster we defined earlier in the notebook accessing the VM. Once we have the client, we need to wait for the workers to be up and running, we do so by adding \n",
    "\n",
    "```python\n",
    "client.wait_for_workers(n_workers)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d362a49-0142-4bd9-9869-dbb7baca6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d03738-485b-4d8e-acf9-857c1d299d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "client.wait_for_workers(n_workers*n_gpus_per_worker)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10420c31-d545-468c-98f0-b8dde3488c79",
   "metadata": {},
   "source": [
    "## Machine Learning Workflow\n",
    "\n",
    "Once workers become available, we can now run the rest of our workflow:\n",
    "\n",
    "- read and clean the data\n",
    "- add features\n",
    "- split into training and validation sets\n",
    "- fit a Random Forest model\n",
    "- predict on the validation set\n",
    "- compute RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e2e4c-5857-4c91-a746-8266d7ffdbac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. Read and Clean Data\n",
    "\n",
    "The data needs to be cleaned up before it can be used in a meaningful way. We verify the columns have appropriate datatypes to make it ready for computation using cuML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279f1ee7-6e31-4abf-be94-dfc60e7bb8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of all columns & dtypes the df must have for reading\n",
    "col_dtype = {\n",
    "    'VendorID': 'int32',\n",
    "    'tpep_pickup_datetime': 'datetime64[ms]',\n",
    "    'tpep_dropoff_datetime': 'datetime64[ms]',\n",
    "    'passenger_count': 'int32',\n",
    "    'trip_distance': 'float32',\n",
    "    'pickup_longitude': 'float32',\n",
    "    'pickup_latitude': 'float32',\n",
    "    'RatecodeID': 'int32',\n",
    "    'store_and_fwd_flag': 'int32',\n",
    "    'dropoff_longitude': 'float32',\n",
    "    'dropoff_latitude': 'float32',\n",
    "    'payment_type':'int32',\n",
    "    'fare_amount': 'float32',\n",
    "    'extra':'float32',\n",
    "    'mta_tax':'float32',\n",
    "    'tip_amount': 'float32',\n",
    "    'total_amount': 'float32',\n",
    "    'tolls_amount': 'float32',\n",
    "    'improvement_surcharge': 'float32',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad3749-ed7d-423d-905c-c3006c4333c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = dask_cudf.read_csv(\"s3://nyc-tlc/trip data/yellow_tripdata_2016-02.csv\",\n",
    "                             storage_options={\"anon\": True},\n",
    "                             dtype=col_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b7a3f-6de4-48bd-bc59-1aa22802c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary of required columns and their datatypes\n",
    "must_haves = {\n",
    "    'pickup_datetime': 'datetime64[ms]',\n",
    "    'dropoff_datetime': 'datetime64[ms]',\n",
    "    'passenger_count': 'int32',\n",
    "    'trip_distance': 'float32',\n",
    "    'pickup_longitude': 'float32',\n",
    "    'pickup_latitude': 'float32',\n",
    "    'rate_code': 'int32',\n",
    "    'dropoff_longitude': 'float32',\n",
    "    'dropoff_latitude': 'float32',\n",
    "    'fare_amount': 'float32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c381da-dcc4-4217-872e-f590fa04f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(ddf, must_haves):\n",
    "    # replace the extraneous spaces in column names and lower the font type\n",
    "    tmp = {col:col.strip().lower() for col in list(ddf.columns)}\n",
    "    ddf = ddf.rename(columns=tmp)\n",
    "\n",
    "    ddf = ddf.rename(columns={\n",
    "        'tpep_pickup_datetime': 'pickup_datetime',\n",
    "        'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "        'ratecodeid': 'rate_code'\n",
    "    })\n",
    "    \n",
    "    ddf['pickup_datetime'] = ddf['pickup_datetime'].astype('datetime64[ms]')\n",
    "    ddf['dropoff_datetime'] = ddf['dropoff_datetime'].astype('datetime64[ms]')\n",
    "\n",
    "    for col in ddf.columns:\n",
    "        if col not in must_haves:\n",
    "            ddf = ddf.drop(columns=col)\n",
    "            continue\n",
    "        if ddf[col].dtype == 'object':\n",
    "            # Fixing error: could not convert arg to str\n",
    "            ddf = ddf.drop(columns=col)\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(ddf[col].dtype):\n",
    "                ddf[col] = ddf[col].astype('int32')\n",
    "            if 'float' in str(ddf[col].dtype):\n",
    "                ddf[col] = ddf[col].astype('float32')\n",
    "            ddf[col] = ddf[col].fillna(-1)\n",
    "    \n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe7e93-aed3-4c03-98f4-dc61354e82a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.map_partitions(clean, must_haves, meta=must_haves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38f7aa-3f7b-4d1f-87bd-7f3b1232a1a7",
   "metadata": {},
   "source": [
    "#### 2. Add Features\n",
    "\n",
    "We'll add new features to the dataframe:\n",
    "\n",
    "1. We can split the datetime column to retrive year, month, day, hour, day_of_week columns. Find the difference between pickup time and drop off time. \n",
    "2. Haversine Distance between the pick-up and drop-off coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aecc584-3e37-4727-b431-e7442b7cd248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## add features\n",
    "\n",
    "taxi_df['hour'] = taxi_df['pickup_datetime'].dt.hour.astype('int32')\n",
    "taxi_df['year'] = taxi_df['pickup_datetime'].dt.year.astype('int32')\n",
    "taxi_df['month'] = taxi_df['pickup_datetime'].dt.month.astype('int32')\n",
    "taxi_df['day'] = taxi_df['pickup_datetime'].dt.day.astype('int32')\n",
    "taxi_df['day_of_week'] = taxi_df['pickup_datetime'].dt.weekday.astype('int32')\n",
    "taxi_df['is_weekend'] = (taxi_df['day_of_week']>=5).astype('int32')\n",
    "\n",
    "#calculate the time difference between dropoff and pickup.\n",
    "taxi_df['diff'] = taxi_df['dropoff_datetime'].astype('int32') - taxi_df['pickup_datetime'].astype('int32')\n",
    "taxi_df['diff']=(taxi_df['diff']/1000).astype('int32')\n",
    "\n",
    "taxi_df['pickup_latitude_r'] = taxi_df['pickup_latitude']//.01*.01\n",
    "taxi_df['pickup_longitude_r'] = taxi_df['pickup_longitude']//.01*.01\n",
    "taxi_df['dropoff_latitude_r'] = taxi_df['dropoff_latitude']//.01*.01\n",
    "taxi_df['dropoff_longitude_r'] = taxi_df['dropoff_longitude']//.01*.01\n",
    "\n",
    "taxi_df = taxi_df.drop('pickup_datetime', axis=1)\n",
    "taxi_df = taxi_df.drop('dropoff_datetime', axis=1)\n",
    "\n",
    "def haversine_dist(df):\n",
    "    import cuspatial\n",
    "    h_distance = cuspatial.haversine_distance(df['pickup_longitude'], df['pickup_latitude'], df['dropoff_longitude'], df['dropoff_latitude'])\n",
    "    df['h_distance']= h_distance\n",
    "    df['h_distance']= df['h_distance'].astype('float32')\n",
    "    return df\n",
    "taxi_df = taxi_df.map_partitions(haversine_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478fb4ad-effd-4bcc-a634-d326bde720d3",
   "metadata": {},
   "source": [
    "#### 3. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e2c1b5-5a74-4cb8-9b0c-8191e8b16722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "X, y = taxi_df.drop([\"fare_amount\"], axis=1).astype('float32'), taxi_df[\"fare_amount\"].astype('float32')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173cb8b9-74e3-4e7b-9f11-0c4a5225e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = client.has_what().keys()\n",
    "X_train, X_test, y_train, y_test = dask_utils.persist_across_workers(client,\n",
    "                                                     [X_train, X_test, y_train, y_test],\n",
    "                                                     workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d75aa9-e090-4b19-a4ad-990c3c07e65e",
   "metadata": {},
   "source": [
    "#### 4. Create and fit a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d7d4c-7fea-4ba5-b963-4683ce19eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cuml.dask RF regressor\n",
    "cu_dask_rf = RandomForestRegressor(ignore_empty_partitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbdfb89-8338-41aa-be5b-c8b706dc298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit RF model\n",
    "cu_dask_rf = cu_dask_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e7a88-dc31-4d1d-94ed-d49f025ad8d2",
   "metadata": {},
   "source": [
    "#### 5. Predict on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b59737-9170-432a-bb42-7877fe545dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on validation set\n",
    "y_pred = cu_dask_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf28b4d-cb17-45d2-bb1d-369198709198",
   "metadata": {},
   "source": [
    "#### 6. Compute RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6309f0-3733-4720-b0b3-d318a9bd1758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute RMSE\n",
    "score = mean_squared_error(y_pred.compute().to_array(), y_test.compute().to_array())\n",
    "print(\"Workflow Complete - RMSE: \", np.sqrt(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf7952-3e49-45ed-87bd-2bd0e36fa9a2",
   "metadata": {},
   "source": [
    "### Resource Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd4744-362c-480e-9053-11095ef64451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eac59b-2073-4598-9b79-c5ecd5b58d8f",
   "metadata": {},
   "source": [
    "#### Learn More\n",
    "\n",
    "- [Multi-node multi-GPU Random Forest example on Azure using dask-cloudprovider](https://github.com/rapidsai/cloud-ml-examples/blob/main/azure/notebooks/Azure-MNMG-RF.ipynb)\n",
    "\n",
    "- [\n",
    "Multi-Node Multi-GPU XGBoost example on Azure using dask-cloudprovider](https://github.com/rapidsai/cloud-ml-examples/blob/main/azure/notebooks/Azure-MNMG-XGBoost.ipynb)\n",
    "\n",
    "- [Dask Cloud Provider](https://cloudprovider.dask.org/en/latest/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
